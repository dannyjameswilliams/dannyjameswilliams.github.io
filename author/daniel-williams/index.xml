<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel Williams | Danny James Williams</title>
    <link>https://dannyjameswilliams.co.uk/author/daniel-williams/</link>
      <atom:link href="https://dannyjameswilliams.co.uk/author/daniel-williams/index.xml" rel="self" type="application/rss+xml" />
    <description>Daniel Williams</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><lastBuildDate>Thu, 24 Jun 2021 12:19:03 +0000</lastBuildDate>
    <image>
      <url>https://dannyjameswilliams.co.uk/images/icon_hu6de9a8f7dd4e8a8bd7c2613cf2ad59bf_37670_512x512_fill_lanczos_center_3.png</url>
      <title>Daniel Williams</title>
      <link>https://dannyjameswilliams.co.uk/author/daniel-williams/</link>
    </image>
    
    <item>
      <title>Daniel Williams</title>
      <link>https://dannyjameswilliams.co.uk/author/daniel-williams/</link>
      <pubDate>Thu, 24 Jun 2021 12:19:03 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/author/daniel-williams/</guid>
      <description>&lt;p&gt;I am studying a PhD in machine learning and data science at the University of Bristol. I graduated from the University of Exeter with a Masters in Mathematics in 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How can we do data science without all of our data?</title>
      <link>https://dannyjameswilliams.co.uk/post/nodata/</link>
      <pubDate>Thu, 24 Jun 2021 12:19:03 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/nodata/</guid>
      <description>  

&lt;script src=&#34;https://dannyjameswilliams.co.uk/post/nodata/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;This research serves as a pre-cursor to the work that I am carrying out as part of my PhD, and involves unnormalised and truncated probability density estimation. This is a field which I find extremely interesting, and I believe that it has far reaching applications not just across data science and machine learning, but across many other disciplines as well. This blog post is an attempt at a more accessible explanation behind some of the core concepts in &lt;a href=&#34;https://arxiv.org/pdf/1910.03834.pdf&#34;&gt;this paper (in pre-print)&lt;/a&gt;, by Song Liu, Takafumi Kanamori and myself.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Firstly, consider the locations of reported crimes in Chicago below, and think about where the ‘centres’ of these reported crimes are.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/chicago.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 1:&lt;/strong&gt; Homicide locations inside the city of Chicago recorded in 2008.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Now think about where typical statistical estimation methods would put these crime ‘centres’. How do you think these would differ to what the ‘true’ centres are? Something like &lt;em&gt;maximum likelihood&lt;/em&gt; estimation may be slightly incorrect in some way, because the reported crimes get cut off, or &lt;em&gt;truncated&lt;/em&gt; beyond the city’s borders.&lt;/p&gt;
&lt;p&gt;The dataset is incomplete; crimes do not magically stop where we decide the city ends, but data collection does.&lt;/p&gt;
&lt;p&gt;This is an example of a more general problem in statistics named &lt;strong&gt;truncated probability density estimation&lt;/strong&gt;: How do we estimate the parameters of a statistical model when data are not fully observed, and are cut off by some artificial boundary?&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;A statistical model (a probability density function involving some parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt;) is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\boldsymbol{x}; \boldsymbol{\boldsymbol{\theta}}) = \frac{1}{Z(\boldsymbol{\boldsymbol{\theta}})} \bar{p}(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}}), \qquad Z(\boldsymbol{\boldsymbol{\theta}}) = \int \bar{p}(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}})d\boldsymbol{x}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is made up of two components: &lt;span class=&#34;math inline&#34;&gt;\(\bar{p}(\boldsymbol{x}; \boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt;), the unnormalised part of the model, which is known analytically; and &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt;, the normalising constant. Over complicated boundaries such as city borders, &lt;em&gt;this normalising constant cannot be calculated directly&lt;/em&gt;, which is why something like maximum likelihood would fail in this case.&lt;/p&gt;
&lt;p&gt;When we perform estimation, we aim to find &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt; which makes our model density, &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x}; \boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt;, as closely resemble a theoretical data density, &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;, as possible. Usually, we can estimate &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt; by trying to minimise some measure of difference between the &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;. But we cannot do this while &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt; is unknown!&lt;/p&gt;
&lt;p&gt;Most methods in this regard opt for a numerical approximation to integration, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&#34;&gt;MCMC&lt;/a&gt;. But these methods are usually very slow and computationally heavy. Surely there must be a better way?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-recipe-for-a-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Recipe for a Solution&lt;/h2&gt;
&lt;div id=&#34;ingredient-1-score-matching&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ingredient #1: Score Matching&lt;/h4&gt;
&lt;p&gt;A novel estimation method called score matching enables estimation even when the normalising constant, &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt;, is unknown. Score matching begins by taking the derivative of the logarithm of the probability density function, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta}) = \nabla_\boldsymbol{x} \log \bar{p}(\boldsymbol{x};\boldsymbol{\theta}),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which has become known as the score function. When taking the derivative, the dependence on &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\theta})\)&lt;/span&gt; is removed. To estimate the parameter vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}\)&lt;/span&gt;, we can minimise the difference between &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x};\boldsymbol{\theta})\)&lt;/span&gt; by minimising the difference between the two score functions, &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\boldsymbol{x} \log q(\boldsymbol{x})\)&lt;/span&gt;. One such distance measure is the expected squared distance, so that score matching aims to minimise&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} [\| \nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta}) - \nabla_\boldsymbol{x} \log q(\boldsymbol{x})\|_2^2].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With this first ingredient, we have eliminated the requirement that we must know the normalising constant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ingredient-2-a-weighting-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ingredient #2: A Weighting Function&lt;/h4&gt;
&lt;p&gt;Heuristically, we can imagine our weighting function should vary with how close a point is to the boundary. To satisfy score matching assumptions, we require that this weighting function &lt;span class=&#34;math inline&#34;&gt;\(g(\boldsymbol{x})\)&lt;/span&gt; must have the property that &lt;span class=&#34;math inline&#34;&gt;\(g(\boldsymbol{x}&amp;#39;)\)&lt;/span&gt; = 0 for any point &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}&amp;#39;\)&lt;/span&gt; on the boundary. A natural candidate would be the Euclidean distance from a point &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt; to the boundary, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
g(\boldsymbol{x}) = \|\boldsymbol{x} - \tilde{\boldsymbol{x}}\|_2, \qquad \tilde{\boldsymbol{x}} = \text{argmin}_{\boldsymbol{x}&amp;#39;\text{ in boundary}} \|\boldsymbol{x} - \boldsymbol{x}&amp;#39;\|_2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This easily satisfies our criteria. The distance is going to be exactly zero on the boundary itself, and will approach zero the closer the points are to the edges.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ingredient-3-any-statistical-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ingredient #3: Any Statistical Model&lt;/h4&gt;
&lt;p&gt;Since we do not require knowledge of the normalising constant through the use of score matching, we can choose any probability density function &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x}; \boldsymbol{\theta})\)&lt;/span&gt; that is appropriate for our data. For example, if we are modelling count data, we may choose a Poisson distribution. If we have some sort of centralised location data, such as in the Chicago crime example in the introduction, we may choose a multivariate Normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;combine-ingredients-and-mix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Combine Ingredients and Mix&lt;/h3&gt;
&lt;p&gt;We aim to minimise the expected distance between the score functions, and we weight this by our function &lt;span class=&#34;math inline&#34;&gt;\(g(\boldsymbol{x})\)&lt;/span&gt;, to give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\boldsymbol{\theta}} \mathbb{E} [g(\boldsymbol{x}) \| \nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta}) - \nabla_\boldsymbol{x} \log q(\boldsymbol{x})\|_2^2].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The only unknown in this equation now is the data density &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt; (if we knew the true data density, there would be no point in estimating it with &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x};\boldsymbol{\theta})\)&lt;/span&gt;). However, we can rewrite this equation using integration by parts as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\boldsymbol{\theta}} \mathbb{E} \big[ g(\boldsymbol{x}) [\|\nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta})\|_2^2 + 2\text{trace}(\nabla_\boldsymbol{x}^2 \log p(\boldsymbol{x}; \boldsymbol{\theta}))] + 2 \nabla_\boldsymbol{x} g(\boldsymbol{x})^\top \nabla_\boldsymbol{x} \log p(\boldsymbol{x};\boldsymbol{\theta})\big].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This can be numerically minimised, or if &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x}; \boldsymbol{\theta})\)&lt;/span&gt; is in the exponential family, analytically minimised to obtain estimates for &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;artificial-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Artificial Data&lt;/h3&gt;
&lt;p&gt;As a sanity check for testing estimation methods, it is often reassuring to perform some experiments on simulated data before moving to real world applications. Since we know the true parameter values, it is possible to calculate how far our estimated values are from their true counterparts, thus giving a way to measure estimation accuracy.&lt;/p&gt;
&lt;p&gt;Pictured below in Figure 2 are two experiments where data are simulated from a multivariate normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^* = [1, 1]\)&lt;/span&gt; and known variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1\)&lt;/span&gt;. Our aim is to estimate the parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat{\boldsymbol{\mu}}\)&lt;/span&gt; to be close to &lt;span class=&#34;math inline&#34;&gt;\([1,1]\)&lt;/span&gt;. The red crosses in the image are the true means at &lt;span class=&#34;math inline&#34;&gt;\([1,1]\)&lt;/span&gt;, and the red dots are the estimates given by truncated score matching.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/mvn_l2.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 2 (a)&lt;/strong&gt;: Points are only visible around a unit ball from [0,0].
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/mvn_box.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 2 (b)&lt;/strong&gt;: Points are only visible inside a box to the left of the mean.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;These figures clearly indicate that in this basic case, this method is giving sensible results. Even by ignoring most of our data, as long as we formulate our problem correctly, we can still get accurate results for estimation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chicago-crime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chicago Crime&lt;/h3&gt;
&lt;p&gt;Back to our motivating example, where the task is to estimate the ‘centres’ of reported crime in the city of Chicago, given the longitudes and latitudes of homicides in 2008. Our specification changes from the synthetic data somewhat:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;from the original plots, it seems like there are two centres, so the statistical model we choose is a 2-dimensional mixed multivariate Normal distribution;&lt;/li&gt;
&lt;li&gt;the variance is no longer known, but to keep estimation straightforward, the standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is fixed so that &lt;span class=&#34;math inline&#34;&gt;\(2\sigma\)&lt;/span&gt; roughly covers the ‘width’ of the city (&lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.06^\circ\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below I implement estimation for the mean for both truncated score matching and maximum likelihood.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/chicago_estimate.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 3:&lt;/strong&gt; Homicides in Chicago recorded in 2008, with the means estimated by truncated score matching (truncSM) and maximum likelihood estimation (MLE).
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Truncated score matching has placed its estimates for the means in similar, but slightly different places than standard MLE. Whereas the MLE means are more central to the truncated dataset, the truncated score matching estimates are placed closer to the outer edges of the city. For the region of crime in the top half of the city, the data are more ‘tightly packed’ around the border – which is a property we expect of Normally distributed data closer to its mean.&lt;/p&gt;
&lt;p&gt;Whilst we don’t have any definitive ‘true’ mean to compare it to, we could say that the truncSM mean is closer to what we would expect than the one estimated by MLE.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that this result does not reflect the true likelihood of crime in a certain neighbourhood, and has only been presented to provide a visual explanation of the method. The actual likelihood depends on various characteristics in the city that are not included in our analysis, &lt;a href=&#34;https://link.springer.com/article/10.1007/s41109-019-0239-8&#34;&gt;see here for more details&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;Score matching is a powerful tool, and by not requiring any form of normalising constant, enables the use of some more complicated models. Truncated probability density estimation is just one such example of an intricate problem which can be solved with this methodology, but it is one with far reaching and interesting applications.&lt;/p&gt;
&lt;p&gt;Whilst this blog post has focused on location-based data and estimation, truncated probability density estimation could have a range of applications. For example, consider disease/virus modelling such as the COVID-19 pandemic: The true number of cases is obscured by the number of tests that can be performed, so the density evolution of the pandemic through time could be fully modelled with incorporation of this constraint using this method. Other applications as well as extensions to this method will be fully investigated in my future PhD projects.&lt;/p&gt;
&lt;div id=&#34;further-reading-and-bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further Reading and Bibliography&lt;/h3&gt;
&lt;p&gt;Firstly, &lt;a href=&#34;https://github.com/dannyjameswilliams/truncsm-blog&#34;&gt;here is the github page&lt;/a&gt; where you can reproduce all the plots made available in this blog post, as well as some generic code for the implementation of truncated score matching.&lt;/p&gt;
&lt;p&gt;The original score matching reference:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf&#34;&gt;Estimation of Non-Normalized Statistical Models by Score Matching, Aapo Hyvärinen (2005)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper from which this blog post originates:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.03834.pdf&#34;&gt;Estimating Density Models with Truncation Boundaries using Score Matching. Song Liu, Takafumi Kanamori, &amp;amp; Daniel J. Williams (2021)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hot Takes for R</title>
      <link>https://dannyjameswilliams.co.uk/post/hottakes/</link>
      <pubDate>Sun, 28 Jun 2020 15:39:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/hottakes/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/post/hottakes/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The arrow assigment operator &lt;code&gt;&amp;lt;-&lt;/code&gt; is useless. Before I’m crucified by the R community, hear me out and read this post.&lt;/p&gt;
&lt;p&gt;Every time I read code written by an academic, lecturer or someone who uses R frequently, I come across the arrow symbol &lt;code&gt;&amp;lt;-&lt;/code&gt; used for assignment of variables. Never in my career have I seen someone systematically use the equals symbol &lt;code&gt;=&lt;/code&gt; across their code.&lt;/p&gt;
&lt;div id=&#34;benefits-of-the-arrow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Benefits of the arrow&lt;/h3&gt;
&lt;p&gt;A frequent association with &lt;code&gt;&amp;lt;-&lt;/code&gt; is in how assignment works in R. The variable on the right hand side of the operator is assigned to the one on the left. Hence the arrow makes a lot of sense. We can also do it the other way around, for instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;3 -&amp;gt; x
y &amp;lt;- 5
cat(&amp;quot;x is&amp;quot;, x, &amp;quot;and y is&amp;quot;, y, &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## x is 3 and y is 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the arrow has a benefit when teaching programming, so if you’re a beginner it is obvious which way around variables are assigned. If you’re not a beginner, it might reinforce this knowledge so that you don’t make mistakes.&lt;/p&gt;
&lt;p&gt;You can also use the arrow inside of functions to assign variables, for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(x &amp;lt;- solve(matrix(rnorm(100^2), 100, 100)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.002   0.000   0.004&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can view &lt;code&gt;x&lt;/code&gt; separately, even though it was assigned inside the &lt;code&gt;system.time&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x[1:5, 1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]        [,2]        [,3]       [,4]        [,5]
## [1,]  0.50730275 -0.35703351 -0.39847262  0.7788050  0.14551130
## [2,] -0.18092188  0.23194703  0.11982541 -0.4136690 -0.04548487
## [3,] -0.09788994  0.08614508  0.10585201 -0.1223789 -0.01548020
## [4,]  0.06537141 -0.16506482  0.09142846  0.1438222  0.02654319
## [5,]  0.03935626 -0.05008914 -0.09419370  0.2286113 -0.03929192&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is perhaps its most useful application, which you cannot do with &lt;code&gt;=&lt;/code&gt;. The &lt;code&gt;=&lt;/code&gt; sign inside of a function argument is strictly used for matching the function argument with the variable you’re passing through.&lt;/p&gt;
&lt;p&gt;The arrow also has historical significance, since R’s predecessor, S, used &lt;code&gt;&amp;lt;-&lt;/code&gt; exclusively. This &lt;a href=&#34;https://www.r-bloggers.com/why-do-we-use-arrow-as-an-assignment-operator/&#34;&gt;R-bloggers post&lt;/a&gt; explains that S was based on an older language called APL, which was designed on a keyboard that had an arrow key exactly like &lt;code&gt;&amp;lt;-&lt;/code&gt;. But our keyboards now only have a key for &lt;code&gt;=&lt;/code&gt;, right?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-you-should-accept-the-equals-sign&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why you should accept the equals sign&lt;/h3&gt;
&lt;p&gt;But I’m here today to tell you to not use &lt;code&gt;&amp;lt;-&lt;/code&gt; and to use &lt;code&gt;=&lt;/code&gt; instead. Start by asking yourself why you use the arrow? Maybe you have historical reasons and used R before 2001, or more likely, you’re following convention for coding in R that even &lt;a href=&#34;https://google.github.io/styleguide/Rguide.html&#34;&gt;styling guides&lt;/a&gt; &lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34;&gt;recommend&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Firstly, no other programming language uses the arrows, at least, none of the most frequently used ones such as Python, MATLAB, C++, Julia, Javascript, etc. So if you’re like me and use R alongside other programming languages, why would you bother using &lt;code&gt;&amp;lt;-&lt;/code&gt; instead of &lt;code&gt;=&lt;/code&gt;? Wouldn’t you like consistency across the languages you write in, at least so that your muscle memory doesn’t have to change depending on whether you’re fitting a Neural network in Python, or a GAM in R?&lt;/p&gt;
&lt;p&gt;Okay fair enough, maybe you don’t mind switching coding styles depending on what language you’re writing in, after all, you are going to be changing a lot more than just the assignment operator. So what other benefits does &lt;code&gt;=&lt;/code&gt; have?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a button for it on the keyboard.&lt;/li&gt;
&lt;li&gt;Consistency between function arguments and assignment.&lt;/li&gt;
&lt;li&gt;Increased readability and neatness since it has fewer character (admittedly, this is subjective).&lt;/li&gt;
&lt;li&gt;Similarity with equality operator (&lt;code&gt;==&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;No confusion between for example &lt;code&gt;x&amp;lt;-2&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(x=2\)&lt;/span&gt;) and &lt;code&gt;x &amp;lt; -2&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; -2\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Consistency with &lt;em&gt;mathematics itself&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, I prefer to use the equals assigment operator over the arrow, because I like to code in more than just one language.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-neat-full-stop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The neat full stop&lt;/h3&gt;
&lt;p&gt;While I’m on the subject of the arrow, using a full stop in a variable name brings a lot of confusion. This one is a lot less controversial than disregarding the arrow in my opinion. We can name a variable in R as&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;some.variable = 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks neat! But in other languages, this would throw an error. Why is that? Languages like Python use &lt;code&gt;.&lt;/code&gt; as a class operator, and you use it to access elements of a class exclusively, so you cannot use it in variable names. But R doesn’t have this problem, right?&lt;/p&gt;
&lt;p&gt;When defining an S3 class in R, you can overwrite some default functions (such as &lt;code&gt;print&lt;/code&gt; or &lt;code&gt;plot&lt;/code&gt;) with a new function that handles these default operations in a different way for your specific S3 class. To do this for an S3 class called &lt;code&gt;mys3class&lt;/code&gt;, you would write a new functions as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.mys3class = function(x, ...){
  ...
}
plot.mys3class = function(x, ...){
  ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look familiar? So full stops &lt;em&gt;do&lt;/em&gt; have a purpose in R apart from assigning neat variable names. For me, I don’t like using full stops for the main reason I don’t like using the &lt;code&gt;&amp;lt;-&lt;/code&gt; operator: &lt;strong&gt;consistency&lt;/strong&gt;. If I’m using &lt;code&gt;&amp;lt;-&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt;, it will be for a specific purpose where I cant use &lt;code&gt;=&lt;/code&gt; or &lt;code&gt;_&lt;/code&gt; (however, these are rules that I’ve broken myself, and you can probably find instances of it in my portfolios).&lt;/p&gt;
&lt;p&gt;So whilst neither the arrow (&lt;code&gt;&amp;lt;-&lt;/code&gt;) for assignment nor the full stop (&lt;code&gt;.&lt;/code&gt;) for variable naming are completely useless, better alternatives &lt;em&gt;do&lt;/em&gt; exist. However, if you value your code looking neat above all else, and aren’t bothered by cross platform consistency; then you can use R’s exclusive &lt;code&gt;&amp;lt;-&lt;/code&gt;, or its inconsistent &lt;code&gt;.&lt;/code&gt; without issue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(1:5, 1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/post/hottakes/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Electricity Demand Forecasting Hackathon</title>
      <link>https://dannyjameswilliams.co.uk/post/hackathon/</link>
      <pubDate>Mon, 17 Feb 2020 15:39:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/hackathon/</guid>
      <description>


&lt;p&gt;In February I participated in a COMPASS hackathon, where me and my fellow students fit statistical models to try to improve predictions in forecasting electricity demand based on weather related variables.&lt;/p&gt;
&lt;p&gt;We were fortunate to be visited by Dr Jethro Browell, a Research Fellow at the University of Strathclyde, who gave a brief lecture on how electricity demand was calculated, and how much it has changed over the last decade. After the lecture, Dr Mateo Fasiolo, a lecturer who works with us, explained a basic Generalised Additive Model (GAM) which can be used to forecast electricity demand for a particular dataset.&lt;/p&gt;
&lt;p&gt;Our task was to output a set of predictions for a testing dataset and submit them to the group git repository. We only had access to the predictor variables of this dataset, so we wouldn’t know how well our model was doing until it was submit and checked by Dr Fasiolo, who then put all submitted scores on the projector at the front of the room. The team with the lowest root mean squared error at the end would be crowned the winner.&lt;/p&gt;
&lt;p&gt;Me and my team “Jim” (named so because we went to the gym) performed well at the start, extending the basic GAM to include additional covariates and interactions, as well as including some feature engineering. The second team “AGang” (because all of their names began with “A”) took the edge over us by removing a single variable that we didn’t realise was actually making our model worse, and their GAM produced better predictions overall by a small margin. The third team “D &amp;amp; D” (because both their names began with a D) was having no luck at all, trying to implement a random forest model as opposed to a GAM, but their predictions were significantly off, and their code took much longer to run than ours, leaving them with little time to troubleshoot.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/hackathon.jpg&#34; alt=&#34;The COMPASS cohort after participating in the hackathon, with Dr Jethro Browell and Dr Mateo Fasiolo in the front.&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The COMPASS cohort after participating in the hackathon, with Dr Jethro Browell and Dr Mateo Fasiolo in the front.
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;Try as we did, we were unable to do any better than our original model; but we limited our scope to a GAM, and did not try anything out-of-the-box compared to the other two teams.&lt;/p&gt;
&lt;p&gt;The “AGang” were set to win it, until a surprise twist of fate sent “D&amp;amp;D” soaring into the lead, with predictions that had a far smaller error than anyone elses. The random forest model they were fitting before had an error, and they managed to fix the error, finish running the model and submit their predictions with only moments to spare. Thus, we came last.&lt;/p&gt;
&lt;p&gt;This was a fun competition, even though we lost. I realise that our mistake now was that we did not include anything special in our model that accounted for different weather patterns in different regions. Our model would have done very well if it was more variable; so that certain predictors were included in some areas that had more solar power, for instance. The way which we fit the model was the same for all regions, even though they were all quite different.&lt;/p&gt;
&lt;p&gt;You can read the article from the Bristol school of mathematics &lt;a href=&#34;https://www.bristol.ac.uk/maths/news/2020/compass-hackathon.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
