<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP | Danny James Williams</title>
    <link>https://dannyjameswilliams.co.uk/tag/nlp/</link>
      <atom:link href="https://dannyjameswilliams.co.uk/tag/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>NLP</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><lastBuildDate>Fri, 21 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dannyjameswilliams.co.uk/images/icon_hu6de9a8f7dd4e8a8bd7c2613cf2ad59bf_37670_512x512_fill_lanczos_center_3.png</url>
      <title>NLP</title>
      <link>https://dannyjameswilliams.co.uk/tag/nlp/</link>
    </image>
    
    <item>
      <title>Exploring Natural Language Embeddings of A Game of Thrones</title>
      <link>https://dannyjameswilliams.co.uk/post/asoiaf/</link>
      <pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/asoiaf/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/post/asoiaf/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;For those of you familiar with either HBO’s Game of Thrones, or George RR Martin’s A Song of Ice and Fire (ASOIAF), you will probably be aware of the vast differences between each character and the depth of each storyline. For those of you who aren’t familiar; each book chapter is written from the perspective of a different character, and George RR Martin writes each character in a distinct and unique way. A chapter written from the perspective of Eddard Stark, an aged and grizzled war veteran, will be portrayed vastly different to a chapter from the perspective of his son, Bran Stark, a 9 year old innocent ‘sweet summer child’.&lt;/p&gt;
&lt;p&gt;The stories surrounding all of the characters span continents, explore diverse themes and face different problems, and so we would expect there to be intrinsic differences between the language used for all of these different perspectives. &lt;em&gt;Can we use Natural Language Processing (NLP) to show how George RR Martin writes each of these characters?&lt;/em&gt; More specifically, is there actually a &lt;em&gt;mathematical difference&lt;/em&gt; between the chapters, grouped by character, based on the language alone?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A note on spoilers&lt;/strong&gt;: I am not including any excerpts from either the books or the show, and do not discuss the story. However, you will be able to see how many chapters are included for each character, as well as if any characters &lt;em&gt;stop&lt;/em&gt; having point-of-view chapters after a specific book.&lt;/p&gt;
&lt;div id=&#34;how-are-we-going-to-use-nlp-for-game-of-thrones&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How are we going to use NLP for Game of Thrones?&lt;/h2&gt;
&lt;p&gt;To show how characters are different amongst the ASOIAF books, I will use &lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;BERT&lt;/a&gt;, which is a NLP model built by Google, and an extremely powerful tool which can output &lt;em&gt;sentence embeddings&lt;/em&gt;. Put simply, a sentence embedding is a sequence of words, e.g. a sentence, which has been converted to a very high-dimensional mathematical vector. The vector itself won’t have any meaning to you or I on its own, but it has context relative to other sentences. For example, if two different sentence embeddings are very far away from each other, we can probably infer that the two original sentences are quite different.&lt;/p&gt;
&lt;p&gt;BERT can achieve this as it has been trained on a large corpus of language data: &lt;a href=&#34;https://arxiv.org/pdf/1506.06724.pdf&#34;&gt;a dataset of books&lt;/a&gt; and the full English Wikipedia. One way in which BERT trains is by &lt;em&gt;masked language modelling&lt;/em&gt;, which means it tries to predict random words in the dataset using the ‘transformer’ architecture, which is an extremely clever way of incorporating left-to-right and right-to-left directionality in a sentence so that the full context of the word is integrated. By learning where words come in the context of a sentence, BERT learns a lot about the English language. Whilst we aren’t trying to predict any words for our task, we will use the information that BERT learned whilst training as our emebeddings.&lt;/p&gt;
&lt;p&gt;Fortunately for us, BERT is made completely open-source and free to use via &lt;a href=&#34;https://huggingface.co/bert-base-uncased&#34;&gt;Huggingface&lt;/a&gt;, a collection of NLP models, datasets, and more. We can extract the embeddings from BERT and visualise them for each chapter to see if there is a strong degree of separation, which would indicate key differences in the language for different characters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asoiaf-embeddings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ASOIAF Embeddings&lt;/h1&gt;
&lt;p&gt;We can give BERT a sentence from ASOIAF, and it will output a relevant vector which gives information about this sentence. Repeating this for every sentence in the ASOIAF book series, we can obtain the high-dimensional sentence embeddings from each chapter of ASOIAF, by grouping by chapter and taking the average embedding across sentences.&lt;/p&gt;
&lt;p&gt;The embedding dimension output given by the base BERT model is 768, which we definitely cannot visualise. For this reason I have used &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/&#34;&gt;UMAP&lt;/a&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, a low-dimension projection method, to reduce the dimension to 3 so that we can visualise it.&lt;/p&gt;
&lt;div id=&#34;separation-by-character&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Separation by Character&lt;/h2&gt;
&lt;p&gt;Below I have plotted the (now 3D) embeddings in an interactive scatter plot, which you can rotate and move around. You can hover over each point to see the point-of-view character for each chapter and the corresponding book, as well as click on character names on the legend to remove them.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;750&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;100%&#34; src=&#34;//plotly.com/~dannyjameswilliams/169.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Mean BERT sentence embeddings of chapters in ASOIAF, split by character (with each book labelled), restricted to the 10 most frequently occuring point-of-view characters. The dimension was reduced from 768 to 3 using UMAP. &lt;a href=&#34;https://chart-studio.plotly.com/~dannyjameswilliams/169/#/&#34;&gt;See in fullscreen here&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;It is important to note that &lt;em&gt;no information about the classes (chapters/characters) was used at any point&lt;/em&gt;, so any structure we can see that separates the different characters or chapters is purely based on the language alone. The factors that could influence this are, for example: word choice, writing style, sentence length, and more.&lt;/p&gt;
&lt;p&gt;So what can we infer from this? The key aspects we are looking for are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How clustered are chapters from the same character?&lt;/li&gt;
&lt;li&gt;How separated are chapters from different characters?&lt;/li&gt;
&lt;li&gt;Are similar characters close to one another?&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;my-interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My Interpretation&lt;/h3&gt;
&lt;p&gt;From what I can see, the honourable and consistent Eddard Stark occupies a distinct region of the plot, and doesn’t stray far from it, but he is joined by other chapters from the first book. Daenerys’s storyline is mostly separated from the rest of the characters, so it makes sense that her chapters are grouped together and distinct in the plot. Other characters, such as Arya, seem to be uniquely identifiable due to being separated from the other clusters. The questionably lovable dwarf, Tyrion, has a plotline which spans battlefields, court intrigue, romance, death, and more. This seems to be shown here by his embeddings being spread across the entire space, representing the large variability in his changing viewpoints and scenarios.&lt;/p&gt;
&lt;p&gt;Surprisingly, I expected the child characters, most notably Sansa and Bran, to occupy a distinct region because of their naive and childlike viewpoints, but their distributions aren’t too different from most other characters. In fact, Jon’s and Bran’s chapters seem to exist in the same regions of the plot, which could indicate a similarity in the characters, or at least how the characters are written.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;separation-by-book&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Separation by Book&lt;/h2&gt;
&lt;p&gt;There seems to be a divide based on the book which each chapter is written in. Just by switching the labels (and removing the top 10 character restriction), we can inspect the same plot with a different angle.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;750&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;100%&#34; src=&#34;//plotly.com/~dannyjameswilliams/171.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Mean BERT sentence embeddings of chapters in ASOIAF, split by book. The dimension was reduced from 768 to 3 using UMAP. &lt;a href=&#34;https://chart-studio.plotly.com/~dannyjameswilliams/171/#/&#34;&gt;See in fullscreen here&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;The divide between classes is far more clear in this case&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; - books 2 and 3 (ACOK and ASOS respectively), are far, far, different from the other books in the series. The first, fourth and fifth books (AGOT, AFFC and ADWD respectively), are more closely related, with the first book being in a more distinctive class. Books 4 and 5 were written at the same time, which might explain their embeddings being so intertwined.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Thoughts&lt;/h1&gt;
&lt;p&gt;It is interesting how we can &lt;em&gt;mathematically see the difference in language&lt;/em&gt; based on the book or the character. There is a clear mathematical distinction between writing styles used for different characters, and we have even shown the evolution of George RR Martin’s prose over the course of writing the ASOIAF series.&lt;/p&gt;
&lt;p&gt;We can see the potential usefulness of these BERT embeddings, and they have far more use outside of plotting them to see their groups. We could’ve calculated the distance between embeddings to see exactly which characters are the most different or the most similar. We could use the embeddings themselves in a data science application; for example, how well can we classify which character is being written about based on only language?&lt;/p&gt;
&lt;p&gt;Existing research in NLP has given us the avenue to do all of this. It is an extremely interesting field, and is constantly developing. The methods we used here are free, and open to anyone for experimenting with. What other applications do you think would be cool to see? If you would like to ask any questions or have any discussion, &lt;a href=&#34;https://dannyjameswilliams.co.uk/#contact&#34;&gt;see my contact page&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The UMAP transform was fit with hyperparameters &lt;code&gt;min_dist = 0&lt;/code&gt; and &lt;code&gt;n_neighbors = 30&lt;/code&gt;, chosen by trial and error, to try to separate the classes as much as possible. All other values were kept as default in &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/api.html#umap&#34;&gt;this function&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For completeness, here are the initialisms used for each book. AGOT: A Game of Thrones, ACOK: A Clash of Kings, ASOS: A Storm of Swords, AFFC: A Feast for Crows, ADWD: A Dance with Dragons&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>(Tutorial) Exploring Natural Language Embeddings</title>
      <link>https://dannyjameswilliams.co.uk/tutorials/tutorialasoiaf/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/tutorials/tutorialasoiaf/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/tutorials/tutorialasoiaf/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Interested in how &lt;a href=&#34;http://dannyjameswilliams.co.uk/post/asoiaf/&#34;&gt;to plot BERT embeddings for different text&lt;/a&gt;? In the example linked, I obtained &lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;BERT&lt;/a&gt; embeddings for all the sentences from the book series ‘A Song of Ice and Fire’ (ASOIAF), the original novels which gave birth to the ‘A Game of Thrones’ TV show. By inspecting these embeddings, we could make mathematically make clear distinctions between how different characters were written, and how George RR Martin’s writing style changed for each book.&lt;/p&gt;
&lt;p&gt;This is straightforward to do if you know how. Overall, you need to follow the process:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Get the texts or the data that you want to embed and split it into chunks.&lt;/li&gt;
&lt;li&gt;Use each input in a forward pass on a pretrained BERT model, getting the last hidden state for the first token from the BERT model output.&lt;/li&gt;
&lt;li&gt;(Optional) Project each embedding into a lower dimension using dimensionality reduction, and plot.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These methods will use &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;. If you don’t have it installed, you will need to install it either via&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install torch&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install torch&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in your terminal. If you are using a notebook (like this tutorial), you must append an exclamation mark to the beginning of the lines.&lt;/p&gt;
&lt;div id=&#34;get-the-text-or-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;1. Get the Text or Data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For the ASOIAF novels, I had access to the e-books, which I converted into a text file using &lt;a href=&#34;https://ghostscript.com/&#34;&gt;Ghostscript&lt;/a&gt;. For the tutorial, I am going to assume that you have access to a file which contains the data you want to embed, otherwise I will be providing a different example. Due to copyright reasons, I will not be using any excerpts from a Song of Ice and Fire in this tutorial.&lt;/p&gt;
&lt;p&gt;Instead, I will use an open source data set from the &lt;code&gt;datasets&lt;/code&gt; library, called &lt;code&gt;sms_spam&lt;/code&gt;. This contains raw SMS messages and a hand-classified label indicating if they are spam or not. You can of course completely customise the text that you use, as all these methods will be generally applicable.&lt;/p&gt;
&lt;div id=&#34;download-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Download the dataset&lt;/h3&gt;
&lt;p&gt;If you haven’t already, you will need to install the library into python, using e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install datasets&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that, we load the dataset.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from datasets import load_dataset
df = load_dataset(&amp;quot;sms_spam&amp;quot;, split=&amp;quot;train&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Reusing dataset sms_spam (/home/danny/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the dataset is loaded, we can inspect it:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Dataset({
##     features: [&amp;#39;sms&amp;#39;, &amp;#39;label&amp;#39;],
##     num_rows: 5574
## })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Currently it is a huggingface &lt;code&gt;Dataset&lt;/code&gt; object. It makes it easier to use a &lt;code&gt;pandas&lt;/code&gt; dataframe, which we will use only 10% of (for simplicity, it’s a big dataset).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
df = pd.DataFrame(df).sample(frac=0.1)
df.columns = [&amp;quot;text&amp;quot;, &amp;quot;label&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a first look at the dataset using the &lt;code&gt;.head()&lt;/code&gt; command giving us the top 5 entries&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                    text  label
## 2995  No idea, I guess we&amp;#39;ll work that out an hour a...      0
## 3951  I got to video tape pple type in message lor. ...      0
## 3134                        So no messages. Had food?\n      0
## 3873  I am joining today formally.Pls keep praying.w...      0
## 456   Si si. I think ill go make those oreo truffles.\n      0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, a 1 corresponds to the text message being a spam message and 0 corresponds to a regular SMS.&lt;/p&gt;
&lt;p&gt;We can embed each message (in the &lt;code&gt;text&lt;/code&gt; column) individually, and see if there is any separability between spam messages and regular ones. Note that similar to the &lt;a href=&#34;http://dannyjameswilliams.co.uk/post/asoiaf/&#34;&gt;original ASOIAF example&lt;/a&gt;, we may also want to do an embedding for each individual sentence. In that case, we may want to split the text by sentence, then form a large dataframe for each sentence. However, for simplicity, I am going to embed each text in its entirety with BERT.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pass-the-text-through-the-bert-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;2. Pass the text through the BERT model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To get access to the BERT model, we will use &lt;a href=&#34;https://huggingface.co/bert-base-uncased&#34;&gt;Huggingface&lt;/a&gt;. This is a python library and open source repository containing large amounts of language models and datasets. Like before, we will need to install the &lt;code&gt;transformers&lt;/code&gt; library to access the models, via&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install transformers&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then we can use python to load the BERT model.&lt;/p&gt;
&lt;div id=&#34;using-a-gpu-or-cpu&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using a GPU or CPU&lt;/h3&gt;
&lt;p&gt;If you have access to a GPU with CUDA set up, great! It makes the model run a lot faster, but requires moving variables to the &lt;code&gt;cuda&lt;/code&gt; device. Otherwise, a CPU will be fine. Either way, you can run this line to set up the device that we are using.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import torch
device = &amp;quot;cuda&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For me, this is:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;cuda&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-and-using-the-model-and-tokenizer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Downloading and using the Model and Tokenizer&lt;/h3&gt;
&lt;p&gt;Now we import two classes from Huggingface: the model itself, used for forward passes, and the tokenizer, used to translate raw text input into something that the model understands. Then we use the &lt;code&gt;.from_pretrained()&lt;/code&gt; function to download the specific model and tokenizer from the BERT class of models that we want to use. In this case, I am specifying &lt;em&gt;bert-base-cased&lt;/em&gt;, the smaller BERT model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from transformers import BertModel, BertTokenizer
model = BertModel.from_pretrained(&amp;quot;bert-base-cased&amp;quot;).to(device)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: [&amp;#39;cls.predictions.bias&amp;#39;, &amp;#39;cls.predictions.transform.LayerNorm.bias&amp;#39;, &amp;#39;cls.predictions.transform.LayerNorm.weight&amp;#39;, &amp;#39;cls.seq_relationship.bias&amp;#39;, &amp;#39;cls.seq_relationship.weight&amp;#39;, &amp;#39;cls.predictions.decoder.weight&amp;#39;, &amp;#39;cls.predictions.transform.dense.weight&amp;#39;, &amp;#39;cls.predictions.transform.dense.bias&amp;#39;]
## - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
## - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tokenizer = BertTokenizer.from_pretrained(&amp;quot;bert-base-cased&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I make a function where raw text is the input, and the output is a BERT embedding. I will explain each step in this function afterwards.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def embed(prompt):
    encoded_input = tokenizer(prompt, return_tensors=&amp;#39;pt&amp;#39;, padding=True, truncation=True).to(device)
    X = model(**encoded_input)
    return X.last_hidden_state[:, 0, :]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function inputs a &lt;code&gt;prompt&lt;/code&gt;, which is a string. This string gets passed to the model’s &lt;code&gt;tokenizer&lt;/code&gt; (which is already defined in the environment), which is padded and truncated. This means that if the string is too large for the model, it cuts off the end, and if it is too small, null strings get padded so that all inputs are the same length. Next, the encoded input goes through as forward pass of the &lt;code&gt;model&lt;/code&gt;, which means augmenting the original input many times based on the parameters of BERT.&lt;/p&gt;
&lt;p&gt;During the tokenizing of the string, an extra &lt;code&gt;[CLS]&lt;/code&gt; token is added to the beginning of the text, called a &lt;em&gt;classification token&lt;/em&gt;. This token depends on the context of the sentence/paragraph because of the model architecture. This token is exactly what we want to use to represent our input, so we take the final hidden state output by the BERT model, and then the first element from that hidden state which corresponds to the &lt;code&gt;[CLS]&lt;/code&gt; token, which is what the function is returning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;embedding-the-text&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Embedding the Text&lt;/h3&gt;
&lt;p&gt;As an example of what the embedding actually is, we input a text and output a vector with shape:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;embed(&amp;quot;Help I&amp;#39;m stuck in a function input&amp;quot;).shape&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## torch.Size([1, 768])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And so is a 768-dimensional representation of our input. Now we just need to repeat this for our entire data set to build our embeddings. In this example, we have a representation of &lt;code&gt;Help I&#39;m stuck in a function input&lt;/code&gt;, but we will want representations of text in our data. We preemptively make a new &lt;code&gt;DataFrame&lt;/code&gt; for our embeddings, and embed each input text as a row, by looping over our data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;embedding_df = pd.DataFrame(index=df.index, columns = range(768))
for i in range(len(df)):
    embedding_df.loc[df.index[i], :] = embed(df.text.values[i]).cpu().detach()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in my case, I am saving &lt;code&gt;embedding_df&lt;/code&gt; to the CPU, as I won’t have enough memory to store this on my GPU. The extra &lt;code&gt;.cpu().detach()&lt;/code&gt; line is not necessary if you have been using a CPU the whole time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;project-the-embeddings-into-a-lower-dimension-to-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;3. Project the Embeddings into a Lower Dimension to Plot&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Since the embeddings are 768-dimensional, we cannot inspect them manually to see how they look! Instead, we are going to use &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/&#34;&gt;UMAP&lt;/a&gt; to project the 768 dimensions to 2 (or 3) and plot them as a scatter plot. For completeness, I will use the same hyperparameters as the original blog post, with &lt;code&gt;n_neighbors=30&lt;/code&gt; and &lt;code&gt;min_dist=0&lt;/code&gt;, to try to preserve out-of-class clustering. See the &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/parameters.html&#34;&gt;basic UMAP parameters&lt;/a&gt; section on the UMAP documentation for more information.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import umap
reducer = umap.UMAP(n_components = 2, n_neighbors = 30, min_dist=0.0, )
projected_embeddings = reducer.fit_transform(embedding_df.values)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have used the parameter &lt;code&gt;n_components=2&lt;/code&gt; as we want UMAP in 2D. If you wanted to do a 3D scatter plot, you could change this to &lt;code&gt;n_components=3&lt;/code&gt;, but you would also need to adjust the scatter plot below to be 3D.&lt;/p&gt;
&lt;p&gt;Now we can plot the projected embeddings.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
labels = df.label
for l in labels.unique():
    plt.scatter(projected_embeddings[labels == l, 0], projected_embeddings[labels == l, 1], label = l)
plt.legend();&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;plot.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recall: a label of 1 corresponds to spam and 0 corresponds to no spam. We can clearly see there is a large distinction between spam messages and regular text messages, as expected!&lt;/p&gt;
&lt;p&gt;This indicates that the language that a spam message uses is identifiably different to the language from a regular message. Whilst this might seem trivial to a human, it means that we can use these embeddings to classify a spam message. Since we can easily draw a line to distinguish the two classes, a classification approach (such as logistic regression) can be straightforwardly applied to accurately detect if a message is a spam one.&lt;/p&gt;
&lt;p&gt;These methods are obviously applicable to more complex scenarios, such as book chapters in the &lt;a href=&#34;http://dannyjameswilliams.co.uk/post/asoiaf/&#34;&gt;original post&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
