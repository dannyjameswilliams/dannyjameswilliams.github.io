<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research | Danny James Williams</title>
    <link>https://dannyjameswilliams.co.uk/tag/research/</link>
      <atom:link href="https://dannyjameswilliams.co.uk/tag/research/index.xml" rel="self" type="application/rss+xml" />
    <description>Research</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><lastBuildDate>Wed, 20 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dannyjameswilliams.co.uk/images/icon_hu6de9a8f7dd4e8a8bd7c2613cf2ad59bf_37670_512x512_fill_lanczos_center_2.png</url>
      <title>Research</title>
      <link>https://dannyjameswilliams.co.uk/tag/research/</link>
    </image>
    
    <item>
      <title>The Importance of Double and Triple Checking</title>
      <link>https://dannyjameswilliams.co.uk/post/mistakes/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/mistakes/</guid>
      <description>


&lt;p&gt;If you’re a PhD student, like me, you’ll probably have made a few (if not a lot) of mistakes over the course of your research. If you’re an aspiring PhD student, I implore you to read this post to learn about the greatest mistake I made during my studentship. This mistake was disguised, and I did not realise the implications it had.&lt;/p&gt;
&lt;div id=&#34;the-bait&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bait&lt;/h2&gt;
&lt;p&gt;In my second year of the PhD, I arrived at this incredible result.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;good_example.png&#34; alt=&#34;An experiment with good results&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 1&lt;/strong&gt;: Estimation error (lower is better) for my method, TKSD, against the previous method, &lt;em&gt;TruncSM&lt;/em&gt;, across different dimensions.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Amazing! TKSD, which is the method I was working on, outperformed the previous state-of-the-art method, &lt;em&gt;TruncSM&lt;/em&gt; by a huge margin. Across all dimensions, my error was lower, and this gap is widening as we increase the number of dimensions. &lt;em&gt;TruncSM&lt;/em&gt; &lt;a href=&#34;#truncsm&#34;&gt;[1]&lt;/a&gt; has been described in a &lt;a href=&#34;https://dannyjameswilliams.co.uk/post/nodata/&#34;&gt;previous blog post&lt;/a&gt; by me.&lt;/p&gt;
&lt;p&gt;The task given to the methods was a simple truncated density estimation problem - estimate the mean of a truncated multivariate Gaussian distribution, assuming the variance is known. We only observe a portion of our dataset, and assuming we know the truncation effect, we can give a good estimate of where the mean is. In this experiment, the true mean to be estimated was &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^\star = \boldsymbol{1}_d\)&lt;/span&gt;, the vector of ones, and whilst both methods got values close to &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{1}_d\)&lt;/span&gt;, TKSD seemed a lot better. But I wouldn’t be writing this blog post if something wasn’t wrong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-switch&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Switch&lt;/h2&gt;
&lt;p&gt;I would like to say that at the time of performing this experiment, I was a young and naive student. But this is not true, I had already been studying at University for 5 years, doing research for maybe 2 years at this point. Instead I think I was blindsided by how excellent these initial results were, I had been working towards this goal for what at the time was my entire PhD, and was very excited that my method was working so well. After seeing these results, I did double check - I tried (in 2D) estimating different values of means &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^\star\)&lt;/span&gt;, such as &lt;span class=&#34;math inline&#34;&gt;\([-1, -1]\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\([0.5, 0.5]\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\([2, 2]\)&lt;/span&gt; and more. All worked just as well, so I thought it can’t have been a fluke. I continued with these good results, put it into a poster I presented at the University (nothing official, thankfully), and got very excited.&lt;/p&gt;
&lt;p&gt;So what went wrong? You may have noticed that the different values of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^\star\)&lt;/span&gt; I double checked with were all repeated values. If we repeat the experiment where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^\star\)&lt;/span&gt; is different, for example, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^\star = [1, \dots, 1, -1]\)&lt;/span&gt;, such that the last dimension of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^\star\)&lt;/span&gt; is -1 instead of 1, we come to a slightly different result.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;bad_example.png&#34; alt=&#34;An experiment with not so good results&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 2&lt;/strong&gt;: Results with an extremely small change in the experiment from &lt;strong&gt;Figure 1&lt;/strong&gt;.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Our error is not good. In fact, it is very bad indeed. So much worse in fact, that it begs the question whether the method is even working at all?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-reason&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Reason&lt;/h2&gt;
&lt;p&gt;No, it isn’t. The method described earlier, TKSD, is not a method at all. It is a glorified bug in the code. I am going to be very slightly technical in this section to explain what I never realised for a very long time when I was working on this.&lt;/p&gt;
&lt;p&gt;Mathematically derived, my objective function that I was minimising over had the following term in it:
&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^n \sum_{j=1}^n \boldsymbol{\psi}_{p, i}^{\top} \boldsymbol{\psi}_{p, j} k\left(\mathbf{X}_i, \mathbf{X}_j\right).
\]&lt;/span&gt;
It is not really important what this term represents, other than that involves summing over some variables twice. If you’re interested, that is the double sum over a kernel function and the score function, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\psi}\)&lt;/span&gt;. Coding this in MATLAB, we could write something like&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;matlab_loop.png&#34; alt=&#34;&#34; /&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;However, using two for loops is slow, especially when we will be evaluating this function multiple times during optimisation. Instead, we often seek to vectorise code, for which I attempted by writing&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;matlab_vectorised.png&#34; alt=&#34;&#34; /&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Those of you familiar with vectorisation will know that this is &lt;strong&gt;not correct&lt;/strong&gt;. The operations are being performed in the wrong order, and what will come out of this ‘vectorised’ implementation is complete nonsense. But I fell for it. &lt;em&gt;The only way I tested if this was working was by running the first experiment&lt;/em&gt;, and since it seemed to work, I never questioned it.&lt;/p&gt;
&lt;p&gt;So there was a bug in the code, a bug that inexplicably caused good results. On its own, the vectorised implementation made no sense. To this day, I have no idea why this one particular change in the MATLAB code caused the method to &lt;em&gt;perform so well for such a randomly specific task&lt;/em&gt;. Trying to estimate parameters of a different distribution doesn’t work either, nor can it be used to estimate the variance of a multivariate Normal. All attempts to do anything other than estimate a symmetrical mean fail, and give nonsensical results. But this highlights the importance of &lt;em&gt;triple checking&lt;/em&gt; your work, maybe even &lt;em&gt;quadruple checking&lt;/em&gt; it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-payoff&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Payoff&lt;/h2&gt;
&lt;p&gt;This isn’t the first mistake I made, and it won’t be the last, but it was the most interesting one. What lessons have I learned from this disaster? Aside from being better at writing code for the remaining 3 years of my PhD, I will now always, &lt;strong&gt;always&lt;/strong&gt;, &lt;strong&gt;ALWAYS&lt;/strong&gt; verify the vectorised code against a looped implementation for a few &lt;em&gt;different&lt;/em&gt; cases before confirming that it works. Don’t rely on a strangely good result to verify that your coding is correct. You will nearly always make a bug in the code, assume it is wrong before assuming anything else.&lt;/p&gt;
&lt;p&gt;This extends further than this specific case - verification in any form of work is probably the most important thing you can do. Even under time pressure, I would prefer to make sure something is correct than to get more results that are wrong.&lt;/p&gt;
&lt;p&gt;But this kind of thing may happen for more than just incorrectly vectorising code - your brain doesn’t always work correctly, even for long periods of time. It’s good to get a new set of eyes on anything that seems fishy. It’s good to doubt yourself and make sure you don’t blindly trust positive results.&lt;/p&gt;
&lt;p&gt;Don’t quit when something goes wrong. It took about another whole year after this stage, but eventually, the TSKD method made it to a fully functioning and well performing method that (after extensive testing, verification, double, triple and quadruple checking) was accepted at ICML! &lt;a href=&#34;#tksd&#34;&gt;[2]&lt;/a&gt; &lt;a href=&#34;https://dannyjameswilliams.co.uk/post/tksd/&#34;&gt;I wrote a blog post&lt;/a&gt; about the finished method, if you are interested. And whilst the results aren’t &lt;em&gt;as&lt;/em&gt; good as the first experiment shown here, at least it is correct!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jmlr.org/papers/volume23/21-0218/21-0218.pdf&#34;&gt;[1]&lt;/a&gt; &lt;a id=&#34;truncsm&#34;&gt;&lt;/a&gt;Liu, S., Kanamori, T., and Williams, D. J. Estimating density models with truncation boundaries using score matching. &lt;i&gt;Journal of Machine Learning Research&lt;/i&gt;, 23(186):1–38, 2022.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://proceedings.mlr.press/v202/williams23b/williams23b.pdf&#34;&gt;[2]&lt;/a&gt; &lt;a id=&#34;tksd&#34;&gt;&lt;/a&gt;Williams, D.J., Liu, S. (2023). Approximate Stein Classes for Truncated Density Estimation. &lt;i&gt;Proceedings of the 40th International Conference on Machine Learning&lt;/i&gt;, in &lt;i&gt;Proceedings of Machine Learning Research&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>(Tutorial) Basic Usage of the Manifold TSM R Package</title>
      <link>https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/</link>
      <pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;von-mises-fisher-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;von-Mises Fisher distribution&lt;/h2&gt;
&lt;p&gt;We simulate &lt;span class=&#34;math inline&#34;&gt;\(n=1000\)&lt;/span&gt; samples from a von-Mises distribution with mean direction &lt;span class=&#34;math inline&#34;&gt;\((\pi/2, \pi)\)&lt;/span&gt; on the sphere, using the &lt;code&gt;rvmf&lt;/code&gt; function from the &lt;code&gt;Rfast&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Simulated VMF
n = 1000              
centre = c(pi/2, pi) 
centre_euclid = sphere_to_euclid(centre)   
set.seed(4)
z = rvmf(n, centre_euclid, k = 6)
x = euclid_to_sphere(z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now this can be estimated directly, using various methods such as MLE, but if we could only see a portion of this data, then truncated score matching can be used. Say there is a boundary set up such that all measurements below a constant line of &lt;span class=&#34;math inline&#34;&gt;\(\phi = \pi/2 + 0.2\)&lt;/span&gt; were missing. This can be constructed as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dV = cbind(rep(pi/2+0.2, 200), seq(0, 2*pi, len=200))
outside_boundary = x[,1] &amp;gt; (pi/2+0.2)
truncated_x = x[outside_boundary,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean direction can be estimated by calling &lt;code&gt;sphere_sm&lt;/code&gt; with a specified boundary function, one of &lt;code&gt;&#34;Haversine&#34;&lt;/code&gt; or &lt;code&gt;&#34;Euclidean&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_hav = sphere_sm(truncated_x, dV, g = &amp;quot;Haversine&amp;quot;, family = vmf(k=6))
est_euc = sphere_sm(truncated_x, dV, g = &amp;quot;Euclidean&amp;quot;, family = vmf(k=6))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;family&lt;/code&gt; argument is &lt;code&gt;vmf(k=6)&lt;/code&gt;. This is a wrapper function that specifies which distribution to use. The argument &lt;code&gt;k=6&lt;/code&gt; corresponds to the concentration parameter being fixed a t 6, if this weren’t supplied and &lt;code&gt;vmf()&lt;/code&gt; was used alone, the concentration parameter will be esitmated. By default the &lt;code&gt;sphere_sm&lt;/code&gt; function uses the von Mises Fisher distribution without specifying a value for &lt;code&gt;k&lt;/code&gt;. Now that the points are estimated, we can plot them using &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data = data.frame(colat = c(x[!as.logical(outside_boundary),1],
                                 x[as.logical(outside_boundary),1]),
                       lon = c(x[!as.logical(outside_boundary),2],
                               x[as.logical(outside_boundary),2]),
                       Data = c(rep(&amp;quot;Not-Observed&amp;quot;, sum(!as.logical(outside_boundary))),
                                rep(&amp;quot;Observed&amp;quot;, sum(outside_boundary))))
centre_data = data.frame(
  colat = c(est_hav$mu[1], est_euc$mu[1], centre[1]),
  lon = c(est_hav$mu[2], est_euc$mu[2], centre[2]),
  Centres = c(&amp;quot;Haversine&amp;quot;, &amp;quot;Projected Euclidean&amp;quot;, &amp;quot;Real Centre&amp;quot;)
)
 ggplot(plot_data) + geom_point(aes(x=lon, y=colat, col=Data), alpha=.7, size=2) +
  scale_color_brewer(type=&amp;quot;qual&amp;quot;, palette=3) + theme_minimal() +
  xlab(expression(theta)) + ylab(expression(phi)) +
   geom_point(data=centre_data, aes(lon, colat, fill=Centres), size=4, shape = &amp;quot;diamond filled&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
The point has been estimated reasonably well. Now lets try the same approach &lt;em&gt;without&lt;/em&gt; fixing the concentration parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_hav = sphere_sm(truncated_x, dV, g = &amp;quot;Haversine&amp;quot;, family = vmf())
est_euc = sphere_sm(truncated_x, dV, g = &amp;quot;Euclidean&amp;quot;, family = vmf())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Again, the centre has been found with good accuracy. We can inspect the output of &lt;code&gt;sphere_sm&lt;/code&gt; to see what value was estimated for &lt;code&gt;k&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_hav
#&amp;gt; $mu
#&amp;gt;    colat      lon 
#&amp;gt; 1.539329 3.112377 
#&amp;gt; 
#&amp;gt; $k
#&amp;gt;        k 
#&amp;gt; 5.415736 
#&amp;gt; 
#&amp;gt; $family
#&amp;gt; [1] &amp;quot;von Mises Fisher&amp;quot;
#&amp;gt; 
#&amp;gt; $val
#&amp;gt; [1] -2.044959
est_euc
#&amp;gt; $mu
#&amp;gt;    colat      lon 
#&amp;gt; 1.584297 3.111693 
#&amp;gt; 
#&amp;gt; $k
#&amp;gt;        k 
#&amp;gt; 5.481328 
#&amp;gt; 
#&amp;gt; $family
#&amp;gt; [1] &amp;quot;von Mises Fisher&amp;quot;
#&amp;gt; 
#&amp;gt; $val
#&amp;gt; [1] -1.713839&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;a-different-boundary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A different Boundary&lt;/h3&gt;
&lt;p&gt;We can experiment with a different boundary, implementing the same approach to the same dataset but a different truncation region. Start by simulating different data from the same distribution and constructing the boundary as before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(7)
z = rvmf(n, centre_euclid, k = 6)
x = euclid_to_sphere(z)
dV = cbind(c(rep(1.2, 200), seq(1.2, 3, len=200)), c(seq(0, 3.6, len=200), rep(3.6, len=200)))
outside_boundary = x[,2] &amp;lt; 3.6 &amp;amp; x[,1] &amp;gt; 1.2
truncated_x = x[as.logical(outside_boundary),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So in this case, the boundary is anything that fits &lt;span class=&#34;math inline&#34;&gt;\(\phi &amp;gt; 1.2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta &amp;lt; 3.6\)&lt;/span&gt;, roughly a box around the top left portion of the data. We call &lt;code&gt;sphere_sm&lt;/code&gt; as before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_hav = sphere_sm(truncated_x, dV, g = &amp;quot;Haversine&amp;quot;, family=vmf())
est_euc = sphere_sm(truncated_x, dV, g = &amp;quot;Euclidean&amp;quot;, family=vmf())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
The results are slightly less accurate than before, but still hold.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kent-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kent Distribution&lt;/h2&gt;
&lt;p&gt;We adopt a similar approach for the Kent distribution, simulating data, truncating it at an artificial boundary and experimenting with fitting the model to see how well the estimated mean direction &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is. Firstly, we use a boundary similar to before, truncated at &lt;span class=&#34;math inline&#34;&gt;\(\phi = \pi/2+0.1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z = rkent(n, k = 10, m = centre_euclid, b=3)
x = euclid_to_sphere(z)
dV = cbind(rep(pi/2+0.1, 500), seq(0, 2*pi, len=500))
outside_boundary = x[,1] &amp;gt; (pi/2 + 0.1)
truncated_x = x[outside_boundary,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the Kent distribution has five parameters, estimation can be difficult and slow in some cases. For the Kent distribution, we assume that the concentration parameter &lt;code&gt;k&lt;/code&gt; and ovalness parameter &lt;code&gt;b&lt;/code&gt; are known, and specified in advance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_hav = sphere_sm(truncated_x, dV, g=&amp;quot;Haversine&amp;quot;, family=kent(k=10, b=3))
est_euc = sphere_sm(truncated_x, dV, g=&amp;quot;Euclidean&amp;quot;, family=kent(k=10, b=3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And another boundary, similar to before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(12)
z = rkent(n, m=centre_euclid, k = 10, b=3)
dV = cbind(c(rep(1.2, 200), seq(1.2, 3, len=200)), c(seq(0, 3.6, len=200), rep(3.6, len=200)))
x = euclid_to_sphere(z)
outside_boundary = x[,2] &amp;lt; 3.6 &amp;amp; x[,1] &amp;gt; 1.2
truncated_x = x[as.logical(outside_boundary),]

est_hav = sphere_sm(truncated_x, dV, g = &amp;quot;Haversine&amp;quot;, family=kent(k=10, b=3))
est_euc = sphere_sm(truncated_x, dV, g = &amp;quot;Euclidean&amp;quot;, family=kent(k=10, b=3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
