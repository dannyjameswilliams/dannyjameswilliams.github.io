<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Danny James Williams</title>
    <link>https://dannyjameswilliams.co.uk/</link>
      <atom:link href="https://dannyjameswilliams.co.uk/index.xml" rel="self" type="application/rss+xml" />
    <description>Danny James Williams</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dannyjameswilliams.co.uk/images/icon_hu6de9a8f7dd4e8a8bd7c2613cf2ad59bf_37670_512x512_fill_lanczos_center_2.png</url>
      <title>Danny James Williams</title>
      <link>https://dannyjameswilliams.co.uk/</link>
    </image>
    
    <item>
      <title>Intro to C&#43;&#43;</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/intro/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/intro/</guid>
      <description>


&lt;div id=&#34;basic-c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic C++&lt;/h2&gt;
&lt;p&gt;C++ programs are written in ‘chunks’, and each chunk can be a function which contains some code, or a &lt;em&gt;main&lt;/em&gt; chunk, which is the program that is run when the code is compiled. Here is an example of a &lt;code&gt;main&lt;/code&gt; chunk that runs some code.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// include &amp;#39;iostream&amp;#39; package
#include &amp;lt;iostream&amp;gt;          

// int means this will output an integer, main() signifies the primary code to run
int main()                               
{
  // pipe operator, using std library, cout is to output something, endl is to end line
  std::cout &amp;lt;&amp;lt; &amp;quot;Hello&amp;quot; &amp;lt;&amp;lt; std::endl;    
  
  // return integer 0 to show all is okay
  return 0;                             
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This chunk of code will output the string &lt;code&gt;Hello&lt;/code&gt; when run, with comments (starting with &lt;code&gt;//&lt;/code&gt;) describing what each line does. However, this cannot be run by itself, since C++ is a &lt;em&gt;compiled&lt;/em&gt; programming language; so we need a compiler. Firstly, we save this file as &lt;code&gt;hello.cpp&lt;/code&gt;, and using the &lt;code&gt;g++&lt;/code&gt; compiler in the terminal will compile this code into an executable program.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ hello.cpp -o hello&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This starts with &lt;code&gt;g++&lt;/code&gt;, telling the terminal to use the &lt;code&gt;g++&lt;/code&gt; program, then chooses the file &lt;code&gt;hello.cpp&lt;/code&gt;, &lt;code&gt;-o hello&lt;/code&gt; specifies that the name of the output is &lt;code&gt;hello&lt;/code&gt;. This has compiled a program into the current working directory which can also be run in the terminal.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;./hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the code has run succesfully! The only output was &lt;code&gt;Hello&lt;/code&gt; as that was all that was specified to be output. Note that in C++ every integer, string, float, etc. needs to be defined in advance. Instead of in programming languages like R and Python, where nothing really needs to be specified in advance, variables need to be defined each time. For example,&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;int a = 3
float b = 14.512231
double c = 4e200
std::string d = &amp;quot;What up&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To further exemplify the usage of C++, consider the following example.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
int main()
{
  for (int i=1; i&amp;lt;=100; i++)
  {
    if (i&amp;gt;=95)
    {
      std::cout &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
    }
    else if (i&amp;lt;5)
    {
      for(float j=-1.5; j&amp;gt;=-3.5; j--)
      {
        std::cout &amp;lt;&amp;lt; i*j &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has used a range of different code, including &lt;code&gt;for&lt;/code&gt; loops and conditional statements &lt;code&gt;if&lt;/code&gt; and &lt;code&gt;else if&lt;/code&gt;. These are implemented in C++ similarly to the way they are implemented in R. In fact, the &lt;code&gt;if&lt;/code&gt; statements almost have the exact same formatting as R. Loops are a bit different. The syntax that C++ uses for basic loops are &lt;code&gt;for(initialise; condition; increment)&lt;/code&gt;, where &lt;code&gt;initialise&lt;/code&gt; refers to the first element that is being looped through, &lt;code&gt;condition&lt;/code&gt; is the stopping condition of the loop and &lt;code&gt;increment&lt;/code&gt; is how much the initaliser increases on each iteration.&lt;/p&gt;
&lt;p&gt;In this case, for the increment we have used &lt;code&gt;i++&lt;/code&gt; and &lt;code&gt;j--&lt;/code&gt;, which is shorthand for &lt;code&gt;i = i + 1&lt;/code&gt; and &lt;code&gt;j = j - 1&lt;/code&gt;. The stopping conditions are when &lt;code&gt;i&lt;/code&gt; is less than or equal to 100, and when &lt;code&gt;j&lt;/code&gt; is greater than or equal to -3.5. Each iteration of the loop checks two conditions, the value of &lt;code&gt;i&lt;/code&gt;, and different operations happen when &lt;code&gt;i&lt;/code&gt; is less than 5, or greater than or equal to 95. We can run this code to see the output.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ loopif.cpp -o loopif
./loopif&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -1.5 -2.5 -3.5 -3 -5 -7 -4.5 -7.5 -10.5 -6 -10 -14 95 96 97 98 99 100&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;times-table-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Times Table Example&lt;/h3&gt;
&lt;p&gt;Consider for a further example writing a function to print the times tables for a given number, a given number of times. For this, we can exemplify the use of &lt;em&gt;multi-file&lt;/em&gt; programs. By creating a header file, with extension &lt;code&gt;.h&lt;/code&gt;, and including this in our main program, we can specify functions in a different file. This helps navigate large code files, and separating different functions in different files can be extremely useful.&lt;/p&gt;
&lt;p&gt;Let’s start by creating a times table function in a &lt;code&gt;timestable.cpp&lt;/code&gt; file. This takes two integer inputs, the first being the times table we want to print, and the second being the maximum number we want to multiply to the first input.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;

void timestable(int a, int b)
{
  for(int i = 1 ; i &amp;lt;= b; i++)
  {
    std::cout &amp;lt;&amp;lt; a*i &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
  }
  std::cout &amp;lt;&amp;lt; std::endl;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this loops from &lt;code&gt;i=1&lt;/code&gt; to &lt;code&gt;b&lt;/code&gt;, and multiplies &lt;code&gt;a&lt;/code&gt; by &lt;code&gt;i&lt;/code&gt; on each iteration. Now we create a header file, called &lt;code&gt;timestable.h&lt;/code&gt;, containing the following&lt;/p&gt;
&lt;pre class=&#34;h&#34;&gt;&lt;code&gt;#ifndef _TIMESTABLE_H
#define _TIMESTABLE_H

void timestable(int, int);
  
#endif &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This header file checks and defines the token &lt;code&gt;_TIMESTABLE_H&lt;/code&gt;, and then simply declares the function &lt;code&gt;timestable&lt;/code&gt;. When reading this function into another C++ file, it will declare the &lt;code&gt;timestable&lt;/code&gt; function that is defined in &lt;code&gt;timestable.cpp&lt;/code&gt;. A main file, called &lt;code&gt;main.cpp&lt;/code&gt; to read the header file, which will define the &lt;code&gt;timestable&lt;/code&gt; function, and run it for for two examples.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;quot;timestable.h&amp;quot;

int main()
{
  std::cout &amp;lt;&amp;lt; &amp;quot;Five times table&amp;quot; &amp;lt;&amp;lt; std::endl;
  timestable(5, 10);

  std::cout &amp;lt;&amp;lt; &amp;quot;Twelve times table&amp;quot; &amp;lt;&amp;lt; std::endl;
  timestable(12, 12);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we are expected the output of the five times table, up to 5 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 10, and the twelve times table, up to 12 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 12. Let’s compile and run this program.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ main.cpp timestable.cpp -o timestable
./timestable&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Five times table
## 5 10 15 20 25 30 35 40 45 50 
## Twelve times table
## 12 24 36 48 60 72 84 96 108 120&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it has worked as expected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to R</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/intro/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/intro/</guid>
      <description>


&lt;div id=&#34;command-console&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Command Console&lt;/h2&gt;
&lt;p&gt;R provides a command console, which is where all code is processed. You can enter commands directly into the command console, or run them from a script. Both will result in whatever command being executed. For example, we can perform an operation such as&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;10*10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and it will output the result. If we wanted to save the output, we assign this code to a &lt;em&gt;variable&lt;/em&gt;, which is saved into the environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;multiplication_variable &amp;lt;- 10*10
multiplication_variable2 = 10*10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now we have two variables in our environment, &lt;code&gt;multiplication_variable&lt;/code&gt; and &lt;code&gt;multiplication_variable2&lt;/code&gt;. Both should be the same value, the only difference in how they were assigned. &lt;code&gt;multiplication_variable&lt;/code&gt; was assigned with the &lt;code&gt;&amp;lt;-&lt;/code&gt; operator, whereas &lt;code&gt;multiplication_variable2&lt;/code&gt; was assigned with the &lt;code&gt;=&lt;/code&gt; operator.&lt;/p&gt;
&lt;p&gt;We can use the &lt;code&gt;==&lt;/code&gt; command to check whether two variables are equal. This is an equality sign, and will output either &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt; (or a vector of &lt;code&gt;TRUE&lt;/code&gt; and &lt;code&gt;FALSE&lt;/code&gt; if working with vectors).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;multiplication_variable == multiplication_variable2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Confirming that the two variables are equal to each other!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;operators-and-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Operators and Functions&lt;/h2&gt;
&lt;p&gt;R has many operators, too many to list here, but you can intuitively understand the basic operators such as divide (&lt;code&gt;/&lt;/code&gt;), multiply (&lt;code&gt;*&lt;/code&gt;), add (&lt;code&gt;+&lt;/code&gt;) and subtract (&lt;code&gt;-&lt;/code&gt;). Some other less common operators include the matrix multiply (&lt;code&gt;%*%&lt;/code&gt;), integer division (&lt;code&gt;%/%&lt;/code&gt;), integer modulus (&lt;code&gt;%%&lt;/code&gt;) and exponentiate (&lt;code&gt;^&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;By default, R loads in a certain number of basic packages, including &lt;code&gt;base&lt;/code&gt;, &lt;code&gt;stats&lt;/code&gt; and &lt;code&gt;utils&lt;/code&gt;. Through these packages, a large amount of functions are available, all useful. Other packages can be loaded by using the &lt;code&gt;library&lt;/code&gt; function. For example, suppose I wanted to simulate from a multivariate Normal distribution. There is no package in base R to do this, but there is a function to do this in the &lt;code&gt;MASS&lt;/code&gt; library. First, if this package is not installed then it needs to be done so by using &lt;code&gt;install.packages(&amp;quot;MASS&amp;quot;)&lt;/code&gt; (which only needs to be done once). To load the library, we run&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the function should be available. To find out what arguments the function takes, and what to input to the function, we can look at its help file by running &lt;code&gt;?mvrnorm&lt;/code&gt;, this has a ‘Usage’ section detailing the following&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mvrnorm(n = 1, mu, Sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE)

n   - the number of samples required.
mu - a vector giving the means of the variables.
Sigma - a positive-definite symmetric matrix specifying the covariance matrix of the variables.
tol - tolerance (relative to largest variance) for numerical lack of positive-definiteness in Sigma.
empirical - logical. If true, mu and Sigma specify the empirical not population mean and covariance matrix.
EISPACK - logical: values other than FALSE are an error.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the variables in &lt;code&gt;mvrnorm&lt;/code&gt; we need to specify are &lt;code&gt;n&lt;/code&gt;, &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;Sigma&lt;/code&gt;. Let’s run the function now&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = mvrnorm(5, mu = c(1,2), Sigma = matrix(c(1,1,1,1),2,2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can look at this output by simply typing &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]      [,2]
## [1,]  0.97765673 1.9776567
## [2,]  0.49305126 1.4930513
## [3,] -0.06323688 0.9367631
## [4,]  0.77825704 1.7782570
## [5,]  2.23512186 3.2351219&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that in the specification to &lt;code&gt;mvrnorm&lt;/code&gt;, two other functions were used; &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;matrix&lt;/code&gt;. If you are curious about these, look at the help files for them. Packages and functions are key to using R effectively and efficiently.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Common R</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/common/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/common/</guid>
      <description>


&lt;div id=&#34;performing-operations-on-vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performing operations on vectors&lt;/h2&gt;
&lt;p&gt;In general, there are three methods that can be used to perform the same operation (such as a mathematical operation) on every element in a vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt;. A simple way of doing this is with a &lt;strong&gt;loop&lt;/strong&gt;, which iterates once for each element in &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt;, and performs the operation one at a time. &lt;strong&gt;Vectorisation&lt;/strong&gt; refers to the process of applying the same operation to every element in a vector at once, whereas the &lt;code&gt;apply&lt;/code&gt; function applies any function across a vector in a single line of code.&lt;/p&gt;
&lt;div id=&#34;comparison-between-vectorisation-loops-and-apply&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparison between vectorisation, loops and &lt;code&gt;apply&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We can test the efficiency of using vectorisation as opposed to using a loop or an &lt;code&gt;apply&lt;/code&gt; function. We will construct three pieces of code to do the same thing, that is, apply the function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = \sin(x)\)&lt;/span&gt; to all elements in a vector, which is constructed of the natural numbers up to 100,000. We start by creating the vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = seq(1,100000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create three functions. One that uses a for loop, one that uses &lt;code&gt;apply&lt;/code&gt; and one that works by vectorising.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loop_function = function(x){
  y = numeric(length(x))
  for(i in 1:length(x)){
    y[i] = sin(x[i])
  }
  return(y)
}

apply_function = function(x){
  y = apply(as.matrix(x), 1, sin)
  return(y)
}

vector_function = function(x){
  y = sin(x)
  return(y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the functions are constructed, we can use the inbuilt R function &lt;code&gt;system.time&lt;/code&gt; to calculate how long it takes each function to run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(loop_function(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.058   0.000   0.080&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(apply_function(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.312   0.000   0.312&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(vector_function(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.006   0.000   0.005&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Naturally, none of these computations take very long to perform, as the process of taking the sine isn’t very complex. However, you can still see the order of which functions are the fastest. In general, vectorisation will always be more efficient than loops or &lt;code&gt;apply&lt;/code&gt; functions, and loops are faster than using &lt;code&gt;apply&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are cases where it will not be possible to use vectorisation to carry out a task on an array. In this case, it is necessary to construct a function to pass through &lt;code&gt;apply&lt;/code&gt;, or performing operations within a loop. In general, loops are faster and more flexible - as they allow you to do more in each iteration than a function could. Some situations where you might want to use &lt;code&gt;apply&lt;/code&gt; is to make a simple process neater in the code. If you were doing something relatively straightforward, you will save space and make the code more readable by using &lt;code&gt;apply&lt;/code&gt;, as opposed to a loop.&lt;/p&gt;
&lt;p&gt;It is common practice to always vectorise your code when you can, as it comes with a significant speed increase, as loops and &lt;code&gt;apply&lt;/code&gt; functions are slower than vectorised code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other functions&lt;/h3&gt;
&lt;p&gt;There are different variants of the &lt;code&gt;apply&lt;/code&gt; function depending on how your data are constructed, and how you would want your output.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apply(X, MARGIN, FUN, ...)&lt;/code&gt; is the basic apply function. &lt;code&gt;MARGIN&lt;/code&gt; refers to which dimension remains constant when performing the function. For example, &lt;code&gt;apply(sum,1,x)&lt;/code&gt; will sum across the columns, and the number of rows will remain constant.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lapply(X, FUN, ...)&lt;/code&gt; is an apply function that returns a list as its output, each element in the list corresponding to applying the given function to each value in &lt;code&gt;X&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sapply(X, FUN, ...)&lt;/code&gt; is a wrapper of &lt;code&gt;lapply&lt;/code&gt; that will simplify the output so that it is not in list form.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mapply(FUN, ...)&lt;/code&gt; is a multi-dimensional version of &lt;code&gt;sapply&lt;/code&gt;, with &lt;code&gt;mapply&lt;/code&gt; it is possible to add more than one input to the function, and it will return a vector of values for each set of inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;map-reduce-and-filter&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Map, Reduce and Filter&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Map&lt;/code&gt; maps a function to a vector. This is similar to &lt;code&gt;lapply&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = seq(1, 3, by=1)
f = function(a) a+5
M = Map(f,x)
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 6
## 
## [[2]]
## [1] 7
## 
## [[3]]
## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has added 5 to every element in &lt;code&gt;x&lt;/code&gt;, and returned a list of outputs for each element. In fact, &lt;code&gt;Map&lt;/code&gt; performs the same operation as &lt;code&gt;mapply&lt;/code&gt; does, which we can see in the function itself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Map&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (f, ...) 
## {
##     f &amp;lt;- match.fun(f)
##     mapply(FUN = f, ..., SIMPLIFY = FALSE)
## }
## &amp;lt;bytecode: 0x55bfca7455a8&amp;gt;
## &amp;lt;environment: namespace:base&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Reduce&lt;/code&gt; performs a given function on pairs of elements in a vector. The procedure is iterated from left to right, and a single value is returned. This can be done from right to left by adding the argument &lt;code&gt;right=TRUE&lt;/code&gt;. As an example, consider division:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = function(x, y) x/y
x = seq(1, 3, by=1)
Reduce(f, x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1666667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Reduce(f, x, right=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the first case, &lt;code&gt;Reduce&lt;/code&gt; worked by dividing 1 by 2, then this result by 3. In the second case, this was in reverse, first dividing 3 by 2, then this result by 1.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Filter&lt;/code&gt; will ‘filter’ an array into values that satisfy the condition. For example&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = seq(1,5)
condition = function(x) x &amp;gt; 3
Filter(condition,x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Filter&lt;/code&gt; is similar to just indexing an array using &lt;code&gt;TRUE&lt;/code&gt;/&lt;code&gt;FALSE&lt;/code&gt; values, but instead of indexing using an array, it indexes using a function. However, we can inspect the interior of the function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Filter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (f, x) 
## {
##     ind &amp;lt;- as.logical(unlist(lapply(x, f)))
##     x[which(ind)]
## }
## &amp;lt;bytecode: 0x55bfc77914d8&amp;gt;
## &amp;lt;environment: namespace:base&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So infact, the function for &lt;code&gt;Filter&lt;/code&gt; simply uses &lt;code&gt;lapply&lt;/code&gt; to get the indices of the &lt;code&gt;TRUE&lt;/code&gt;/&lt;code&gt;FALSE&lt;/code&gt; values, and indexes the array for input &lt;code&gt;x&lt;/code&gt; with a simple subsetting.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parallel-computing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parallel Computing&lt;/h2&gt;
&lt;p&gt;By using the &lt;code&gt;parallel&lt;/code&gt; package, you can make use of all processing cores on your computer. Naturally, if you only have a single core processor, this is irrelevant, but most computers in the modern day have 2, 4, 8 or more cores. Parallel computing will allow R to run up to this many proccesses at the same time. A lot of important tasks in R can be sped up with parallel computing, for example MCMC. In MCMC, using &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; cores can allow you to also run &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; chains at once, with (in theory) no slowdown.&lt;/p&gt;
&lt;p&gt;Supercomputers generally have an extremely large number of cores, so being able to run code in parallel is important in computationally expensive programming jobs.&lt;/p&gt;
&lt;p&gt;There are some disadvantages to this: namely that splitting a process to four different cores will also require four times as much memory. If your memory isn’t sufficient for the amount of cores that you are using, this will cause a significant slowdown.&lt;/p&gt;
&lt;div id=&#34;using-mclapply-or-foreach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using &lt;code&gt;mclapply&lt;/code&gt; or &lt;code&gt;foreach&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;There are two main methods to parallelise a set of commands (or a function) in R. The first method is a parallel version of &lt;code&gt;apply&lt;/code&gt;, and the second method is a parallel version of &lt;code&gt;mclapply&lt;/code&gt;. To illustrate how these work, consider the example of an &lt;span class=&#34;math inline&#34;&gt;\(ARMA(1,1)\)&lt;/span&gt; model, which has an equation of the form
&lt;span class=&#34;math display&#34;&gt;\[
x_t = \epsilon_t + \alpha\epsilon_{t-1} +  \beta x_{t-1}
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\epsilon_t \sim  N(0, 1)
\]&lt;/span&gt;
A function that generates an &lt;span class=&#34;math inline&#34;&gt;\(ARMA(1,1)\)&lt;/span&gt; process can be written as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arma11 = function(alpha=0.5, beta=1, initx, N=1000){
  x = eps = numeric(N)
  x[1] = initx
  eps[1] = rnorm(1,0,1)
  eps[2] = rnorm(1,0,1)
  x[2] = eps[1] + alpha*eps[2] + beta*x[1]
  
  for(i in 3:N){
    eps[i] = rnorm(1,0,1)
    x[i] = eps[i] + alpha*eps[i-1] + beta*x[i-1]
  }
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will generate a vector of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; values for each timestep from &lt;span class=&#34;math inline&#34;&gt;\(t=1,\dots,N\)&lt;/span&gt;. We can see a plot of this generated time series by running a simulated &lt;span class=&#34;math inline&#34;&gt;\(ARMA\)&lt;/span&gt; timeseries of length &lt;span class=&#34;math inline&#34;&gt;\(N=1000\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(1:1000, arma11(initx=0.5,N=1000),type=&amp;quot;l&amp;quot;, xlab=&amp;quot;Time (t)&amp;quot;, ylab=expression(x), main=&amp;quot;Time Series Example&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/common_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that the functions are set up for testing, we can now set up the computer to work in parallel. This involves loading the required packages and detecting the number of cores we have available.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(parallel)
library(MASS)
no.cores = detectCores()
no.cores&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have this many cores to work with (on my laptop, there are 8 cores). The &lt;code&gt;no.cores&lt;/code&gt; variable will be passed into the parallel computing functions. We can now use &lt;code&gt;mclapply&lt;/code&gt; to simulate this &lt;span class=&#34;math inline&#34;&gt;\(ARMA\)&lt;/span&gt; model a large amount of times, and calculate the difference from the first value and the last value as a statistic. By putting the &lt;code&gt;arma11&lt;/code&gt; function in a wrapper, we can pass it through to &lt;code&gt;mclapply&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arma_wrapper = function(x) {
  y = arma11(alpha=1, beta=1, initx=x, N=1000)
  return(head(y,1) - tail(y,1))
}
xvals = rep(0.5,1000)
MCLoutput = unlist(mclapply(xvals, arma_wrapper, mc.cores = no.cores))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now, &lt;code&gt;MCLoutput&lt;/code&gt; is a vector, of length 1000, that contains the differences between the first and last value in a generated time series from an &lt;span class=&#34;math inline&#34;&gt;\(ARMA(1,1)\)&lt;/span&gt; model with &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(MCLoutput)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  -5.373153  -3.740134 -88.112201  53.205845  -4.410669  21.850910&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(MCLoutput)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.2121396&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this process is iterated at every time step, and ran 1000 times on top of that, it will be efficient for testing the efficiency of parallel computing. We can also construct a &lt;code&gt;foreach&lt;/code&gt; loop that will carry out the same task. The &lt;code&gt;foreach&lt;/code&gt; function is supplied by the &lt;code&gt;foreach&lt;/code&gt; library. It is similar to a &lt;code&gt;for&lt;/code&gt; loop but does not depend on each previous iteration of the loop. Instead, &lt;code&gt;foreach&lt;/code&gt; runs the contents of the loop in parallel a specified number of times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(foreach)
library(doParallel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: iterators&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;registerDoParallel(no.cores)
FEoutput = foreach(i=1:1000) %dopar% {
  y = arma11(initx=0.5,N=1000)
  head(y,1) - tail(y,1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;foreach&lt;/code&gt; loop that has been set up performs the same process as the &lt;code&gt;arma_wrapper&lt;/code&gt; function earlier at each iteration, it simulates an &lt;span class=&#34;math inline&#34;&gt;\(ARMA(1,1)\)&lt;/span&gt; process with &lt;span class=&#34;math inline&#34;&gt;\(N=1000\)&lt;/span&gt; time steps 1000 times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(unlist(FEoutput))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -22.303263  -7.588862  20.798046  41.559121  14.506028  29.758766&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(unlist(FEoutput))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.8435657&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that all of the parallel methods are set up, we can time them and compare them to not using parallel at all.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(mclapply(xvals, arma_wrapper, mc.cores = no.cores))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   5.749   0.835   2.056&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(foreach(i=1:1000) %dopar% {
  y = arma11(initx=0.5,N=1000)
  head(y,1) - tail(y,1)
})&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   6.023   0.882   1.847&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(for(i in 1:1000){
    arma11(initx=0.5,N=1000)
  })&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   4.795   0.007   4.802&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows that the fastest method is &lt;code&gt;mclapply&lt;/code&gt;, which is different from the normal case of &lt;code&gt;apply&lt;/code&gt; being slower than a simple loop. Both methods significantly sped up computation time against the non-parallel version.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Integrating R and C</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/rnc/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/rnc/</guid>
      <description>


&lt;p&gt;R can be interfaced with both C and C++, coming with a significant speed up in computation time and efficiency. R has an inbuilt system for calling C with the &lt;code&gt;.Call&lt;/code&gt; function, and RCpp can be used to integrate more easily with C++, enabling the use of pre-existing functions similar to those in base R. This portfolio will detail how this integration between R and C is possible effectively through the use of examples in a statistical sense.&lt;/p&gt;
&lt;div id=&#34;adaptive-kernel-regression-smoothing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adaptive Kernel Regression Smoothing&lt;/h2&gt;
&lt;p&gt;In the situation where we have some data which can not fit into a conventional linear model, we can employ the use of kernel smoothing to fit a more flexible model. More specifically, suppose the data has been generated from the following model
&lt;span class=&#34;math display&#34;&gt;\[
y_i = \sin(\alpha \pi x^3) + \epsilon_i, \qquad \text{ with } \qquad \epsilon_i \sim N(0, \sigma^2),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a uniformly random sample, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; are fixed parameters. This simulated data can be generated in R with parameter values &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 4\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma=0.2\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots, n\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(998)
n = 200
x = runif(n)
y = sin(4*pi*x^3) + rnorm(n, 0, 0.2)
plot(x, y, pch = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
From the plot it is clear that the data are non-linear, so a simple linear model would not be suitable. A kernel regression smoother can be used to estimate the conditional expectation &lt;span class=&#34;math inline&#34;&gt;\(\mu(x) = \mathbb{E}(y|x)\)&lt;/span&gt; more flexibly using
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}(x) = \frac{\sum^n_{i=1}\kappa_\lambda (x, x_i)y_i}{\sum^n_{i=1}\kappa_\lambda(x, x_i)},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt; is a kernel with bandwidth &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt;. This kernel regression can be written in R as well as C, so it provides opportunity for comparison between the two languages.&lt;/p&gt;
&lt;div id=&#34;writing-a-function-in-c&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Writing a function in C&lt;/h3&gt;
&lt;p&gt;We can write a C function to implement a Gaussian kernel with variance &lt;span class=&#34;math inline&#34;&gt;\(\lambda^2\)&lt;/span&gt;, shown below.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;#include &amp;lt;R.h&amp;gt;
#include &amp;lt;Rinternals.h&amp;gt;
#include &amp;lt;Rmath.h&amp;gt;

SEXP meanKRS(SEXP y, SEXP x, SEXP x0, SEXP lambda)
{
  
  int n, n0;
  double lambda0;
  double *outy, *x00, *xy, *y0;
  
  n0 = length(x0);
  
  x = PROTECT(coerceVector(x, REALSXP));
  y = PROTECT(coerceVector(y, REALSXP));
  SEXP out = PROTECT(allocVector(REALSXP, n0));
  
  n = length(x);
  
  outy = REAL(out);
  lambda0 = REAL(lambda)[0];
  x00 = REAL(x0);
  xy = REAL(x);
  y0 = REAL(y);
  
  for(int i=0; i&amp;lt;n0; i++)
  {
    double num = 0, den = 0;
    for(int j=0; j&amp;lt;n; j++)
    {
      num += dnorm(xy[j], x00[i], lambda0, 0)*y0[j];
      den += dnorm(xy[j], x00[i], lambda0, 0);
    }
    outy[i] = num/den;
  }
  
  UNPROTECT(3);
  
  return out;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a large, and quite complicated function to what would normally be implemented in R. The main reason for the length of the function is due to the amount of declarations that have to be made in C. At the start, all integers and doubles are declared, with a &lt;code&gt;*&lt;/code&gt; signifying that these are pointers, which instead point to a location in memory rather than being a variable themselves. &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; are all defined as vectors; either defining the inputs or allocating a new vector, with &lt;code&gt;PROTECT&lt;/code&gt; signifying that these locations in memory are protected. Then the pointers are set up with the &lt;code&gt;REAL&lt;/code&gt; function deciding where to point, and the &lt;code&gt;[0]&lt;/code&gt; index meaning to take the value of where is being pointed to instead of setting up a pointer.&lt;/p&gt;
&lt;p&gt;After this, the actual kernel smoothing is calculated within two &lt;code&gt;for&lt;/code&gt; loops, as there is no global definition for &lt;code&gt;sum&lt;/code&gt; in C. The numerator &lt;code&gt;num&lt;/code&gt; and denominator &lt;code&gt;den&lt;/code&gt; are defined on each iteration of the outer loop, and they are added to themselves in the inner loop. Once the inner loop iterations are complete, the value of &lt;code&gt;out&lt;/code&gt; is assigned as the division of these two (which is pointed to by &lt;code&gt;outy&lt;/code&gt;). Finally, the protected vectors are unprotected, to free up memory.&lt;/p&gt;
&lt;p&gt;The headers at the beginning of the file, written as &lt;code&gt;#include &amp;lt;header.h&amp;gt;&lt;/code&gt; allow the inclusion of certain pre-written functions that make writing C code simpler. Using the &lt;code&gt;R&lt;/code&gt;, &lt;code&gt;Rinternals&lt;/code&gt; and &lt;code&gt;Rmath&lt;/code&gt; were not all necessary, however &lt;code&gt;Rmath&lt;/code&gt; provided the &lt;code&gt;dnorm&lt;/code&gt; function that was necessary for the Gaussian kernel.&lt;/p&gt;
&lt;p&gt;Also note that the use of a double &lt;code&gt;for&lt;/code&gt; loop is not inefficient in C as it would be in R, so there is no significant slowdown coming from nested loops. Now the function is written, it can be saved into the current working directory and loaded into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(&amp;quot;R CMD SHLIB meanKRS.c&amp;quot;)
dyn.load(&amp;quot;meanKRS.so&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A wrapper function can be set up to clean up code, i.e. an R function can be made which explicitly calls the C function with the same arguments. After this, we can plot the results to see how the smoothing looks with an arbitrary value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^2 = 0.02\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanKRS = function(y, x, x0, lambda) .Call(&amp;quot;meanKRS&amp;quot;, y, x, x0, lambda)
plot(x, y, pch = 20)
lines(seq(0, 1, len=1000), meanKRS(y, x, seq(0,1,len=1000), lambda = 0.02), col=&amp;quot;blue&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So this kernel smoothing approach has provided a nice fit to the simulated data, and appears to be going through the centre of the points across the plot.&lt;/p&gt;
&lt;p&gt;To compare the computational efficiency, we can compare this C function with a simpler function written in R, called &lt;code&gt;meanKRS_R&lt;/code&gt; (function not shown here for brevity, the R function performs the same operation whilst making use of R’s &lt;code&gt;sum&lt;/code&gt; function instead of having a loop).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(meanKRS_R(y,x,seq(0,1,len=1000),0.06), meanKRS(y,x,seq(0,1,len=1000),0.06))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the values are exactly the same, and the function is accurate. How much quicker is it? We can use &lt;code&gt;microbenchmark&lt;/code&gt; to display summary statistics of timing both functions 100 times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
microbenchmark(C_KRS = meanKRS(y,x,seq(0,1,len=1000),0.06),
               R_KRS = meanKRS_R(y,x,seq(0,1,len=1000),0.06), times = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##   expr      min       lq     mean   median       uq      max neval
##  C_KRS 14.32545 16.17882 16.36242 16.27690 16.42271 20.37999   100
##  R_KRS 30.25504 32.08685 33.45924 32.42864 34.38512 43.47946   100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the C code is significantly faster, as expected, by about a factor of two.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementing cross-validation&lt;/h3&gt;
&lt;p&gt;The value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; used here was chosen arbitrarily, but in practice it is common to use &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-fold cross-validation to choose a more optimal value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. We can implement a cross-validation routine in C. Implementing a function within a function is difficult in C, but it is relatively straightforward in Rcpp. This example goes through the use of Rcpp to implement cross-validation.&lt;/p&gt;
&lt;p&gt;Rcpp is a package in R which provides tools to implement C++ code in R. It works in a similar way to how the &lt;code&gt;.Call&lt;/code&gt; interface works for C code, but with a more accessible interface. Firstly, we create a file in the working directory called &lt;code&gt;KRS_cv.cpp&lt;/code&gt;, where the Rcpp functions will be stored. This file contains a few functions, the first of which being the &lt;code&gt;meanKRS&lt;/code&gt; function re-written for Rcpp.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;// [[Rcpp::export(name = &amp;quot;meanKRS&amp;quot;)]]
NumericVector meanKRS_I(const NumericVector y, const NumericVector x, 
                        const NumericVector x0, const double lambda)
{
  int n, n0;
  n0 = x0.size();
  n = x.size();
  NumericVector out(n0);
  for(int i=0; i&amp;lt;n0; i++)
  {
    double num = 0, den = 0;
    NumericVector dval = dnorm(x, x0[i], lambda, 1);
    double max_dval = max(dval);
    for(int j=0; j&amp;lt;n; j++)
    {
      num = num + exp(dval[j]-max_dval)*y[j];
      den = den + exp(dval[j]-max_dval);
    }
    out[i] = num/den;
  }
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The cross-validation function &lt;code&gt;cvKRS&lt;/code&gt; is written as:&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;// [[Rcpp::export(name = &amp;quot;cvKRS&amp;quot;)]]
NumericVector cvKRS_I(const NumericVector y, const NumericVector x, 
                      const int k, const NumericVector lambdas)
{
  // declare variables and vectors
  int n = y.size();
  NumericVector mse_lambda(lambdas.size());
  NumericVector out(1);
  NumericVector sorted_x(n);
  NumericVector sorted_y(n);
  
  // sort x and y according to the order of x
  IntegerVector order_x = stl_order(x);
  for(int kk=0; kk &amp;lt; n; kk++)
  {
    int ind = order_x[kk]-1;
    sorted_y[kk] = y[ind];  
    sorted_x[kk] = x[ind];
  }
  
  // set up indices to cross-validate for
  IntegerVector idxs = seq_len(k);
  IntegerVector all_idxs = rep_each(idxs, n/k);

  // different lambdas
  for(int jj = 0; jj &amp;lt; lambdas.size(); jj++)
  {
    double lambda = lambdas[jj];
    NumericVector mse(k);
    
    // cross-validation loop
    for(int ii=1; ii &amp;lt;= k; ii++)
    {
      const LogicalVector kvals = all_idxs != ii;
      
      NumericVector y_t = clone(sorted_y);
      NumericVector x_t = clone(sorted_x);
      NumericVector y_cross = y_t[kvals];
      NumericVector x_cross = x_t[kvals];
      NumericVector fit = meanKRS_I(y_cross, x_cross, sorted_x, lambda);
      
      // calculate mean squared error
      NumericVector error = pow((fit[!kvals] - sorted_y[!kvals]), 2);
      mse[ii-1] = mean(error);
    }
    
    // average mean squared error for each value of lambda
    mse_lambda[jj] = mean(mse);
  }
  
  // output lambda which gave the smallest mean squared error
  int best_pos = which_min(mse_lambda);
  out[0] = lambdas[best_pos];
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comments within the function (succeeding a &lt;code&gt;//&lt;/code&gt;) give explanation of each section of the function. This function also calls the other function &lt;code&gt;meanKRS&lt;/code&gt; by using &lt;code&gt;meanKRS_I&lt;/code&gt;. The function was defined as &lt;code&gt;meanKRS_I&lt;/code&gt; in the &lt;code&gt;.cpp&lt;/code&gt; file, but when exported into R it is defined as &lt;code&gt;meanKRS&lt;/code&gt;, based on the comment preceeding the function definition. Both functions were defined within the same file, and so can call one another, provided the function being called is defined first. There is a function used within &lt;code&gt;cvKRS&lt;/code&gt; that is not part of base Rcpp functionality: &lt;code&gt;stl_order&lt;/code&gt;. This does the same thing as &lt;code&gt;order&lt;/code&gt; in base R, but needed to be defined separately.&lt;/p&gt;
&lt;p&gt;Now that the function is defined, we can call it within R by first loading the &lt;code&gt;Rcpp&lt;/code&gt; library and then using the &lt;code&gt;sourceCpp&lt;/code&gt; function to source the C++ file, located in the same working directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
sourceCpp(&amp;quot;KRS_cv.cpp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the stage where we would get compilation errors, if there were any. Now we can run the function over a series of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values and plot the kernel regression over the simulated data for the optimal &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; selected by cross-validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda_seq = exp(seq(log(1e-6),log(100),len=50))
best_lambda = cvKRS(y, x, k = 20, lambdas = lambda_seq)
plot(x, y, pch = 20)
lines(seq(0, 1, len=1000), meanKRS(y, x, seq(0,1,len=1000), best_lambda), 
      col=&amp;quot;deeppink&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
To compare computational efficiency, we can benchmark speeds against a function written in R (not shown here).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(cvKRS(y, x, k = 20, lambdas = lambda_seq),
               cvKRS_R(y, x, k = 20, lambdas = lambda_seq), 
               times = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: seconds
##                                         expr      min       lq     mean
##    cvKRS(y, x, k = 20, lambdas = lambda_seq) 2.289333 2.293236 2.660317
##  cvKRS_R(y, x, k = 20, lambdas = lambda_seq) 6.390262 6.629278 7.100683
##    median       uq      max neval
##  2.380029 3.141275 3.197711     5
##  7.481269 7.486666 7.515939     5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this further exemplifies the significant speed increase that comes with writing in C, or Rcpp. In fact, this function had an average of three times the speed of that of the R function, against the two times speed up the previous function had. This is likely due to using two functions written in C++ (as cross validation calls the original kernel regression function).&lt;/p&gt;
&lt;p&gt;This has improved the fit, but no single value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; leads to a satisfactory fit, due to the first half of the function (for &lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; 0.5\)&lt;/span&gt;) wanting a smooth &lt;span class=&#34;math inline&#34;&gt;\(\mu(x)\)&lt;/span&gt; as it is quite linear, and the second half wanting a less smooth one, as it is more ‘wiggly’. We can let the smoothness depend on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by constructing &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt;, which will improve the fit. This method is based around modelling the residuals and varying &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; more when the residuals are larger. Thus in practice &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt; is a sequence of values instead of a single value. Below are the contents of the file that contains the functions to implement this written in Rcpp.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

NumericVector fitKRS(const NumericVector x, const NumericVector x0, 
                     const double lambda, const NumericVector y, 
                     const NumericVector lambda_vec)
{
  NumericVector copy_y = clone(y);
  int n = x.size();
  int n0 = x0.size();
  NumericVector out(n0);
  for(int ii=0; ii&amp;lt;n0; ii++)
  {
    NumericVector dval = dnorm(x, x0[ii], lambda*lambda_vec[ii], 1);
    double max_dval = max(dval);
    double num=0, den=0;
    for(int jj=0; jj&amp;lt;n; jj++)
    {
      num = num + exp(dval[jj] - max_dval)*copy_y[jj];
      den = den + exp(dval[jj] - max_dval);
    }
    out[ii] = num/den;
  }
  return out;
}

// [[Rcpp::export(name = &amp;quot;mean_var_KRS&amp;quot;)]]
NumericVector mean_var_KRS_I(const NumericVector y, const NumericVector x, 
                             const NumericVector x0, const double lambda)
{
  int n = x.size();
  int n0 = x0.size();
  NumericVector res(n);
  NumericVector lambda_1sn(n, 1.0);
  NumericVector lambda_1sn0(n0, 1.0);
  NumericVector mu = fitKRS(x, x, lambda, y, lambda_1sn);
  NumericVector resAbs = abs(y - mu);
  NumericVector madHat = fitKRS(x, x0, lambda, resAbs, lambda_1sn0);
  NumericVector w = 1 / madHat;
  w = w / mean(w);
  NumericVector out = fitKRS(x, x0, lambda, y, w);

  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first function, &lt;code&gt;fitKRS&lt;/code&gt; was used to save space, since the same operation is performed multiple times with different parameters. Different weightings &lt;code&gt;w&lt;/code&gt; get added to the vector of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values in the final stage, resulting in the varied &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt; parameter. We can plot this to show this change, using the initial value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; selected by cross-validation earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;quot;meanvarKRS.cpp&amp;quot;)
varied_mu = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = best_lambda)
plot(x, y, pch=20)
lines(seq(0, 1, len=1000), varied_mu, col = &amp;quot;darkgoldenrod&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
This looks like a good fit, and the function is working as intended. How well does the speed of the function written in Rcpp compare to one written in R?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(C = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), 
                                lambda = best_lambda),
               R = mean_var_KRS_R(y = y, x = x, x0 = seq(0,1,len=1000), 
                                  lam = best_lambda), times = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##  expr      min       lq     mean   median       uq       max neval
##     C 25.56793 35.80579 41.67632 39.84551 45.09670  84.63745   500
##     R 35.03011 54.59121 64.87276 61.94020 70.77707 122.68250   500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has an expected speed up again. And to make sure both functions are outputting the same thing, we can use &lt;code&gt;all.equal&lt;/code&gt; again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), 
                              lambda = best_lambda), 
          mean_var_KRS_R(y = y, x = x, x0 = seq(0,1,len=1000), 
                             lam = best_lambda))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation-again&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cross Validation Again&lt;/h3&gt;
&lt;p&gt;The value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; used for fitting this local regression is that picked from cross-validation where &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; does not vary with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, in the previous section. To improve this fit further, we can use cross-validation again, but fitting the model with the new &lt;code&gt;mean_var_KRS&lt;/code&gt; function on each iteration instead of the basic kernel regression. The function is written in Rcpp below.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;// [[Rcpp::export(name = &amp;quot;mean_var_cv_KRS&amp;quot;)]]
NumericVector mean_var_cv_KRS_I(const NumericVector y, const NumericVector x, 
                      const int k, const NumericVector lambdas)
{
  int n = y.size();
  NumericVector mse_lambda(lambdas.size());
  NumericVector out(1);
  NumericVector sorted_x(n);
  IntegerVector order_x = stl_order(x);
  NumericVector test;
  NumericVector sorted_y(n);
  for(int kk=0; kk &amp;lt; n; kk++)
  {
    int ind = order_x[kk]-1;
    sorted_y[kk] = y[ind];  
    sorted_x[kk] = x[ind];
  }
  IntegerVector idxs = seq_len(k);
  IntegerVector all_idxs = rep_each(idxs, n/k);
  
  for(int jj = 0; jj &amp;lt; lambdas.size(); jj++)
  {
    double lambda = lambdas[jj];
    NumericVector mse(k);
    for(int ii=1; ii &amp;lt;= k; ii++)
    {
      const LogicalVector kvals = all_idxs != ii;
      NumericVector y_t = clone(sorted_y);
      NumericVector x_t = clone(sorted_x);
      NumericVector y_cross = y_t[kvals];
      NumericVector x_cross = x_t[kvals];
      NumericVector fit = mean_var_KRS_I(y_cross, x_cross, sorted_x, lambda);
      NumericVector error = pow((fit[!kvals] - sorted_y[!kvals]), 2);
      mse[ii-1] = mean(error);
    }
    mse_lambda[jj] = mean(mse);
  }
  int best_pos = which_min(mse_lambda);
  out[0] = lambdas[best_pos];
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This functions is very similar to the cross-validation function implemented earlier. It loops over different values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, but instead uses these as a starting point to fit a series of points using &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt;. The error is calculated against the cross-validated points and the starting &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; that results in the smallest error is returned. Now we can call this in R, and re-run the &lt;code&gt;mean_var_KRS&lt;/code&gt; function with the newly selected &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and see how it compares.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;quot;meanvarcvKRS.cpp&amp;quot;)
cv_best_lambda = mean_var_cv_KRS(y, x, k=20, exp(seq(log(0.01), log(1), len=50)))
cv_varied_mu = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = cv_best_lambda)
plot(x, y, pch=20)
lines(seq(0, 1, len=1000), cv_varied_mu, col = &amp;quot;darkorchid&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
This looks a lot smoother for lower values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; than before, which looks like a better fit overall!&lt;/p&gt;
&lt;p&gt;A way to improve this model might involve a &lt;em&gt;local&lt;/em&gt; regression approach, which would be fitting parameter values at different (possibly uniform) intervals across the data set. Local regression will be covered in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Local Polynomial Regression with Rcpp</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/rcpp/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/rcpp/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Rcpp &lt;em&gt;sugar&lt;/em&gt; and &lt;em&gt;Armadillo&lt;/em&gt; are libraries included in Rcpp that allow different processes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Sugar&lt;/em&gt; provides an array of basic functions in Rcpp that are similar to inbuilt base R functions. These include functions such as &lt;code&gt;cbind&lt;/code&gt;, &lt;code&gt;sum&lt;/code&gt;, &lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;sample&lt;/code&gt; and many more. These are essential for writing simple code in Rcpp.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Armadillo&lt;/em&gt; is a package used for linear algebra operations in Rcpp. It allows specification of matrices, 3D matrices, vectors and others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This portfolio will explain the use of key functions and features of &lt;em&gt;Armadillo&lt;/em&gt; and &lt;em&gt;sugar&lt;/em&gt; by implementing local polynomial regression on a data set on solar electricity production in Sydney. We can load this data into the R environment first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;solarAU.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(solarAU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       prod          toy tod   logprod
## 8832 0.019 0.000000e+00   0 -3.540459
## 8833 0.032 5.708088e-05   1 -3.170086
## 8834 0.020 1.141618e-04   2 -3.506558
## 8835 0.038 1.712427e-04   3 -3.036554
## 8836 0.036 2.283235e-04   4 -3.079114
## 8837 0.012 2.854044e-04   5 -3.816713&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column &lt;code&gt;prod&lt;/code&gt; is a production measure, and &lt;code&gt;logprod&lt;/code&gt; is the log of the production measure, which will be the response variable. The covariates are &lt;code&gt;toy&lt;/code&gt; - the time of year, within 0 and 1 as a percentage, and &lt;code&gt;tod&lt;/code&gt; - the time of day, from 0 to 47, measured in half an hour intervals. A simple polynomial regression model is of the form
&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}(y|\boldsymbol{x}) = \beta_0 + \beta_1\text{tod} + \beta_2\text{tod}^2 + \beta_3\text{toy} + \beta_4 \text{toy}^2 = \tilde{\boldsymbol{x}}\boldsymbol{\beta}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-linear-regression-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a linear regression model&lt;/h1&gt;
&lt;p&gt;We can use &lt;em&gt;Armadillo&lt;/em&gt; to fit a linear regression model, and to solve the least squares optimisation problem
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\boldsymbol{\beta}} := \operatorname{argmin}_{\boldsymbol{\beta}}\|{\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\|}^2,
\]&lt;/span&gt;
which has solution
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{\hat{\beta}} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}.
\]&lt;/span&gt;
This can be implemented in C using QR decomposition of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
sourceCpp(code=&amp;#39;
// [[Rcpp::depends(RcppArmadillo)]]
#include &amp;lt;RcppArmadillo.h&amp;gt;
#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp::export(name=&amp;quot;lm_cpp&amp;quot;)]]
arma::vec lm_cpp_I(arma::vec&amp;amp; y, arma::mat&amp;amp; X)
{
  arma::mat Q, R;
  arma::qr_econ(Q, R, X);
  arma::vec beta = solve(R, (trans(Q) * y));
  return beta;
}&amp;#39;)
ls = function(formula, data){
  y = data[,all.vars(formula)[1]]
  x = model.matrix(formula, data)
  lm_cpp(y, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;sourceCpp&lt;/code&gt; function has a differing argument &lt;code&gt;code&lt;/code&gt;, where instead of reading code from a file in the directory, the code is supplied directly here. Running &lt;code&gt;sourceCpp&lt;/code&gt; adds the &lt;code&gt;lm_cpp&lt;/code&gt; function to the environment. An R function is set up which takes the inputs &lt;code&gt;formula&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt; and runs the Rcpp function with the given inputs. This makes it comparable to &lt;code&gt;lm&lt;/code&gt;. Firstly though, we can see that both functions output the same parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]
## [1,] -6.26275685
## [2,]  0.86440391
## [3,] -0.01757599
## [4,] -5.91806924
## [5,]  6.14298863&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)         tod    I(tod^2)         toy    I(toy^2) 
## -6.26275685  0.86440391 -0.01757599 -5.91806924  6.14298863&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But which is faster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(
  R_lm = lm(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU),
  C_lm = ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU),
  times = 500
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##  expr      min       lq     mean   median       uq      max neval
##  R_lm 3.531021 4.047208 5.079068 4.205777 5.222140 57.54048   500
##  C_lm 2.522135 2.927327 3.615145 3.026044 3.654492 37.26725   500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the C code is approximately twice as fast with this method, and would be even faster without the R wrapper function. However, the R function &lt;code&gt;lm&lt;/code&gt; performs a number of checks and computes a number of statistics that the C++ code does not, which explains part of the performance gap.&lt;/p&gt;
&lt;p&gt;For a more fair comparison, we can set up the model matrices in advance, and perform the same operations in R and in RCpp.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R_ls_solve = function(y,X){
  QR = qr(X)
  beta = solve(qr.R(QR), (t(qr.Q(QR)) %*% y))
  return(beta)
}
y = solarAU$logprod
X = with(solarAU, cbind(1, tod, tod^2, toy, toy^2))
microbenchmark(R_solve = R_ls_solve(y,X),
               C_solve = lm_cpp(y, X), 
               times = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: microseconds
##     expr      min        lq      mean   median        uq       max neval
##  R_solve 1910.686 2189.3570 5022.8070 2464.270 3779.8105 79083.490   500
##  C_solve  449.401  518.7625  690.3906  584.342  687.4495  8261.433   500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this has come with an approximate speed up of ten times, exemplifying how much more computationally efficient code written in C++ is over R. However, there is a (non-computational) problem with the model. A plot of the residuals from the linear model shows this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta = ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU)
res = y - X %*% beta
predplot = ggplot(solarAU, mapping = aes(x=toy, y=tod, z= X%*%beta)) +
  stat_summary_hex() +  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) +
  theme(legend.title = element_blank())
resplot = ggplot(solarAU, mapping=aes(x=toy, y = tod, z = res)) +
  stat_summary_hex() +  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) +
  theme(legend.title = element_blank())
grid.arrange(predplot, resplot, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc2/rcpp_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
There is a pattern in the residuals! This means that there is a feature not included in the model that can explain some of the noise. We need to extend this model to account for this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;local-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Local Regression&lt;/h1&gt;
&lt;p&gt;We can improve the model fit by adopting a local regression approach, that is, making the parameter estimates depend on the covariates &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\hat{\boldsymbol{\beta}} = \hat{\boldsymbol{\beta}}(\boldsymbol{x})\)&lt;/span&gt;. This is achieved by minimising &lt;span class=&#34;math inline&#34;&gt;\(\hat{\boldsymbol{\beta}}(\boldsymbol{x}_0)\)&lt;/span&gt; for a fixed &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}_0\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\boldsymbol{\beta}}(\boldsymbol{x}_0) = \operatorname{argmin}_{\boldsymbol{\beta}} \sum^n_{i=1}\kappa_{\boldsymbol{H}}(\boldsymbol{x}_0 - \boldsymbol{x}_i)(y_i-\tilde{\boldsymbol{x}}_i^T\boldsymbol{\beta})^2,
\]&lt;/span&gt;
for a density kernel &lt;span class=&#34;math inline&#34;&gt;\(\kappa_{\boldsymbol{H}}\)&lt;/span&gt; with positive definite bandwidth matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{H}\)&lt;/span&gt;. Fitting this model involves re-fitting the linear model once for each row in the data set, which for large data sets is not viable, but shows the need of computationally efficient code in C++. Now we can write the local regression function in RCpp.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;vec local_lm_I(vec&amp;amp; y, rowvec x0, rowvec X0, mat&amp;amp; x, mat&amp;amp; X, mat&amp;amp; H)
{
  mat Hstar = chol(H, &amp;quot;lower&amp;quot;); 
  vec w = dmvnInt(x, x0, Hstar);
  vec beta = lm_cpp_I(y % sqrt(w), X.each_col() % sqrt(w));
  return X0 * beta;
}

// [[Rcpp::export(name=&amp;quot;local_lm_fit&amp;quot;)]]
vec local_lm_fit_I(vec&amp;amp; y, mat x0, mat X0, mat&amp;amp; x, mat&amp;amp; X, mat&amp;amp; H)
{
  int n0 = x0.n_rows;
  vec out(n0);
  for(int ii=0; ii &amp;lt; n0; ii++)
  {
    rowvec x00 = x0.row(ii);
    rowvec X00 = X0.row(ii);
    out(ii) = as_scalar(local_lm_I(y, x00, X00, x, X, H));
    if(ii % 50 == 0) {R_CheckUserInterrupt();}
  }
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These two functions, as well as &lt;code&gt;lm_cpp&lt;/code&gt; from earlier all combine to implement local regression. &lt;code&gt;local_lm_fit_I&lt;/code&gt; loops over all rows in &lt;code&gt;x0&lt;/code&gt; and &lt;code&gt;X0&lt;/code&gt; and fits linear regression for a constant &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}_0\)&lt;/span&gt;, using weights from a Gaussian kernel, imeplemented by a multivariate normal density (given by &lt;code&gt;dmvnInt&lt;/code&gt;, defined separately). &lt;code&gt;local_lm_I&lt;/code&gt; simply calculates the weights and pre-multiplies them by &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X}\)&lt;/span&gt; to go into the fitting function. We can source these functions and run these for a subsetted data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;#39;lm_cpp.cpp&amp;#39;)
nsub = 2000

sub = sample(1:nrow(solarAU), nsub)
y = solarAU$logprod
x = as.matrix(solarAU[c(&amp;quot;tod&amp;quot;, &amp;quot;toy&amp;quot;)])
X = model.matrix(~tod+toy+I(tod^2)+I(toy^2),data=solarAU)
x0 = x[sub, ]
X0 = X[sub, ]
H = diag(c(1,0.01))

cpp_pred_local = local_lm_fit(y, x0, X0, x, X, H)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of which we can plot, for both the predictions and the residuals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predPlot = ggplot(mapping=aes(x=x0[,2], y = x0[,1], z = cpp_pred_local)) + stat_summary_hex() +
  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) + theme(legend.title = element_blank())
resPlot = ggplot(mapping=aes(x=x0[,2], y = x0[,1], z = y[sub] - cpp_pred_local)) + stat_summary_hex() +
  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) + theme(legend.title = element_blank())
grid.arrange(predPlot, resPlot, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc2/rcpp_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
These look a lot better! There isn’t a systematic pattern in these residuals which show that the model isn’t missing anything important. Now we can check this C++ code against the basic R code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm)
lmLocal &amp;lt;- function(y, x0, X0, x, X, H){
  w &amp;lt;- dmvnorm(x, x0, H)
  fit &amp;lt;- lm(y ~ -1 + X, weights = w)
  return( t(X0) %*% coef(fit) )
}
R_pred_local &amp;lt;- sapply(1:nsub, function(ii){
  lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H = diag(c(1, 0.1)^2))
})

all.equal(R_pred_local, as.vector(cpp_pred_local))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since all these predictions are equal in R and Rcpp, how does the speed difference compare? This function takes a long time to run, so we will only time each of them once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(sapply(1:nsub, function(ii){
  lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H = diag(c(1, 0.1)^2))
}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##  23.527   0.076  23.606&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(
  local_lm_fit(y, x0, X0, x, X, H)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
## 872.113   0.309 145.901&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the Rcpp code has come with an approximate ten times speed up again. However, this model could still be improved. We have chosen the bandwidth matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{H}\)&lt;/span&gt; arbitrarily, but this could be improved with cross-validation. Whilst this is not shown here, a similar approach to that given in section 2 could be implemented.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reproducibility</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/reproducibility/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/reproducibility/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The first aspect to being a good programmer is to ensure that your code is &lt;strong&gt;reproducible&lt;/strong&gt;. That is, if an academic unrelated to you, on the other side of the world, were to have access to your conclusions and your dataset, would they be able to reproduce your results? It is important that this is the case, otherwise it may take weeks or even months of effort for someone to catch up to the research that you have already carried out, which makes it difficult for that person to then build on the existing work.&lt;/p&gt;
&lt;p&gt;If you have supplied your code with any publication you have created, it is also important you follow some basic &lt;strong&gt;literate programming&lt;/strong&gt; guidelines. In short, this ensures that whoever reads your code will be able to figure out what it does without too much effort. This can be done with documentation, proper commenting, or writing an Markdown document.&lt;/p&gt;
&lt;p&gt;It is surprising nowadays how many scientific articles are released without some form of code alongside them. If code is released alongside a piece of research, every academic who reads that research should be able to reproduce the results in the article without a large amount of hassle. This would allow other scientists to build on the research, and advance the scientific community as a whole. If you don’t include your code however, your research may never be built on, as you would be making it more difficult for someone to enter your research field.&lt;/p&gt;
&lt;p&gt;An example of literate programming and reproducibility is given below. A least square solver has been implemented on a Prostate Cancer dataset. Firstly, the example goes through the mathematics of least squares estimation, then an example is given on how to code this in R. Each step is explained, and through explaining these steps, it should be possible to reproduce the code through a greater understanding.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibility-example-implementing-a-least-square-solver-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reproducibility Example: Implementing a least square solver in R&lt;/h1&gt;
&lt;p&gt;A least squares estimator will provide coefficients that make up a function which can be used to predict some form of an output. Calling the output &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y}\)&lt;/span&gt;, the series of inputs &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x} = (\boldsymbol{x}_1, \boldsymbol{x}_2, \dots, \boldsymbol{x}_n)^T\)&lt;/span&gt;, and the coefficients to be estimated as &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{w} = (w_0, \boldsymbol{w_1})\)&lt;/span&gt;. Here, &lt;span class=&#34;math inline&#34;&gt;\(w_0\)&lt;/span&gt; is the intercept, or the bias parameter and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the length of the input vector. The equation that we need to solve is:
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{w}_{LS} := \text{argmin}_{\boldsymbol{w}} \sum_{i \in D_0} \left\{y_i - f(\boldsymbol{x}_i; \boldsymbol{w})\right\}^2.
\]&lt;/span&gt;
This has a solution of
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{w}_{LS} = (\boldsymbol{XX}^T)^{-1}\boldsymbol{Xy}^T.
\]&lt;/span&gt;
Which will minimise the squared error between the actual output values &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y}\)&lt;/span&gt; and the expected output values given by the input values &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;prostate-cancer-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prostate Cancer Data&lt;/h2&gt;
&lt;p&gt;To give an example of using a least squares solver, we can use prostate cancer data given by the &lt;code&gt;lasso2&lt;/code&gt; package. If this package is not installed, we can use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;lasso2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to install it. Once this package is installed, we obtain the dataset by running the commands&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(lasso2)
data(Prostate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives us the dataset in our R environment. We start by inspecting the dataset, which can be done with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(Prostate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
## 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
## 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
## 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
## 6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see all the different elements of the dataset, which are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lcavol&lt;/code&gt;: The log of the cancer volume&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lweight&lt;/code&gt;: The log of the prostate weight&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: The individual’s age&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lbph&lt;/code&gt;: The log of the benign prostatic hyperplasia amount&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;svi&lt;/code&gt;: The seminal vesicle invasion&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lcp&lt;/code&gt;: The log of the capsular penetration&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gleason&lt;/code&gt;: The Gleason score&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pgg45&lt;/code&gt;: The percentage of the Gleason scores that are 4 or 5&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lpsa&lt;/code&gt;: The log of the prostate specific antigen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We don’t need to understand what all of these are, but we need to define what the inputs and outputs are. We are interested in the cancer volume, so setting &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y}\)&lt;/span&gt; as &lt;code&gt;lcavol&lt;/code&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt; as the other variables would measure how these other variables affect cancer volume. We can set this by running&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = as.vector(Prostate$lcavol)
X = model.matrix(~., data=Prostate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives us our inputs and outputs. We are selecting the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix going from 2 to &lt;code&gt;dim(Prostate)[2]&lt;/code&gt;, which means that we are selecting the Prostate dataset from the second column to the last one (not including the response as a predictor). If our output variable was in a different column, we would exclude a different column using similar methods.&lt;/p&gt;
&lt;p&gt;Now that we have set up the inputs and outputs, we can use the solution
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{w}_{LS} = (\boldsymbol{XX}^T)^{-1}\boldsymbol{Xy}^T.
\]&lt;/span&gt;
to find the coefficients &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{w_{LS}}\)&lt;/span&gt;. Converting this equation into R is done with the command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wLS = solve(t(X) %*% X) %*% t(X) %*% y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;t(X)&lt;/code&gt; function simply takes the transpose of the matrix. The &lt;code&gt;%*%&lt;/code&gt; function is just selecting matrix multiplication, as opposed to using &lt;code&gt;*&lt;/code&gt; which would be element-wise multiplication. The &lt;code&gt;solve&lt;/code&gt; function finds the inverse of a matrix, so &lt;code&gt;solve(X)&lt;/code&gt; would give &lt;span class=&#34;math inline&#34;&gt;\(X^{-1}\)&lt;/span&gt;. Now we have our coefficients&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wLS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      [,1]
## (Intercept) -2.997602e-14
## lcavol       1.000000e+00
## lweight     -1.214306e-14
## age         -1.049508e-16
## lbph         6.045511e-16
## svi         -2.952673e-14
## lcp          4.538037e-15
## gleason      1.547373e-15
## pgg45       -2.255141e-17
## lpsa         1.706968e-15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can be used to get the predicting function &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{x};\boldsymbol{w})\)&lt;/span&gt; for these given inputs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = X %*% wLS&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see how close the predicted output is to the actual output with a plot. If our predictions are accurate, then they will be close to the actual output, and lie on a diagonal line. We can plot these two values against each other using the command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(f,y,xlab = &amp;quot;Predicted Values&amp;quot;, ylab = &amp;quot;Actual Values&amp;quot;, main = &amp;quot;Log of Cancer Volume&amp;quot;)
abline(0,1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/reproducibility_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
These match up almost perfectly, so our predictions are accurate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-validation&lt;/h2&gt;
&lt;p&gt;Cross-validation is a general purpose method for evaluating the method of the predicting function &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{x};\boldsymbol{w})\)&lt;/span&gt;. This is done by leaving &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; points out of the dataset, fitting the coefficients to the remaining dataset, then using the new coefficients to create a new predicting function which can predict the data point that was missed out. The difference between this predicted point and the actual observed point can be used as a metric to judge the accuracy of the predicting function.&lt;/p&gt;
&lt;p&gt;In more technical terms, we split the dataset &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; into &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; distinct subsets &lt;span class=&#34;math inline&#34;&gt;\(D_1, \dots, D_k\)&lt;/span&gt;, and fit &lt;span class=&#34;math inline&#34;&gt;\(f_{-i}(\boldsymbol{x}_{-i};\boldsymbol{w})\)&lt;/span&gt; for each dataset and for &lt;span class=&#34;math inline&#34;&gt;\(i=0,\dots k\)&lt;/span&gt;. We then take the squared error between &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{x}_{i};\boldsymbol{w})\)&lt;/span&gt;, and average across all subsets, i.e.
&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{k+1} \sum^k_{i=1} \left[ f(\boldsymbol{x}_{i};\boldsymbol{w}) - y_i \right]^2
\]&lt;/span&gt;
To do this in R, we can set up a loop that will create a new data subset on each iteration, carry out the same procedure as detailed before to obtain the coefficients and the predicting function, measure the error and repeat &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; times. This is done with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N = length(y)
error = numeric(N)
for(i in 1:N){
  Xmini = X[-i,]
  ymini = y[-i]
  wLSmini = solve(t(Xmini) %*% Xmini) %*% t(Xmini) %*% ymini
  fout = X[i,] %*% wLSmini
  error[i] = (fout - y[i])^2
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the indexing of &lt;code&gt;X[-i,]&lt;/code&gt; will subset &lt;code&gt;X&lt;/code&gt; to all rows except the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th one. With this reduced data set, as well as only using the &lt;span class=&#34;math inline&#34;&gt;\(y_{-i}\)&lt;/span&gt; inputs, we estimate the coefficients at each iteration and then use that to calculate the squared error, which is saved in the &lt;code&gt;error&lt;/code&gt; array. To find the overall error, we just take the mean of this array.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(error)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.701126e-27&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Removing Features&lt;/h2&gt;
&lt;p&gt;We can use this overall squared error as a reference point, because it can be compared against the same score when removing certain inputs (columns of &lt;code&gt;X&lt;/code&gt;) to see if the prediction function is more accurate without some of these included. We can do this by setting up another loop that loops over the number of inputs, and performing the same methods as before to calculate the averages. This is done with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;D = dim(X)[2]
error_all = numeric(D-1)
for(d in 2:D){
  Xd = X[,-d] 
  error_d = numeric(N)
  for(i in 1:(dim(X)[1])){
    Xmini = Xd[-i,]
    ymini = y[-i]
    wLSmini = solve(t(Xmini) %*% Xmini) %*% t(Xmini) %*% ymini
    fout = Xd[i,] %*% wLSmini
    error_d[i] = (fout - y[i])^2
  }
  error_all[d-1] = mean(error_d)
} &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this starts with setting the array &lt;code&gt;error_all&lt;/code&gt; to the length of the number of inputs. Then there is a loop which iterates over the number of inputs (excluding the first column, which corresponds to the intercept), and removes a column from &lt;code&gt;X&lt;/code&gt;, renaming it &lt;code&gt;Xd&lt;/code&gt;. Another array is set up, called &lt;code&gt;error_d&lt;/code&gt;, which is equivalent to the &lt;code&gt;error&lt;/code&gt; array from the previous section. The nested loop performs the same operation as the cross-validation before. We can inspect this array, and compare it against the previous cross-validation error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error_all&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.351267e-01 2.736666e-27 3.654340e-27 3.473490e-27 3.802165e-27
## [6] 4.893871e-27 9.457742e-28 1.465294e-27 3.635496e-27&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(error)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.701126e-27&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see that some of the cross-validation errors with a feature removed has a lower error than the original error. We can remove these variables if we want to, as it would reduce the cross-validation error. However, there is grounds for keeping the variables in the model, as the difference between errors is rather small, which we can see here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(colnames(X)[2:d],(error_all-mean(error)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]                [,2]                    [,3]                   
## [1,] &amp;quot;lcavol&amp;quot;            &amp;quot;lweight&amp;quot;               &amp;quot;age&amp;quot;                  
## [2,] &amp;quot;0.535126651390945&amp;quot; &amp;quot;-9.64460028673121e-28&amp;quot; &amp;quot;-4.67853198901128e-29&amp;quot;
##      [,4]                    [,5]                   [,6]                 
## [1,] &amp;quot;lbph&amp;quot;                  &amp;quot;svi&amp;quot;                  &amp;quot;lcp&amp;quot;                
## [2,] &amp;quot;-2.27635884835842e-28&amp;quot; &amp;quot;1.01038873950454e-28&amp;quot; &amp;quot;1.1927448494611e-27&amp;quot;
##      [,7]                    [,8]                    [,9]                  
## [1,] &amp;quot;gleason&amp;quot;               &amp;quot;pgg45&amp;quot;                 &amp;quot;lpsa&amp;quot;                
## [2,] &amp;quot;-2.75535156416629e-27&amp;quot; &amp;quot;-2.23583162435088e-27&amp;quot; &amp;quot;-6.5629761986902e-29&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows what the cross-validation error would be if we removed each of these predictors. We would generally prefer a smaller model, and the covariates with a low magnitude are likely not providing much information in the model, so we can exclude them. Overall, this would leave us with &lt;code&gt;lcp&lt;/code&gt; and &lt;code&gt;lpsa&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is only accounting for removing &lt;em&gt;one&lt;/em&gt; predictor and leaving the others in. We could now want to consider what different combinations of input variables we could include that would result in the lowest overall error. Another thing to consider is to relax the condition of linear least squares, so that the output &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y}\)&lt;/span&gt; could depend on some feature transform &lt;span class=&#34;math inline&#34;&gt;\(\phi(\boldsymbol{x})\)&lt;/span&gt;, which could improve the accuracy. In practice, this is an incredibly large amount of combinations, and would be nearly impossible to compare the different errors that would result in an overall ‘optimal’ model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Using the least squares method, we have reduced the model down to having two inputs, the log of the capsular penetration and the log of the prostate specific antigen. These were the only covariates that provided sufficient information about the output, the log of the cancer volume. The parameter estimates for these covariates in a reduced model is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X.new = model.matrix(~lcp+lpsa,data=Prostate)
wLS.new = solve(t(X.new) %*% X.new) %*% t(X.new) %*% y
wLS.new&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   [,1]
## (Intercept) 0.09134598
## lcp         0.32837479
## lpsa        0.53162109&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are reasonably large and positive values, meaning that if an individual has higher values of &lt;code&gt;lcp&lt;/code&gt; and &lt;code&gt;lpsa&lt;/code&gt;, they likely of having a higher cancer volume.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Git and GitHub</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/github/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/github/</guid>
      <description>


&lt;p&gt;RStudio can be used to efficiently make a package in R, and allows an accessible way of implementing version control and git integration. As an example of these processes, I have created an R package which implements a basic form of least squares regression. The following function obtains parameter estimates given a dataset and formula:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.model = function(formula, data){
  ys = all.vars(formula)[1]
  y = data[,ys]
  X = model.matrix(formula, data)
  wLS =  solve(t(X) %*% X) %*% t(X) %*% y
  return(list(Parameters = wLS, df = X, y = y))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other functions in the package include &lt;code&gt;LS.predict&lt;/code&gt; to get predictions and &lt;code&gt;LS.plot&lt;/code&gt; to plot the predicting function on top of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.predict = function(model, newdata=NULL){
  if(is.null(newdata)) return(model$df %*% model$Parameters)
  if(!is.null(newdata)) {
    nd = cbind(1,newdata)
    return((nd) %*% model$Parameters)
    }
}

LS.plot = function(model, var = NULL, ...){
  X = model$df
  y = model$y
  d = dim(X)
  if(is.null(var)){
    print(&amp;quot;var not specified, taking first input value&amp;quot;)
    names = colnames(X)[colnames(X)!=&amp;quot;(Intercept)&amp;quot;]
    var = names[1]
  }
  preds = LS.predict(model)
  o = order(preds)
  plot(X[,var], y, xlab = var, ...)
  lines(X[o,var], preds[o], col=&amp;quot;red&amp;quot;, lwd=2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;least-squares-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Least Squares Example&lt;/h3&gt;
&lt;p&gt;To see the usage of this package, see the following example using the prostate cancer dataset from the &lt;code&gt;lasso2&lt;/code&gt; package. Firstly, starting by fitting the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lasso2)
data(Prostate)
fit = LS.model(lpsa ~ lcavol, data = Prostate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output &lt;code&gt;fit&lt;/code&gt; can be passed into &lt;code&gt;LS.plot&lt;/code&gt; and &lt;code&gt;LS.predict&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(LS.predict(fit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]
## 1 1.0902222
## 2 0.7921115
## 3 1.1398502
## 4 0.6412553
## 5 2.0478064
## 6 0.7521390&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.plot(fit, var=&amp;quot;lcavol&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/github_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating the Package&lt;/h2&gt;
&lt;p&gt;RStudio allows the creation of a package to be relatively straightforward, with an option to create a template for an R package. The package structure consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DESCRIPTION&lt;/code&gt;: plain text file that contains information about the title of the package, the author, version etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LICENSE&lt;/code&gt;: description of copyright and licensing information for the package&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NAMESPACE&lt;/code&gt;: describes imports and exports, for example you can import another package if you are using it for your own package&lt;/li&gt;
&lt;li&gt;&lt;code&gt;R&lt;/code&gt; folder: folder which contains all the code for the package&lt;/li&gt;
&lt;li&gt;&lt;code&gt;man&lt;/code&gt; folder: contains documentation files to describe your functions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test&lt;/code&gt; folder: contains testing functions for testing the packages&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;documentation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Documentation&lt;/h3&gt;
&lt;p&gt;After installing the &lt;code&gt;devtools&lt;/code&gt; package, the &lt;code&gt;roxygen&lt;/code&gt; package can be used to automatically generate a documentation structure and populate the &lt;code&gt;NAMESPACE&lt;/code&gt; file. In RStudio, you can go to &lt;code&gt;Code -&amp;gt; Insert Roxygen skeleton&lt;/code&gt; when the cursor is inside a function to create the documentation skeleton for each function, and manually fill it in to describe the inputs, outputs, descriptions etc. of the function. Within this structure, fields are defined by the &lt;code&gt;@&lt;/code&gt; symbol, so for example &lt;code&gt;@param&lt;/code&gt; will define the input parameter of the model. As well as this, you can use &lt;code&gt;@import &amp;lt;package_name&amp;gt;&lt;/code&gt; to get Roxygen to add a particular package to the &lt;code&gt;NAMESPACE&lt;/code&gt; file. For example, the roxygen structure for &lt;code&gt;LS.model&lt;/code&gt; is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#&amp;#39; Least Squares Regression
#&amp;#39;
#&amp;#39; @param formula an object of class &amp;quot;formula&amp;quot;
#&amp;#39; @param data data frame to which the formula relates
#&amp;#39; @return list containing three elements: Parameters, df, y
#&amp;#39; @import stats
#&amp;#39; @examples
#&amp;#39; df = data.frame(y = c(1,2,3,4), x = c(2,5,3,1))
#&amp;#39; LS.model(y~x, data=df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The documentation can be generated by running the command &lt;code&gt;devtools::document()&lt;/code&gt; (or pressing &lt;code&gt;Ctrl + Shift + D&lt;/code&gt; in RStudio).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing&lt;/h3&gt;
&lt;p&gt;In most cases, testing is done manually. After creating a function, you can put a certain amount of inputs in, and make sure that the outputs match up with what you were expecting. This can be automated with the &lt;code&gt;testthat&lt;/code&gt; package. This allows testing to be consistent throughout code changes, so if you change some code, you can run the test again to see if the outputs match with what you were expecting, without having to manually test again. The command &lt;code&gt;usethis::use_test(&amp;quot;&amp;lt;name&amp;gt;&amp;quot;)&lt;/code&gt; can be used to populate the &lt;code&gt;tests&lt;/code&gt; directory, where the testing functions are stored.&lt;/p&gt;
&lt;p&gt;For the &lt;code&gt;LS.model&lt;/code&gt; function, some useful tests were to ensure that the output dimension &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; matched the input dimension. Using the &lt;code&gt;test_that&lt;/code&gt; and &lt;code&gt;expect_equal&lt;/code&gt; function achieved this functionality:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(testthat)
test_that(&amp;quot;output dimension (n)&amp;quot;, {
  df = data.frame(y=c(1,2,3,4),x=c(4,5,6,7))
  m = LS.model(y~x,data=df)
  expect_equal(dim(as.matrix(df))[1], 4)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other tests were also implemented for checking this function as well as the other functions. You can run all the tests by running &lt;code&gt;devtools:test()&lt;/code&gt; (or &lt;code&gt;Ctrl + Shift + T&lt;/code&gt; in RStudio):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; devtools::test()

Loading simpleLS
Testing simpleLS
✔ |  OK F W S | Context
✔ |   4       | LS.model
✔ |   2       | LSr

══ Results ═══════════════════════════════════════════════════════════
Duration: 0.2 s

OK:       6
Failed:   0
Warnings: 0
Skipped:  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows the tests that were passed, and can show the tests that were unsuccessful. If tests do not pass, then details will be given why, so that you know where something has gone wrong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coverage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coverage&lt;/h3&gt;
&lt;p&gt;Another useful functionality is to test how much your tests actually test. The coverage of your tests (as a percentage) will tell you how much code is not being tested, so generally higher coverage is better. This can be implemented with the &lt;code&gt;covr&lt;/code&gt; package. Running &lt;code&gt;covr::report()&lt;/code&gt; will generate a report. For this package, this received&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;simpleLS coverage - 95.45%&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;git-integration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Git Integration&lt;/h3&gt;
&lt;p&gt;Git and Github allow easy access to version control, and online storage and supply of an R package. By initialising a repository for the package directory, and allowing access to it on Github, your code and package is freely available online. ‘Committing’ and then ‘pushing’ your changes and files to your repository will update your package to the latest version, and you are able to view older versions of code and previous changes you made in case something goes wrong. This is very useful in software development, for example if you want to revert to the last stable version.&lt;/p&gt;
&lt;p&gt;The repository for this package can be found at:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/DanielWilliamsS/simpleLS&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;travis-ci-integration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Travis CI integration&lt;/h3&gt;
&lt;p&gt;A publicly available R package can be tested online using a tool known as Travis CI (CI - Continuous Integration). When a pull request is made, or new changes are pushed to the Github repository, Travis CI will automatically test the code using the testing functions described previously. This allows someone who downloads the package to be sure that the code works, and provides a way of automatically testing new versions of code. This is especially useful in collaborative coding projects.&lt;/p&gt;
&lt;p&gt;Environmental variables can be included in the Travis CI settings, which allows Travis to do other things. For example, one environmental variable will test the coverage of the code testing, as described previously. Another environmental variable can enable Travis to build RMarkdown pages and deploy them to a Github pages website, allowing you to publish your html RMarkdown document online.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Intro to OpenMP in C&#43;&#43;</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/openmp/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/openmp/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;‘Normal’ programming can deal with performing operations one at a time, i.e. writing code in such a way that it can be seen as a set of instructions to be performed sequentially. A more efficient process will be maximising the use of the cores in your processor so that multiple operations are performed at once, with different processes happening on different cores.&lt;/p&gt;
&lt;p&gt;OpenMP is a way of performing parallel computation for C, C++ and Fortran, but this portfolio will go over the use of OpenMP in C++.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;A simple C++ script which uses OpenMP for parallel computing will look like this&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;

#ifdef _OPENMP
    #include &amp;lt;omp.h&amp;gt;
#else
    #define omp_get_num_threads() 0
    #define omp_get_thread_num() 0
#endif

int main(int argc, const char **argv)
{
    std::cout &amp;lt;&amp;lt; &amp;quot;Hello I am here safe and sound home in the main thread.\n&amp;quot;;

    #pragma omp parallel
    {
        int nthreads = omp_get_num_threads();
        int thread_id = omp_get_thread_num();

        std::cout &amp;lt;&amp;lt; &amp;quot;Help I am trapped in thread number &amp;quot; &amp;lt;&amp;lt; thread_id
                  &amp;lt;&amp;lt; &amp;quot; out of a total &amp;quot; &amp;lt;&amp;lt; nthreads 
                  &amp;lt;&amp;lt; std::endl;
    }

    std::cout &amp;lt;&amp;lt; &amp;quot;Thank god I&amp;#39;m safe back home now.\n&amp;quot;;

    return 0;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important parts here are the &lt;code&gt;ifdef _OPENMP&lt;/code&gt; section at the start, and the &lt;code&gt;#pragma omp parallel&lt;/code&gt; line before the process of the script. The comments succeeding the hash symbol can be seen as a ‘hint’ to the compiler of what to do. The compiler is free to ignore this if need be.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;if_def _OPENMP&lt;/code&gt; line checks if we are using OpenMP or not, and if so, includes the &lt;code&gt;omp.h&lt;/code&gt; header file. The &lt;code&gt;pragma omp parallel&lt;/code&gt; line tells the compiler to run the section enclosed in braces &lt;code&gt;{}&lt;/code&gt; a certain amount of times, depending on the input number of threads.&lt;/p&gt;
&lt;p&gt;We can compile this code (which is saved in &lt;code&gt;basic_openmp.cpp&lt;/code&gt;) to see what happens.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp basic_openmp.cpp -o basic_openmp
export OMP_NUM_THREADS=1
./basic_openmp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello I am here safe and sound home in the main thread.
## Help I am trapped in thread number 0 out of a total 1
## Thank god I&amp;#39;m safe back home now.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp basic_openmp.cpp -o basic_openmp
export OMP_NUM_THREADS=8
./basic_openmp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello I am here safe and sound home in the main thread.
## Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number 462 out of a total 8 out of a total  out of a total 8
## 
## 8
## 3 out of a total 8
## Help I am trapped in thread number 0 out of a total 8
## Help I am trapped in thread number 5 out of a total 8
## 7 out of a total 8
## Help I am trapped in thread number 1 out of a total 8
## Thank god I&amp;#39;m safe back home now.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This person really got trapped in a time loop. This is because the threads do not run sequentially, so each thread is printing out what it is asked as soon as it can, and these commands are being run at the same time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sections-and-loops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sections and Loops&lt;/h2&gt;
&lt;p&gt;OpenMP sections are ways of telling the compiler that each section can be operated on different threads. This is done by adding the line &lt;code&gt;#pragma omp section&lt;/code&gt; and braces &lt;code&gt;{}&lt;/code&gt; to each section that can be operated on individually, but only once. This is a way of breaking your code into parallel ‘chunks’ without executing the same code a lot of times by each thread.&lt;/p&gt;
&lt;p&gt;A loop can be run in parallel by writing the line &lt;code&gt;#pragma omp for&lt;/code&gt; above the loop. This specifies that each iteration in the loop is independent and can be run separately. Then each thread runs on a different iteration of the loop at the same time. Below is an example of a loop running in parallel.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;

#ifdef _OPENMP
    #include &amp;lt;omp.h&amp;gt;
#else
    #define omp_get_thread_num() 0
#endif

int main(int argc, const char **argv)
{
    #pragma omp parallel
    {
        int nloops = 0;
        
        #pragma omp for
        for (int i=0; i&amp;lt;1000; ++i)
        {
            ++nloops;
        }

        int thread_id = omp_get_thread_num();
        #pragma omp critical
        {
          std::cout &amp;lt;&amp;lt; &amp;quot;Thread &amp;quot; &amp;lt;&amp;lt; thread_id &amp;lt;&amp;lt; &amp;quot; performed &amp;quot;
                    &amp;lt;&amp;lt; nloops &amp;lt;&amp;lt; &amp;quot; iterations of the loop.\n&amp;quot;;
        }
    }

    return 0;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code will display how many iterations of the loop each thread is performing. Note that the &lt;code&gt;#pragma omp critical&lt;/code&gt; line is specifying that only one thread can enter the code in braces &lt;code&gt;{}&lt;/code&gt; at a time, just so that the printed output does not get jumbled up like it has done previously. In most code, this line will not be added as it will come with a significant slow down, I have only included it here for illustration purposes. We can compile and then run this code.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp loop_openmp.cpp -o loop_openmp
export OMP_NUM_THREADS=4
./loop_openmp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Thread 0 performed 250 iterations of the loop.
## Thread 1 performed 250 iterations of the loop.
## Thread 2 performed 250 iterations of the loop.
## Thread 3 performed 250 iterations of the loop.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So these threads have split the job evenly, but in some cases (maybe for a large amount of threads) the job would not be split evenly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo Exercise&lt;/h2&gt;
&lt;p&gt;For an exercise, consider using an OpenMP parallel program to calculate &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; using a Monte Carlo algorithm. We can do this by simulating a unit circle and a unit square, since &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is the ratio of the area of a circle to the area of a square. By simulating random points for a circle and for a square, provided we have a large enough number of simulations we can estimate this proportion and hence &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;cmath&amp;gt;
#include &amp;lt;cstdlib&amp;gt;
#include &amp;lt;iostream&amp;gt;

#ifdef _OPENMP
    #include &amp;lt;omp.h&amp;gt;
#else
    #define omp_get_thread_num() 0
#endif

double rand_one()
{
    return std::rand() / (RAND_MAX + 1.0);
}

int main()
{
    
    // declare variables
    int circle_points = 0;
    int square_points = 0;
    
    int circle_points_loop = 0;
    int square_points_loop = 0;

    // set up parallel OpenMP
    #pragma omp parallel
    {   

        // run for loop in parallel
        #pragma omp for
        for(int ii=0; ii &amp;lt; 100000; ii++)
        {

            // get random x and y coordinates
            double x_coord = (2*rand_one() - 1);
            double y_coord = (2*rand_one() - 1);

            // calculate radius
            double r = std::sqrt(pow(x_coord,2) + pow(y_coord,2));

            // if r is less than or equal to 1 then it is within the circle
            if(r &amp;lt; 1.0)
            {
                ++circle_points_loop;
            } else 
            {
                ++square_points_loop;
            }
        
        }

        // use critical when counting the final number of counts for each thread
        #pragma omp critical
        {
            circle_points += circle_points_loop;
            square_points += square_points_loop;
        }
    }

    // calculate final value of pi using ratios
    double pi = (4.0*circle_points)/(square_points+circle_points);
    
    // print pi
    std::cout &amp;lt;&amp;lt; pi &amp;lt;&amp;lt; std::endl;
    return 0;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The comments in the code above will explain why each section of code is run at each point in time. To check this result is valid, we can compile and run it.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp pi.cpp -o pi
export OMP_NUM_THREADS=8
./pi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3.13372&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is not a bad approximation!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Performance and Debugging</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/performance/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/performance/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/profvis/profvis.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/profvis/profvis.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/highlight/textmate.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/highlight/highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/profvis-binding/profvis.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Debugging&lt;/strong&gt; is an important part of any programming process. It is unlikely that an individual will write their code correctly on the first write up, and it is generally accepted that the code will only become usable after a few debugging iterations. &lt;strong&gt;Perfomance&lt;/strong&gt; is the measure of how efficient your code is with respect to speed, so that you can do the same operation in as quick a time as possible.&lt;/p&gt;
&lt;p&gt;In this section of the portfolio, the debugging process and the performance aspects will be explained by way of an example. This example will be a large function that is deliberately inefficiently and incorrectly written, and will be fixed and improved using different methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-least-squares-with-feature-transform&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Least Squares with Feature Transform&lt;/h2&gt;
&lt;p&gt;Take for example least squares regression with the choice of three basis functions; linear, quadratic, or trigonometric. This function can take one output vector &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and one input vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, and estimate parameters &lt;span class=&#34;math inline&#34;&gt;\(w_{LS}\)&lt;/span&gt; from the solution
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{w}_{LS} = \left(\boldsymbol{\phi}(\boldsymbol{X})\boldsymbol{\phi}(\boldsymbol{X})^T +\lambda \boldsymbol{I}\right)^{-1}\boldsymbol{\phi}(\boldsymbol{X})\boldsymbol{y}^T,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X}\)&lt;/span&gt; is the model matrix, and &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is a feature transform function. The first version of this function looks like&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.feature.transform.fit &amp;lt;- function(y, x, ft, b){
  
  if(ft == &amp;quot;Polynomial&amp;quot;) basis_function = function(x) poly(x, b, raw=TRUE)
  if(ft == &amp;quot;Linear&amp;quot;) basis_function = function(x) x 
  if(ft == &amp;quot;Trigonometric&amp;quot;) {
    basis_function = function(x){
      basis = c()
      for(i in 1:b){
        basis = cbind(basis, sin(i*x), cos(i*x))
      }
      return(basis)
    }
  }
  
  Phi = matrix(NA, length(x), length(basis_function(x))+1)
  for(i in 1:length(x)){
    Phi[i,] = c(1, basis_function(x[i]))
  }
  
  wLS =  solve(t(Phi) %*% Phi) %*% t(Phi) %*% y
  return(wLS)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes the argument &lt;code&gt;ft&lt;/code&gt;, meaning feature transform. The first thing that the function does is try to recognise which feature transform is being input, by a series of three &lt;code&gt;if&lt;/code&gt; functions. Then &lt;code&gt;basis_function&lt;/code&gt; is assigned to a function corresponding to the correct feature transform. After this, &lt;code&gt;Phi&lt;/code&gt; is set up as a matrix with the correct dimensions, that is the length of the data and the dimension of the feature space (including a column of 1’s).&lt;/p&gt;
&lt;p&gt;Let’s test this function. Good practice is to create a series of testing functions or scripts that will test as many aspects of the function as possible. This code chunk below will test the function for each basis function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lasso2)
data(Prostate)
LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,&amp;quot;Linear&amp;quot;,1)
LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,&amp;quot;Polynomial&amp;quot;,3)
LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,&amp;quot;Trigonometric&amp;quot;,1)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;debugging&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Debugging&lt;/h3&gt;
&lt;p&gt;Let’s try the function for a linear (identity) feature transform. When this line is run, the following error is returned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,&amp;quot;Linear&amp;quot;,1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in t(Phi) %*% Phi : 
##   requires numeric/complex matrix/vector arguments&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This error message is not informative enough to go back and fix our function immediately. We can use the &lt;code&gt;traceback&lt;/code&gt; function to get more information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceback()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2: solve(t(Phi) %*% Phi) at #20
## 1: LS.feature.transform.fit(Prostate$lpsa, Prostate$lcavol, &amp;quot;Linear&amp;quot;, 
       1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not as useful here, because it does not say much more than the previous error message. We do now know that the error is on line &lt;code&gt;#20&lt;/code&gt;, but the actual problem could be before that, presumably in the definition of &lt;code&gt;Phi&lt;/code&gt;. We can also use another debugging function, called &lt;code&gt;browser()&lt;/code&gt;, which allows you to open an interactive debugging environment. From here we can interactively view all elements and find where the problem is. &lt;em&gt;Note that RStudio also allows an interactive debugging environment.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let’s take a look at the elements in the function after the definition of &lt;code&gt;Phi&lt;/code&gt;. We can first look at the first few rows and columns of &lt;code&gt;Phi&lt;/code&gt;. The following code was run after running all other parts of the function up to the definition of &lt;code&gt;Phi&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Phi[1:5,1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]       [,2] [,3]       [,4] [,5]
## [1,]    1 -0.5798185    1 -0.5798185    1
## [2,]    1 -0.9942523    1 -0.9942523    1
## [3,]    1 -0.5108256    1 -0.5108256    1
## [4,]    1 -1.2039728    1 -1.2039728    1
## [5,]    1  0.7514161    1  0.7514161    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dimension of &lt;code&gt;Phi&lt;/code&gt; is wrong, as the columns are being repeated! It should only have two columns, a column of 1’s regarding to the intercept, and a column of each &lt;span class=&#34;math inline&#34;&gt;\(\phi(x)\)&lt;/span&gt;. This error must come from where the dimension is defined, in the line &lt;code&gt;Phi = matrix(NA, length(x), length(basis_function(x))+1)&lt;/code&gt;. Looking at &lt;code&gt;Phi&lt;/code&gt; in this case, we see we do not want to use the length of the basis function, but instead the number of columns that it contains. So instead we can change &lt;code&gt;length(basis_function(x))+1&lt;/code&gt; to &lt;code&gt;dim(as.matrix(basis_function(x)))[2]+1&lt;/code&gt;. Now we run the test again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,&amp;quot;Linear&amp;quot;,1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] 1.5072975
## [2,] 0.7193204&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the function works! We can see if the parameter estimates are correct by comparing to the output from a linear model with &lt;code&gt;lm&lt;/code&gt;, since we have the known result that &lt;span class=&#34;math inline&#34;&gt;\(w_{LS} = w_{MLE}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(lpsa ~ lcavol, data = Prostate)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)      lcavol 
##   1.5072975   0.7193204&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And ensuring that our function works for the other two feature transforms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol, &amp;quot;Polynomial&amp;quot;, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]
## [1,]  1.66387139
## [2,]  0.69613468
## [3,] -0.18630511
## [4,]  0.06164228&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(lpsa ~ poly(lcavol, 3, raw=TRUE), data = Prostate)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  (Intercept) poly(lcavol, 3, raw = TRUE)1 
##                   1.66387139                   0.69613468 
## poly(lcavol, 3, raw = TRUE)2 poly(lcavol, 3, raw = TRUE)3 
##                  -0.18630511                   0.06164228&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol, &amp;quot;Trigonometric&amp;quot;, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]
## [1,]  2.4198088
## [2,]  0.3801217
## [3,] -1.2342905
## [4,]  0.4748111
## [5,]  0.3759252&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(lpsa ~ sin(lcavol) + cos(lcavol) + sin(2*lcavol) + cos(2*lcavol), data = Prostate)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     (Intercept)     sin(lcavol)     cos(lcavol) sin(2 * lcavol) cos(2 * lcavol) 
##       2.4198088       0.3801217      -1.2342905       0.4748111       0.3759252&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are no errors, and our parameter estimates match those from &lt;code&gt;lm&lt;/code&gt;, so this debugging has been a success.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Performance&lt;/h3&gt;
&lt;p&gt;So far we have only tested this for a length &lt;span class=&#34;math inline&#34;&gt;\(n=97\)&lt;/span&gt; dataset, so performance has been relatively fast. We can use the function &lt;code&gt;microbenchmark&lt;/code&gt; from the &lt;code&gt;microbenchmark&lt;/code&gt; package to test how quickly our function runs, and gives a summary of statistics on how quickly the function runs. Before we do that, we want to have a larger data set to see more obvious difference in how our computation times are. We can do this with some simulated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.test = runif(10000, 1, 5)
y.test = rexp(10000,rate=1.5*x.test-1)
library(microbenchmark)
# microbenchmark(LS.feature.transform.fit(y.test,x.test,&amp;quot;Polynomial&amp;quot;,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This range of speeds is not bad, but could be improved. To see where we can improve the performance of our code, we can do something called &lt;em&gt;profiling&lt;/em&gt;. A statistical profiler can determine where the code is spending most of its time being run, by using operating system interrupts. An implementation of profiling in R is provided by the &lt;code&gt;profvis&lt;/code&gt; package, and the &lt;code&gt;profvis&lt;/code&gt; function. We start by running this on our function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(profvis)
x = x.test; y = y.test; ft = &amp;quot;Polynomial&amp;quot;; b = 5
profvis({
  if(ft == &amp;quot;Polynomial&amp;quot;) basis_function = function(x) poly(x, b, raw=TRUE)
  if(ft == &amp;quot;Linear&amp;quot;) basis_function = function(x) x 
  if(ft == &amp;quot;Trigonometric&amp;quot;) {
    basis_function = function(x){
      basis = c()
      for(i in 1:b){
        basis = cbind(basis, sin(i*x), cos(i*x))
      }
      return(basis)
    }
  }
  
  Phi = matrix(NA, length(x), dim(as.matrix(basis_function(x)))[2]+1)
  for(i in 1:length(x)){
    Phi[i,] = c(1, basis_function(x[i]))
  }
  
  wLS =  solve(t(Phi) %*% Phi) %*% t(Phi) %*% y
})&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:600px;&#34; class=&#34;profvis html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;message&#34;:{&#34;prof&#34;:{&#34;time&#34;:[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,4,4,4,4,5,5,5,6,6,7,7,7,8,8,8,9,9,9,9,10,10,10,10,11,11,11,11,12,12,12,13,13,13,14,14,14,15,15,15,16,16,16,16,17,17,17,18,18,19,19,19,20,20,20,21,21,21,21,22,22,22,23,23,24,24,24,25,25,25,25,26,26,26,26,27,27,27,27,28,28,29,29,29,30,30,30,31,31,31,32,32,32,33,33,33,34,34,34,34,35,35,35,36,36,36,37,37,38,38,38,39,39,40,40,40,41,41,42,42,42,43,43,43,44,44,45,45,45,46,46,46,47,47,47,47,47,47,48,48,48,49,49,49,50,51,51,51,52,52,52,52,53],&#34;depth&#34;:[18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,3,2,1,4,3,2,1,4,3,2,1,4,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,2,1,3,2,1,2,1,3,2,1,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,6,5,4,3,2,1,3,2,1,3,2,1,1,3,2,1,4,3,2,1,1],&#34;label&#34;:[&#34;delayedAssign&#34;,&#34;findCenvVar&#34;,&#34;getInlineInfo&#34;,&#34;isBaseVar&#34;,&#34;FUN&#34;,&#34;lapply&#34;,&#34;unlist&#34;,&#34;Filter&#34;,&#34;findLocalsList&#34;,&#34;funEnv&#34;,&#34;make.functionContext&#34;,&#34;cmpfun&#34;,&#34;doTryCatch&#34;,&#34;tryCatchOne&#34;,&#34;tryCatchList&#34;,&#34;tryCatch&#34;,&#34;compiler:::tryCmpfun&#34;,&#34;basis_function&#34;,&#34;get&#34;,&#34;match.fun&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;is.array&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;%in%&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;match.fun&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;attributes&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;&lt;GC&gt;&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;attributes&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;rep.int&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;rep.int&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;is.data.frame&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;match.fun&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;if(ft == \&#34;Polynomial\&#34;) basis_function = function(x) poly(x, b, raw=TRUE)&#34;,&#34;basis_function&#34;,&#34;length&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;match.fun&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;colnames&lt;-&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;if(ft == \&#34;Polynomial\&#34;) basis_function = function(x) poly(x, b, raw=TRUE)&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;as.character&#34;,&#34;get&#34;,&#34;match.fun&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;outer&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;Phi[i,] = c(1, basis_function(x[i]))&#34;,&#34;if(ft == \&#34;Polynomial\&#34;) basis_function = function(x) poly(x, b, raw=TRUE)&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;%in%&#34;,&#34;structure&#34;,&#34;poly&#34;,&#34;basis_function&#34;,&#34;%*%&#34;],&#34;filenum&#34;:[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,1,null,null,null,1,1,null,1,1,null,null,1,1,null,1,1,1,1,null,1,1,null,1,1,null,null,1,1,null,null,1,1,null,null,1,1,null,1,1,null,1,1,null,1,1,null,1,1,null,null,1,1,null,1,1,1,1,null,1,1,null,1,1,null,null,1,1,null,1,1,1,1,null,1,1,null,null,1,1,null,null,1,1,null,null,1,1,1,1,null,1,1,null,1,1,null,1,1,null,1,1,null,1,1,null,null,1,1,null,1,1,null,1,1,1,1,null,1,1,1,1,null,1,1,1,1,null,1,1,null,1,1,1,1,null,1,1,null,1,1,null,null,null,null,1,1,null,1,1,null,1,1,1,1,1,1,null,null,1,1,1],&#34;linenum&#34;:[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,18,18,null,null,null,4,18,null,4,18,null,null,4,18,null,4,18,4,18,null,4,18,null,4,18,null,null,4,18,null,null,4,18,null,null,4,18,null,4,18,null,4,18,null,4,18,null,4,18,null,null,4,18,null,4,18,4,18,null,4,18,null,4,18,null,null,4,18,null,4,18,4,18,null,4,18,null,null,4,18,null,null,4,18,null,null,4,18,4,18,null,4,18,null,4,18,null,4,18,null,4,18,null,4,18,null,null,4,18,null,4,18,null,4,18,4,18,null,4,18,4,18,null,4,18,4,18,null,4,18,null,4,18,4,18,null,4,18,null,4,18,null,null,null,null,4,18,null,4,18,null,4,18,18,4,4,18,null,null,4,18,21],&#34;memalloc&#34;:[9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,9.78263854980469,10.3573379516602,10.3573379516602,10.3573379516602,10.3573379516602,10.3573379516602,10.6617584228516,10.6617584228516,10.6617584228516,10.7928085327148,10.7928085327148,10.7928085327148,10.7928085327148,10.9356842041016,10.9356842041016,10.9356842041016,11.1370620727539,11.1370620727539,11.4857635498047,11.4857635498047,11.4857635498047,11.7755355834961,11.7755355834961,11.7755355834961,12.1318359375,12.1318359375,12.1318359375,12.1318359375,12.2556838989258,12.2556838989258,12.2556838989258,12.2556838989258,12.6467514038086,12.6467514038086,12.6467514038086,12.6467514038086,12.9204025268555,12.9204025268555,12.9204025268555,12.9824066162109,12.9824066162109,12.9824066162109,9.10933685302734,9.10933685302734,9.10933685302734,9.28131866455078,9.28131866455078,9.28131866455078,9.45974731445312,9.45974731445312,9.45974731445312,9.45974731445312,9.83200073242188,9.83200073242188,9.83200073242188,10.0734939575195,10.0734939575195,10.2781219482422,10.2781219482422,10.2781219482422,10.3605041503906,10.3605041503906,10.3605041503906,10.6409683227539,10.6409683227539,10.6409683227539,10.6409683227539,10.8977584838867,10.8977584838867,10.8977584838867,11.0669555664062,11.0669555664062,11.2500152587891,11.2500152587891,11.2500152587891,11.6833038330078,11.6833038330078,11.6833038330078,11.6833038330078,11.8435363769531,11.8435363769531,11.8435363769531,11.8435363769531,9.03913879394531,9.03913879394531,9.03913879394531,9.03913879394531,9.24923706054688,9.24923706054688,9.57025909423828,9.57025909423828,9.57025909423828,9.84332275390625,9.84332275390625,9.84332275390625,10.0794372558594,10.0794372558594,10.0794372558594,10.3390197753906,10.3390197753906,10.3390197753906,10.5665054321289,10.5665054321289,10.5665054321289,10.6869735717773,10.6869735717773,10.6869735717773,10.6869735717773,11.0885314941406,11.0885314941406,11.0885314941406,11.1933975219727,11.1933975219727,11.1933975219727,11.4212493896484,11.4212493896484,11.6744003295898,11.6744003295898,11.6744003295898,9.05852508544922,9.05852508544922,9.35573577880859,9.35573577880859,9.35573577880859,9.75798797607422,9.75798797607422,10.0541076660156,10.0541076660156,10.0541076660156,10.4486618041992,10.4486618041992,10.4486618041992,10.6064147949219,10.6064147949219,11.0232009887695,11.0232009887695,11.0232009887695,11.1743316650391,11.1743316650391,11.1743316650391,11.5469055175781,11.5469055175781,11.5469055175781,11.5469055175781,11.5469055175781,11.5469055175781,11.7929534912109,11.7929534912109,11.7929534912109,9.10997772216797,9.10997772216797,9.10997772216797,9.43967437744141,9.74562835693359,9.74562835693359,9.74562835693359,10.0864334106445,10.0864334106445,10.0864334106445,10.0864334106445,11.6077194213867],&#34;meminc&#34;:[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.574699401855469,0,0,0,0,0.304420471191406,0,0,0.131050109863281,0,0,0,0.142875671386719,0,0,0.201377868652344,0,0.348701477050781,0,0,0.289772033691406,0,0,0.356300354003906,0,0,0,0.123847961425781,0,0,0,0.391067504882812,0,0,0,0.273651123046875,0,0,0.0620040893554688,0,0,-3.87306976318359,0,0,0.171981811523438,0,0,0.178428649902344,0,0,0,0.37225341796875,0,0,0.241493225097656,0,0.204627990722656,0,0,0.0823822021484375,0,0,0.280464172363281,0,0,0,0.256790161132812,0,0,0.169197082519531,0,0.183059692382812,0,0,0.43328857421875,0,0,0,0.160232543945312,0,0,0,-2.80439758300781,0,0,0,0.210098266601562,0,0.321022033691406,0,0,0.273063659667969,0,0,0.236114501953125,0,0,0.25958251953125,0,0,0.227485656738281,0,0,0.120468139648438,0,0,0,0.401557922363281,0,0,0.104866027832031,0,0,0.227851867675781,0,0.253150939941406,0,0,-2.61587524414062,0,0.297210693359375,0,0,0.402252197265625,0,0.296119689941406,0,0,0.394554138183594,0,0,0.157752990722656,0,0.416786193847656,0,0,0.151130676269531,0,0,0.372573852539062,0,0,0,0,0,0.246047973632812,0,0,-2.68297576904297,0,0,0.329696655273438,0.305953979492188,0,0,0.340805053710938,0,0,0,1.52128601074219],&#34;filename&#34;:[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,null,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;]},&#34;interval&#34;:10,&#34;files&#34;:[{&#34;filename&#34;:&#34;&lt;expr&gt;&#34;,&#34;content&#34;:&#34;library(profvis)\nx = x.test; y = y.test; ft = \&#34;Polynomial\&#34;; b = 5\nprofvis({\n  if(ft == \&#34;Polynomial\&#34;) basis_function = function(x) poly(x, b, raw=TRUE)\n  if(ft == \&#34;Linear\&#34;) basis_function = function(x) x \n  if(ft == \&#34;Trigonometric\&#34;) {\n    basis_function = function(x){\n      basis = c()\n      for(i in 1:b){\n        basis = cbind(basis, sin(i*x), cos(i*x))\n      }\n      return(basis)\n    }\n  }\n  \n  Phi = matrix(NA, length(x), dim(as.matrix(basis_function(x)))[2]+1)\n  for(i in 1:length(x)){\n    Phi[i,] = c(1, basis_function(x[i]))\n  }\n  \n  wLS =  solve(t(Phi) %*% Phi) %*% t(Phi) %*% y\n})&#34;,&#34;normpath&#34;:&#34;&lt;expr&gt;&#34;}],&#34;prof_output&#34;:&#34;/tmp/RtmpEOHPfa/file294e688a56c6.prof&#34;,&#34;highlight&#34;:{&#34;output&#34;:[&#34;^output\\$&#34;],&#34;gc&#34;:[&#34;^&lt;GC&gt;$&#34;],&#34;stacktrace&#34;:[&#34;^\\.\\.stacktraceo(n|ff)\\.\\.$&#34;]},&#34;split&#34;:&#34;h&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Looking at this graph, we can see that most of the time and memory is spent in the &lt;code&gt;basis_function&lt;/code&gt;. This is likely due to the basis function being run at every iteration in the loop. One way of improving this would be to assign elements of &lt;code&gt;Phi&lt;/code&gt; in terms of columns instead of rows. This is because R uses &lt;em&gt;column-major storage&lt;/em&gt;, meaning that when a matrix is stored in memory, it is being assigned in chunks that correspond to columns, not rows. Therefore defining a matrix column-wise needs fewer operations than defining a matrix row-wise.&lt;/p&gt;
&lt;p&gt;The performance can be greatly increased by &lt;em&gt;vectorising&lt;/em&gt; so that the basis function need only be applied once instead of many times (as this is where the slowdown was). This eliminates the loop as well, another source of inefficiency. We make the following changes when defining the matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\phi}(\boldsymbol{x})\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Phi = as.matrix(cbind(1, basis_function(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see that the dimensions do not need to be set up in advance. Now if we benchmark and profile the function again, we get&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##                                                       expr      min       lq
##  LS.feature.transform.fit(y.test, x.test, &amp;quot;Polynomial&amp;quot;, 5) 7.071979 15.42255
##      mean   median       uq      max neval
##  23.93168 20.15049 25.85308 171.5225   100&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:600px;&#34; class=&#34;profvis html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;message&#34;:{&#34;prof&#34;:{&#34;time&#34;:[1,1,2,2,3,4,5],&#34;depth&#34;:[2,1,2,1,1,1,1],&#34;label&#34;:[&#34;solve.default&#34;,&#34;solve&#34;,&#34;solve.default&#34;,&#34;solve&#34;,&#34;%*%&#34;,&#34;%*%&#34;,&#34;%*%&#34;],&#34;filenum&#34;:[null,1,null,1,1,1,1],&#34;linenum&#34;:[null,32,null,32,32,32,32],&#34;memalloc&#34;:[11.9669342041016,11.9669342041016,11.9669342041016,11.9669342041016,12.8827972412109,12.8827972412109,12.8827972412109],&#34;meminc&#34;:[0,0,0,0,0.915863037109375,0,0],&#34;filename&#34;:[null,&#34;&lt;expr&gt;&#34;,null,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;,&#34;&lt;expr&gt;&#34;]},&#34;interval&#34;:10,&#34;files&#34;:[{&#34;filename&#34;:&#34;&lt;expr&gt;&#34;,&#34;content&#34;:&#34;LS.feature.transform.fit &lt;- function(y, x, ft, b){\n  if(ft == \&#34;Polynomial\&#34;) basis_function = function(x) poly(x, b, raw=TRUE)\n  if(ft == \&#34;Linear\&#34;) basis_function = function(x) x \n  if(ft == \&#34;Trigonometric\&#34;) {\n    basis_function = function(x){\n      basis = c()\n      for(i in 1:b){\n        basis = cbind(basis, sin(i*x), cos(i*x))\n      }\n      return(basis)\n    }\n  }\n  Phi = as.matrix(cbind(1, basis_function(x)))\n  wLS =  solve(t(Phi) %*% Phi) %*% t(Phi) %*% y\n}\nlibrary(microbenchmark)\nmicrobenchmark(LS.feature.transform.fit(y.test,x.test,\&#34;Polynomial\&#34;,5))\nprofvis({\n  if(ft == \&#34;Polynomial\&#34;) basis_function = function(x) poly(x, b, raw=TRUE)\n  if(ft == \&#34;Linear\&#34;) basis_function = function(x) x \n  if(ft == \&#34;Trigonometric\&#34;) {\n    basis_function = function(x){\n      basis = c()\n      for(i in 1:b){\n        basis = cbind(basis, sin(i*x), cos(i*x))\n      }\n      return(basis)\n    }\n  }\n  \n  Phi = as.matrix(cbind(1, basis_function(x)))\n  wLS =  solve(t(Phi) %*% Phi) %*% t(Phi) %*% y\n})&#34;,&#34;normpath&#34;:&#34;&lt;expr&gt;&#34;}],&#34;prof_output&#34;:&#34;/tmp/RtmpEOHPfa/file294e22c0c55d.prof&#34;,&#34;highlight&#34;:{&#34;output&#34;:[&#34;^output\\$&#34;],&#34;gc&#34;:[&#34;^&lt;GC&gt;$&#34;],&#34;stacktrace&#34;:[&#34;^\\.\\.stacktraceo(n|ff)\\.\\.$&#34;]},&#34;split&#34;:&#34;h&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Here the benchmarks are significantly faster, and whilst the function seems to get stuck in the same place, the times that it gets stuck there is order of magnitudes smaller than it was previously.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;In this portfolio section, we explained and showed an extended example of &lt;em&gt;debugging&lt;/em&gt;, &lt;em&gt;profiling&lt;/em&gt; and &lt;em&gt;optimising performance&lt;/em&gt;. There are better ways of implementing all of these things than what was demonstrated here.&lt;/p&gt;
&lt;p&gt;For &lt;em&gt;debugging&lt;/em&gt;, we simply printed out the code where we thought the errors were, but a more rigorous debugging process would have involved an interactive debugger, which was discussed but not implemented. Using the &lt;code&gt;debug&lt;/code&gt; function in R would allow RStudio to go through each line of the function and show results at each point. RStudio also allows breakpoints in functions, so that when the function is run, it will stop at a breakpoint instead of having to go through every line.&lt;/p&gt;
&lt;p&gt;For &lt;em&gt;optimising performance&lt;/em&gt;, R is generally inefficient for user written functions and scripts. Code written would be a lot more efficient and faster if it was written in a language such as C. Many core R routines and packages are written in C, greatly improving their efficiency. This can be achieved with the &lt;code&gt;RCpp&lt;/code&gt; package, which provides an accessible way of writing efficient R code in C++.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidyverse</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/tidyverse/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/tidyverse/</guid>
      <description>


&lt;div id=&#34;tidyverse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidyverse&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;tidyverse&lt;/em&gt; is a set of packages in R that share the same programming philosophy. These packages are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;readr&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;magrittr&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these packages provide different functionalities. They can all be loaded at once by loading &lt;code&gt;library(tidyverse)&lt;/code&gt;. The combination of these packages provide an easier ‘front end’ to R. The tidyverse packages streamline the process of data manipulation compared to base R, as well as providing additional functions to simplify plotting and visualisation. This portfolio will go through an example demonstrating the usage of functions from these packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-energy-output-from-buildings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Energy Output from Buildings&lt;/h2&gt;
&lt;p&gt;This dataset was obtained from the &lt;em&gt;ASHRAE - Great Energy Predictor III&lt;/em&gt;. It is a large dataset that contains information about energy meter readings from 1448 buildings, which are classified by their primary use (e.g. education), their square footage, the year they were built and the number of floors they have. Meter readings are taken at all hours of the day, and these are available for a long time period for each building separately.&lt;/p&gt;
&lt;p&gt;This dataset is very large, the &lt;code&gt;train.csv&lt;/code&gt; training dataset file is around 386Mb, and so some processes can be very slow and cumbersome using base R functions. This is a good example of using functions from &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;magrittr&lt;/code&gt; to manipulate the dataset.&lt;/p&gt;
&lt;div id=&#34;joining-and-structuring-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Joining and Structuring Data&lt;/h3&gt;
&lt;p&gt;We have two datasets to start with - &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt;, which are the training set data and the building metadata respectively. We can inspect what kind of variables are in these datasets initially by using base R functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   building_id meter           timestamp meter_reading
## 1           0     0 2016-01-01 00:00:00             0
## 2           1     0 2016-01-01 00:00:00             0
## 3           2     0 2016-01-01 00:00:00             0
## 4           3     0 2016-01-01 00:00:00             0
## 5           4     0 2016-01-01 00:00:00             0
## 6           5     0 2016-01-01 00:00:00             0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(metadata)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   site_id building_id primary_use square_feet year_built floor_count
## 1       0           0   Education        7432       2008          NA
## 2       0           1   Education        2720       2004          NA
## 3       0           2   Education        5376       1991          NA
## 4       0           3   Education       23685       2002          NA
## 5       0           4   Education      116607       1975          NA
## 6       0           5   Education        8000       2000          NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first goal is to combine these data sets together, and we can see that both data sets share the &lt;code&gt;building_id&lt;/code&gt; column, so these data sets need to be joined together by this. In base R, some combination of the &lt;code&gt;match&lt;/code&gt; function and subsetting would be required to do this, involving multiple lines of code and maybe some confusion. With the &lt;code&gt;right_join&lt;/code&gt; (or &lt;code&gt;left_join&lt;/code&gt;) function from &lt;code&gt;dplyr&lt;/code&gt;, the process is much simpler.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new.train &amp;lt;- train %&amp;gt;% right_join(metadata, by = &amp;quot;building_id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training data set has been updated in one line with the function &lt;code&gt;right_join&lt;/code&gt;, and it now includes the columns from both &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;building_id&amp;quot;   &amp;quot;meter&amp;quot;         &amp;quot;timestamp&amp;quot;     &amp;quot;meter_reading&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(new.train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;building_id&amp;quot;   &amp;quot;meter&amp;quot;         &amp;quot;timestamp&amp;quot;     &amp;quot;meter_reading&amp;quot;
## [5] &amp;quot;site_id&amp;quot;       &amp;quot;primary_use&amp;quot;   &amp;quot;square_feet&amp;quot;   &amp;quot;year_built&amp;quot;   
## [9] &amp;quot;floor_count&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When joining the data sets, the command &lt;code&gt;%&amp;gt;%&lt;/code&gt; was used. This is referred to as a ‘pipe’, and is a part of the &lt;code&gt;magrittr&lt;/code&gt; package. It iteratively performs operations such as the ones demonstrated above. In this case, it took the first input &lt;code&gt;train&lt;/code&gt;, a dataset, then performed the function &lt;code&gt;right_join&lt;/code&gt;, of which the first argument of the function was automatically defined as &lt;code&gt;train&lt;/code&gt;. Pipes are very useful in organising and structuring code, and allow you to neatly run a lot of commands in one line.&lt;/p&gt;
&lt;p&gt;Now, imagine that we wanted to look at the average meter reading for each type of building, grouped by respective primary use, at each hour of the day. We first need to restructure our data slightly to achieve this. Since our &lt;code&gt;timestamp&lt;/code&gt; column is a string containing the date and the time of the meter reading, we can subset this to just the first two characters of the time, which are in positions 12 and 13.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new.train$time = as.numeric(substr(new.train$timestamp, 12, 13))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has created a new column in the &lt;code&gt;new.train&lt;/code&gt; dataframe which corresponds to which hour the meter reading was taken. Now we can pipe the &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarise&lt;/code&gt; functions through &lt;code&gt;new.train&lt;/code&gt; to average over the different type of building and the time of day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanreadings = new.train %&amp;gt;% group_by(time, primary_use) %&amp;gt;% 
                    summarise(mean=mean(meter_reading))
meanreadings&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 384 x 3
## # Groups:   time [24]
##     time primary_use                    mean
##    &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;                         &amp;lt;dbl&amp;gt;
##  1     0 Education                     4488.
##  2     0 Entertainment/public assembly  448.
##  3     0 Food sales and service         335.
##  4     0 Healthcare                     757.
##  5     0 Lodging/residential            271.
##  6     0 Manufacturing/industrial       273.
##  7     0 Office                         484.
##  8     0 Other                          127.
##  9     0 Parking                        183.
## 10     0 Public services                264.
## # … with 374 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this has resulted in a &lt;em&gt;tibble&lt;/em&gt;, which is a form of a data frame. The same goals can be achieved with a tibble that would otherwise be achieved with the normal dataframe type.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-and-visualising-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plotting and Visualising Data&lt;/h3&gt;
&lt;p&gt;To visualise the average meter readings over the course of the day, we can use the &lt;code&gt;ggplot&lt;/code&gt; function, from the &lt;code&gt;ggplot2&lt;/code&gt; package. This is a plotting package that provides easy access to different types of graphics. It also allows structure within plots, as you can save a plot object as a variable. Additions to the plot can done by simply adding other layers to the existing object. This can be seen in action here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl &amp;lt;- ggplot(meanreadings) + geom_line(mapping = aes(time, mean, col=primary_use), size=1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initially, the &lt;code&gt;ggplot&lt;/code&gt; function was called on the data frame &lt;code&gt;meanreadings&lt;/code&gt;, initialising the plotting sequence, then the &lt;code&gt;geom_line&lt;/code&gt; layer was added. The &lt;code&gt;mapping&lt;/code&gt; argument defines what goes into the plot, so that &lt;code&gt;time&lt;/code&gt; is on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis, and &lt;code&gt;mean&lt;/code&gt; is on the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis. The specification of &lt;code&gt;col=primary_use&lt;/code&gt; in &lt;code&gt;mapping&lt;/code&gt; separated the different categories and plotted their lines separately on the same plot. We can see the plot by inspecting the &lt;code&gt;pl&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/tidyverse_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
This is an interesting plot, but doesn’t tell us much about the variation in meter readings on average during the day, for each building type. It does show that the ‘Education’ and ‘Services’ types of buildings on average require a lot more energy (or just have higher meter readings). To look at the variation between building types more closely, we can normalise each building type to be centred on zero by subtracting the mean across the average day and dividing by the standard deviation. This can be achieved by first creating a new data frame which includes both the mean and standard deviation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanreadings2 &amp;lt;- meanreadings %&amp;gt;% group_by(primary_use) %&amp;gt;% summarise(mean2 = mean(mean), sd = sd(mean)) %&amp;gt;% right_join(meanreadings, by = &amp;quot;primary_use&amp;quot;)
meanreadings2$norm.mean &amp;lt;- (meanreadings2$mean - meanreadings2$mean2)/meanreadings2$sd
ggplot(meanreadings2) + geom_line(aes(time, norm.mean, col=primary_use),size=1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/tidyverse_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On inspection of this plot, we can make some interpretations about the mean daily temperature based on the primary use of the building. Most of these buildings seem to follow a sine curve, where the meter readings increase at midday at a time of around 0600 to 1900. The meter readings also seem to be periodic, as at the end of the day they finish at around the value they started at. Most building types follow the same periodic structure, but we can see that the Manufacturing/industrial category has a higher peak in the morning, and the parking category has a lower peak in the evening.&lt;/p&gt;
&lt;p&gt;We can also split this analysis by season, and inspect how the season affects the mean meter readings per day. This can be achieved by first creating a new ‘season’ column in the original data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new.train$month = substr(new.train$timestamp, 6, 7)
new.train$season = new.train$month
new.train$season[new.train$season==&amp;quot;02&amp;quot; | new.train$season==&amp;quot;03&amp;quot; | new.train$season==&amp;quot;04&amp;quot;] = &amp;quot;Spring&amp;quot;
new.train$season[new.train$season==&amp;quot;05&amp;quot; | new.train$season==&amp;quot;06&amp;quot; | new.train$season==&amp;quot;07&amp;quot;] = &amp;quot;Summer&amp;quot;
new.train$season[new.train$season==&amp;quot;08&amp;quot; | new.train$season==&amp;quot;09&amp;quot; | new.train$season==&amp;quot;10&amp;quot;] = &amp;quot;Autumn&amp;quot;
new.train$season[new.train$season==&amp;quot;01&amp;quot; | new.train$season==&amp;quot;11&amp;quot; | new.train$season==&amp;quot;12&amp;quot;] = &amp;quot;Winter&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then by using the &lt;code&gt;group_by&lt;/code&gt;, &lt;code&gt;summarise&lt;/code&gt; and &lt;code&gt;right_join&lt;/code&gt; functions, we will make a data set that is grouped by season, time and primary use, to go into a plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seasonal_readings &amp;lt;- new.train %&amp;gt;% group_by(season, time, primary_use) %&amp;gt;% 
  summarise(mean_reading=mean(meter_reading)) 
seasonal_readings &amp;lt;- seasonal_readings %&amp;gt;% group_by(primary_use) %&amp;gt;% 
      summarise(mean_mean_reading = mean(mean_reading), sd = sd(mean_reading)) %&amp;gt;% right_join(seasonal_readings, by = &amp;quot;primary_use&amp;quot;)

seasonal_readings$norm.mean &amp;lt;- (seasonal_readings$mean_reading - seasonal_readings$mean_mean_reading)/seasonal_readings$sd
ggplot(seasonal_readings, aes(time, norm.mean, group=1,col=primary_use)) + 
  geom_line(aes(group=primary_use), size=1.2)+
  facet_wrap(~season) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/tidyverse_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first thing to note here is that the normalisation was done &lt;em&gt;before&lt;/em&gt; the seasonal split, which is why some building types have higher normalised mean in some seasons. For example, we can see that the education category uses on average more energy in the summer than in other seasons. There is also a general shift upwards for meter readings in winter, and downwards for summer, which could be due to requiring more energy for central heating.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>(Sparse) Matrices</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/matrices/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/matrices/</guid>
      <description>


&lt;div id=&#34;matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrices&lt;/h2&gt;
&lt;p&gt;A matrix is a two-dimensional data structure. The &lt;code&gt;matrix&lt;/code&gt; function is used to create matrices, and can have multiple arguments. You can specify the names of the columns and rows by supplying a list to the &lt;code&gt;dimnames&lt;/code&gt; argument, and can choose to populate the matrix by column (default) or by row with &lt;code&gt;byrow=TRUE&lt;/code&gt;. The function &lt;code&gt;as.matrix&lt;/code&gt; will convert a relevant argument to a matrix, and &lt;code&gt;is.matrix&lt;/code&gt; results a &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt; if the argument is or isn’t a matrix. We can see these here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A = matrix(1:12, nrow=3, ncol=4)
A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A = matrix(1:12, nrow=3, ncol=4, byrow=TRUE)
A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    5    6    7    8
## [3,]    9   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A = matrix(1:12, nrow=3, ncol=4, 
           dimnames = list(c(&amp;quot;Row1&amp;quot;, &amp;quot;Row2&amp;quot;, &amp;quot;Row3&amp;quot;), 
                           c(&amp;quot;Column1&amp;quot;, &amp;quot;Column2&amp;quot;, &amp;quot;Column3&amp;quot;, &amp;quot;Column4&amp;quot;)))
A&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Column1 Column2 Column3 Column4
## Row1       1       4       7      10
## Row2       2       5       8      11
## Row3       3       6       9      12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accessing elements in a matrix can be done by indexing over either the column or the row. &lt;code&gt;A[,i]&lt;/code&gt; will access the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th column, and &lt;code&gt;A[i,]&lt;/code&gt; will access the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th row. These arguments will return a vector, and will lose the structure of the matrix. For example, if we take the 1st column of &lt;code&gt;A&lt;/code&gt; we get&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Row1 Row2 Row3 
##    1    2    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is not a column any more! This is important to consider when working with matrices. To keep the structure of the matrix intact, we can specify &lt;code&gt;drop=FALSE&lt;/code&gt; when indexing, e.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A[,1,drop=FALSE]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Column1
## Row1       1
## Row2       2
## Row3       3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;array&lt;/code&gt; function in R works like a ‘stack of matrices’, and any number of dimensions can be specified. Instead of the &lt;code&gt;matrix&lt;/code&gt; function, &lt;code&gt;array&lt;/code&gt; takes one argument corresponding to the dimension, which is a vector; each element being the length of the corresponding dimension, i.e.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;array(1:27,c(3,3,3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    8
## [3,]    3    6    9
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]   10   13   16
## [2,]   11   14   17
## [3,]   12   15   18
## 
## , , 3
## 
##      [,1] [,2] [,3]
## [1,]   19   22   25
## [2,]   20   23   26
## [3,]   21   24   27&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Matrix multiplication between more than 2 matrices can also be sped up by precisely choosing the location of your brackets. Since matrix multiplication works right-to-left, the brackets need to be on the right side. For a large matrix, if we test the speeds of two forms of multiplication, we get&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N = 1000
M1 = matrix(rnorm(N^2),N,N)
M2 = matrix(rnorm(N^2),N,N)
M3 = matrix(rnorm(N^2),N,N)

system.time(M1 %*% M2 %*% M3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.462   0.172   0.194&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(M1 %*% (M2 %*% M3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.431   0.159   0.195&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(M1 %*% M2 %*% M3, M1 %*% (M2 %*% M3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So specification of brackets is quite a bit faster, and can speed up computation times for larger problems. Note that the function &lt;code&gt;all.equal&lt;/code&gt; checks whether the two arguments are the same within some tolerance, as they are not exactly the same (see later section on Numerical types in R).&lt;/p&gt;
&lt;div id=&#34;solving-linear-systems&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solving linear systems&lt;/h3&gt;
&lt;p&gt;Often a linear algebra problem we are interested in is solving &lt;span class=&#34;math inline&#34;&gt;\(A\boldsymbol{x} =\boldsymbol{b}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x},\boldsymbol{b} \in \mathbb{R}^n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A \in \mathbb{R}^{n\times n}\)&lt;/span&gt;. One solution to this is simply &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x} = A^{-1}\boldsymbol{b}\)&lt;/span&gt;, but the problem here is that &lt;em&gt;getting&lt;/em&gt; the matrix inverse, as it will take of order &lt;span class=&#34;math inline&#34;&gt;\(n^3\)&lt;/span&gt; operations. For example, a 1000x1000 matrix (which is not uncommon) will take around 1000&lt;span class=&#34;math inline&#34;&gt;\(^3\)&lt;/span&gt; = 1,000,000,000 operations, which is inefficient. If you did want to solve the system this way, the function for inverting a matrix in R is &lt;code&gt;solve&lt;/code&gt;, e.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A = matrix(rnorm(9),3,3)
solve(A)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]      [,3]
## [1,]  -8.888322   2.915525 -1.461660
## [2,]  30.044457 -10.021586  3.193355
## [3,] -11.184011   3.327956 -1.104080&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function can also take a second argument, being the right hand side of the system, which in our case is &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. This will roughly be the same as &lt;code&gt;solve(A) %*% b&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b = c(1,2,3)
solve(A) %*% b&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## [1,] -7.442254
## [2,] 19.581350
## [3,] -7.840339&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solve(A, b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -7.442254 19.581350 -7.840339&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although you can see the dimension of the output is different, &lt;code&gt;solve(A) %*% b&lt;/code&gt; maintains the column structure. However, the method &lt;code&gt;solve(A,b)&lt;/code&gt; is faster than &lt;code&gt;solve(A) %*% b&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-types-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Numerical types in R&lt;/h3&gt;
&lt;p&gt;If we were to find the ‘type’ of a normal integer in R, we get&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;typeof(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;double&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does it mean by a ‘double’? This means that it is a &lt;code&gt;binary64&lt;/code&gt; floating point number, i.e. the information stored in the computer for this value is stored in 64 bits; 1 bit for the &lt;em&gt;sign&lt;/em&gt; of the number, 11 bits for the &lt;em&gt;exponent&lt;/em&gt; and 52 bits for the &lt;em&gt;significant precision&lt;/em&gt;. So the largest number we can store is &lt;code&gt;2^1023&lt;/code&gt;, since&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2^1024&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Inf&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;simply returns &lt;code&gt;Inf&lt;/code&gt;. We know this number isn’t &lt;strong&gt;actually&lt;/strong&gt; infinity, but R recognises that it is too large, and anything over the largest number is stored as the highest possible value. This also means that really &lt;em&gt;small&lt;/em&gt; numbers aren’t stored correctly either. There is always some form of &lt;strong&gt;floating point error&lt;/strong&gt; in R, of order 2&lt;span class=&#34;math inline&#34;&gt;\(^{-52}\)&lt;/span&gt;. Showing this in practice:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(1 + 2^(-52), digits=22)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.000000000000000222045&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(1 + 2^(-53), digits=22)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another format R can store numbers in is in the format ‘Long’, specified by a &lt;code&gt;L&lt;/code&gt; after the number.&lt;/p&gt;
&lt;div id=&#34;effect-on-matrices&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Effect on Matrices&lt;/h4&gt;
&lt;p&gt;Any form of arithmetic in R is going to be affected by floating point error. Most of the time it does not cause any problems though, as it will only affect things at &lt;em&gt;really small&lt;/em&gt; or &lt;em&gt;really large&lt;/em&gt; magnitudes. Matrices are specifically succeptible to floating point errors however, as matrix multiplication contains many operations.&lt;/p&gt;
&lt;p&gt;Let’s look at some simple matrix multiplication on large matrices and inspect whether there are floating point errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N = 100  # Square Number
A = matrix(rnorm(N),sqrt(N),sqrt(N))
B = matrix(rnorm(N),sqrt(N),sqrt(N))
C = matrix(rnorm(N),sqrt(N),sqrt(N))

test = c(solve(A %*% (B %*% C)) - solve(A %*% B %*% C))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since these two operations are the same, all entries in the matrix &lt;code&gt;test&lt;/code&gt; should be zero. However, this is not the case, as seen below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -1.774e-11 -1.969e-12  2.685e-13  3.719e-14  2.575e-12  1.345e-11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is due to floating point error. Not all entries in &lt;code&gt;test&lt;/code&gt; are zero, but they are very small. Most of the time, this might not make much of a difference, but when performing calculations involving small numbers this is important to consider.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sparse-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sparse Matrices&lt;/h2&gt;
&lt;p&gt;A sparse matrix is one where most of the entries are zero. The problem with sparse matrices in programming is that a very large matrix (for example a &lt;span class=&#34;math inline&#34;&gt;\(10000 \times 10000\)&lt;/span&gt; matrix), the computer would store &lt;em&gt;every&lt;/em&gt; element in the matrix, even though most are zero. There are various package for dealing with sparse matrices in a better way in R, but the most popular is the &lt;code&gt;Matrix&lt;/code&gt; package. This package extends the base R functionality with both sparse and dense matrices, allolwing more operations and more efficient calculations. In this package, we can use the function &lt;code&gt;rankMatrix&lt;/code&gt; to return the rank of a input matrix. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A = matrix(rnorm(25),5,5)
c(rankMatrix(A))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A sparse matrix can be stored as a &lt;code&gt;dgCMatrix&lt;/code&gt; (where the &lt;code&gt;C&lt;/code&gt; stands for storing by column, other options are row or triplet). Let’s look at the difference between storing a sparse matrix in this way against the default way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A = matrix(0, 1000, 1000)
for(i in 1:1000) A[sample(1:1000,50,1),i] = sample(1:10,50,replace=TRUE)
B = Matrix(A, sparse=TRUE)
A[1:4,1:15]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
## [1,]    0    0    0    0    0    0    0    0    0     0     0     0     0     0
## [2,]    0    0    0    0    0    0    0    0    0     3     0     0     0     0
## [3,]    0    0    0    7    0    0    0    0    0     0     6     0     0     0
## [4,]    4    0    0    0    0    0   10    0    0     0     0     0     0     0
##      [,15]
## [1,]     0
## [2,]     0
## [3,]     0
## [4,]     0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B[1:4,1:15]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4 x 15 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                                    
## [1,] . . . . . .  . . . . . . . . .
## [2,] . . . . . .  . . . 3 . . . . .
## [3,] . . . 7 . .  . . . . 6 . . . .
## [4,] 4 . . . . . 10 . . . . . . . .&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(object.size(A),object.size(B))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8000216  590616&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the sparse matrix is stored at a much smaller object size than a normal matrix. Note that the &lt;code&gt;Matrix&lt;/code&gt; function is part of the &lt;code&gt;Matrix&lt;/code&gt; package, not to be confused with &lt;code&gt;matrix&lt;/code&gt; from base R. The conversion to being stored as a sparse &lt;code&gt;dgCMatrix&lt;/code&gt; was done after construction of the matrix, but it could be constructed as a sparse matrix from the start. We can inspect the &lt;code&gt;dgCMatrix&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(B)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Formal class &amp;#39;dgCMatrix&amp;#39; [package &amp;quot;Matrix&amp;quot;] with 6 slots
##   ..@ i       : int [1:48759] 3 15 46 61 70 72 87 116 144 170 ...
##   ..@ p       : int [1:1001] 0 50 100 148 197 245 294 344 394 444 ...
##   ..@ Dim     : int [1:2] 1000 1000
##   ..@ Dimnames:List of 2
##   .. ..$ : NULL
##   .. ..$ : NULL
##   ..@ x       : num [1:48759] 4 10 5 6 6 8 6 9 8 4 ...
##   ..@ factors : list()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has a few pieces of information relating to the non-zero elements of the matrix &lt;code&gt;B&lt;/code&gt;. The often most interesting ones being the &lt;code&gt;i&lt;/code&gt; attribute: the locations of the non-zero entries, and the &lt;code&gt;x&lt;/code&gt; attribute: the non-zero entries in these corresponding spots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Sparse matrices can have relevant application in many scenarios. For example, in a modelling problem where you want to model the effects of different categorical predictors, you can use ‘one-hot encoding’. This replaces a multi-class input &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x} \in \{0, 1, \dots, K \}^n\)&lt;/span&gt; with a vector of 1’s and 0’s, where the location of the 1’s correspond to which class is being represented. For example, if we set up a vector of factors in R, we have&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x.factor = factor(sample(c(&amp;quot;Class1&amp;quot;, &amp;quot;Class2&amp;quot;, &amp;quot;Class3&amp;quot;, &amp;quot;Class4&amp;quot;), 20, replace=TRUE))
x.factor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] Class1 Class4 Class3 Class1 Class2 Class1 Class3 Class3 Class2 Class2
## [11] Class3 Class3 Class1 Class1 Class1 Class2 Class2 Class2 Class2 Class3
## Levels: Class1 Class2 Class3 Class4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When fitting a model, we typically will add these entries to a model matrix, and work with that. The &lt;code&gt;model.matrix&lt;/code&gt; function in R will automatically set up one-hot encoding in this scenario.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M = model.matrix(~x.factor)
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    (Intercept) x.factorClass2 x.factorClass3 x.factorClass4
## 1            1              0              0              0
## 2            1              0              0              1
## 3            1              0              1              0
## 4            1              0              0              0
## 5            1              1              0              0
## 6            1              0              0              0
## 7            1              0              1              0
## 8            1              0              1              0
## 9            1              1              0              0
## 10           1              1              0              0
## 11           1              0              1              0
## 12           1              0              1              0
## 13           1              0              0              0
## 14           1              0              0              0
## 15           1              0              0              0
## 16           1              1              0              0
## 17           1              1              0              0
## 18           1              1              0              0
## 19           1              1              0              0
## 20           1              0              1              0
## attr(,&amp;quot;assign&amp;quot;)
## [1] 0 1 1 1
## attr(,&amp;quot;contrasts&amp;quot;)
## attr(,&amp;quot;contrasts&amp;quot;)$x.factor
## [1] &amp;quot;contr.treatment&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This matrix is primarily made up of 1’s and 0’s, and so would be better suited to being stored as a sparse matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Matrix(M, sparse=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 20 x 4 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##    (Intercept) x.factorClass2 x.factorClass3 x.factorClass4
## 1            1              .              .              .
## 2            1              .              .              1
## 3            1              .              1              .
## 4            1              .              .              .
## 5            1              1              .              .
## 6            1              .              .              .
## 7            1              .              1              .
## 8            1              .              1              .
## 9            1              1              .              .
## 10           1              1              .              .
## 11           1              .              1              .
## 12           1              .              1              .
## 13           1              .              .              .
## 14           1              .              .              .
## 15           1              .              .              .
## 16           1              1              .              .
## 17           1              1              .              .
## 18           1              1              .              .
## 19           1              1              .              .
## 20           1              .              1              .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the case of a large data set, when using categorical variables, this will speed up computation time quite significantly.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Object Oriented and Functional Programming</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/oop/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/oop/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Object oriented programming (OOP) is a programming language model that defines &lt;em&gt;objects&lt;/em&gt;; which are elements in R that contain &lt;em&gt;attributes&lt;/em&gt;, or &lt;em&gt;fields&lt;/em&gt;, which have some specification in the definition of the object itself. Objects are defined in advance, and are very useful in conceptualising coding goals, and allowing the end-user a better experience when using your functions and/or code.&lt;/p&gt;
&lt;p&gt;Base R has three different ways of defining objects, which are the three different models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S3&lt;/li&gt;
&lt;li&gt;S4&lt;/li&gt;
&lt;li&gt;Reference Class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of which have their merits and disadvantages. S3 is the simplest model, and is useful for defining a basic object. S4 is more complex, as classes have to be defined explicitly, but adds more clarity and allows inclusion of integrity checks. Reference Class is more complex again, but further improves on teh structure of the class definition, through incorporation of a higher degree of encapsulation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples-dealing-cards-in-reference-class&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples: Dealing Cards in Reference Class&lt;/h2&gt;
&lt;p&gt;For this example, I will be using the Reference Class model. This example is concerned with being able to deal a card from a standard card deck. To start with, we make a class called &lt;code&gt;Card&lt;/code&gt; which will contain two properties; the suit and the value. This is set up as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Card &amp;lt;- setRefClass(&amp;quot;Card&amp;quot;,
                    fields = c(
                      suit = &amp;quot;character&amp;quot;,
                      value = &amp;quot;numeric&amp;quot;,
                      pairs = &amp;quot;numeric&amp;quot;
                    ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;setRefClass&lt;/code&gt; function is used to create this class, and it has the two attributes that are required of a standard card. We can set up a function to deal a random card from a deck by now specifying two more commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dealHand = function(n){
  y &amp;lt;- Card$new(n)
  return(y)
}

Card$methods(
  initialize = function(n){
    suits &amp;lt;- c(&amp;quot;Diamonds&amp;quot;,&amp;quot;Hearts&amp;quot;,&amp;quot;Clubs&amp;quot;,&amp;quot;Spades&amp;quot;)
    s &amp;lt;- sample(0:51, n)
    
    .self$suit &amp;lt;- suits[(s %/% 13) + 1]
    .self$value &amp;lt;-  (s %% 13)+1
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;dealHand&lt;/code&gt; has its only input as &lt;code&gt;n&lt;/code&gt;, which is the size of the hand. The assignment here is given by the &lt;code&gt;initialize&lt;/code&gt; method in the &lt;code&gt;$methods&lt;/code&gt; substructure of &lt;code&gt;Card&lt;/code&gt;. By setting the method of &lt;code&gt;initialize&lt;/code&gt; to randomly sample both value and suit, this will deal a random card every time that &lt;code&gt;dealCard(n)&lt;/code&gt; is run. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dealHand(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Reference class object of class &amp;quot;Card&amp;quot;
## Field &amp;quot;suit&amp;quot;:
## [1] &amp;quot;Diamonds&amp;quot; &amp;quot;Spades&amp;quot;   &amp;quot;Clubs&amp;quot;    &amp;quot;Hearts&amp;quot;   &amp;quot;Spades&amp;quot;  
## Field &amp;quot;value&amp;quot;:
## [1]  9  7  5  4 11
## Field &amp;quot;pairs&amp;quot;:
## numeric(0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also add another method that will recognise if there are any pairs in the hand that has been dealt. This is done by adding an additional method to &lt;code&gt;Card$methods&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Card$methods(
  initialize = function(n){
    suits &amp;lt;- c(&amp;quot;Diamonds&amp;quot;,&amp;quot;Hearts&amp;quot;,&amp;quot;Clubs&amp;quot;,&amp;quot;Spades&amp;quot;)
    s &amp;lt;- sample(0:51, n)
    
    .self$suit &amp;lt;- suits[(s %/% 13) + 1]
    .self$value &amp;lt;-  (s %% 13) + 1
  },
  getPairs = function(){
    .self$pairs &amp;lt;- as.numeric(names(table(.self$value))[table(.self$value)&amp;gt;=2])
  }
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that now, if we are dealt a hand , we can see how many pairs there are in the hand, and what the value of the pair is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2)
hand = dealHand(5)
hand$getPairs()
hand&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Reference class object of class &amp;quot;Card&amp;quot;
## Field &amp;quot;suit&amp;quot;:
## [1] &amp;quot;Hearts&amp;quot;   &amp;quot;Hearts&amp;quot;   &amp;quot;Diamonds&amp;quot; &amp;quot;Spades&amp;quot;   &amp;quot;Clubs&amp;quot;   
## Field &amp;quot;value&amp;quot;:
## [1]  8  2  6 11  6
## Field &amp;quot;pairs&amp;quot;:
## [1] 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our hand here contains the 8 of Hearts, the 2 of Hearts, the 6 of Diamonds, the Jack (11) of Spades, and the 6 of Clubs. So we have two sixes, and one pair. The class &lt;code&gt;hand&lt;/code&gt; now has a new entry, a field named &lt;code&gt;pairs&lt;/code&gt;, which contains the number 6, showing 6 is our only pair.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;functional-programming&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Functional Programming&lt;/h2&gt;
&lt;p&gt;Functional programmings is (obviously) focused on using functions. We call functions ‘first class’, because they&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can be embedded into lists/dataframes&lt;/li&gt;
&lt;li&gt;Can be an argument to another function&lt;/li&gt;
&lt;li&gt;Can be returned by other functions&lt;/li&gt;
&lt;li&gt;And more&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can consider functions as another type of variable, as you would store and use them in similar ways. For example, consider the list of functions&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mylist = list(add_function = function(x,y) x+y, subtract_function = function(x,y) x-y)
mylist$add_function(1,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mylist$subtract_function(2,1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see how this could be useful, in setting a list of different functions. Applications could include including a list of link functions in some form of regression, or basis functions. Functions can also return other functions, consider&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_link_function = function(which_f){
  if(which_f == &amp;quot;exponential&amp;quot;) f = function(x) exp(x)
  if(which_f == &amp;quot;identity&amp;quot;) f = function(x) x
  return(f)
}
link_function = make_link_function(&amp;quot;exponential&amp;quot;)
link_function(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]   2.718282   7.389056  20.085537  54.598150 148.413159&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a style of function output that could be used in making a general linear model, for example, to link the parameter to the predictor.&lt;/p&gt;
&lt;p&gt;In regards to functional programming, there are some important definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pure functions&lt;/strong&gt;: A function which always gives the same output for the same inputs, and does not have any side effects. An impure function will be one that, for example, generates (pseudo) random numbers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closures&lt;/strong&gt;: A function that outputs another function, but contains a closed variable that is defined only within the main function itself and not in global variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lazy evaluation&lt;/strong&gt;: The inputs to a function come from the global variables, instead of what was previously defined. For example the function &lt;code&gt;function(exp) function(x) x^exp&lt;/code&gt; returns a function that will raise &lt;code&gt;x&lt;/code&gt; to the power of &lt;code&gt;exp&lt;/code&gt;. If you change the value of &lt;code&gt;exp&lt;/code&gt; after defining the first function, it will change what power &lt;code&gt;x&lt;/code&gt; is raised to later on, even though the function was defined before &lt;code&gt;exp&lt;/code&gt; was changed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Optimisation</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/optimisation/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/optimisation/</guid>
      <description>


&lt;div id=&#34;numerical-optimisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numerical Optimisation&lt;/h2&gt;
&lt;p&gt;The general idea in optimisation is to find a &lt;em&gt;minimum&lt;/em&gt; (or &lt;em&gt;maximum&lt;/em&gt;) of some function. Generally, our problem has the form
&lt;span class=&#34;math display&#34;&gt;\[
\min_{\boldsymbol{x}} f(\boldsymbol{x}).
\]&lt;/span&gt;
Sometimes our problem can be &lt;em&gt;constrained&lt;/em&gt;, which would take the general form
&lt;span class=&#34;math display&#34;&gt;\[
\min_{\boldsymbol{x}} f(\boldsymbol{x}) 
\]&lt;/span&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{subject to } g_i(x) \leq 0
\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,m\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f:\mathbb{R}^n \to \mathbb{R}\)&lt;/span&gt;. These are important problems to solve, and it is often that there is no analytical solution to the problem, or the analytical solution is unavailable. This portfolio will explain the most popular numerical optimisation methods, and those readily available in R.&lt;/p&gt;
&lt;div id=&#34;optimising-a-complicated-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Optimising a complicated function&lt;/h3&gt;
&lt;p&gt;To demonstrate the different optimisation methods, the speeds and abilities of each, consider optimising the Rastrigin function. This is a non-convex function that takes the form
&lt;span class=&#34;math display&#34;&gt;\[
f(\boldsymbol{x}) = An + \sum^n_{i=1} [x_i^2-A\cos(2\pi x_i)],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the length of the vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt;. We can plot this function in 3D using the &lt;code&gt;plotly&lt;/code&gt; package to inspect it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = function(x) A*n + sum(x^2 - A*cos(2*pi*x))
A = 5
n = 2
x1 = seq(-10,10,length=100)
x2 = seq(-10,10,length=100)
xy = expand.grid(x1,x2)
z = apply(xy,1,f)
dim(z) = c(length(x1),length(x2))
z.plot = list(x=x1, y=x2, z=z)
image(z.plot, xlab = &amp;quot;x&amp;quot;, ylab = &amp;quot;y&amp;quot;, main = &amp;quot;Rastrigin Function&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/optimisation_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;

So we are interested in optimising the function &lt;code&gt;f&lt;/code&gt;. We can see from inspection of the plot that there is a global minimum at &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x} = \boldsymbol{0}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{0}) = 0\)&lt;/span&gt;, and likewise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f(c(0,0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we will be evaluating optimisation methods based on how close they get to this true solution. We continue this portfolio by explaining the different optimisation methods, and evaluating their performance in finding the global minimum of the Rastrigin function.&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(n=2\)&lt;/span&gt;, the gradient and hessian for this function can be calculated analytically:
&lt;span class=&#34;math display&#34;&gt;\[
\nabla f(\boldsymbol{x}) = \begin{pmatrix}
2 x_1 + 2\pi A \sin(2\pi x_1) \\
2 x_2 + 2\pi A \sin(2\pi x_2)
\end{pmatrix}
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\nabla^2 f(\boldsymbol{x}) = \begin{pmatrix}
2 + 4\pi^2 A \cos (2\pi x_1) &amp;amp; 0 \\
0 &amp;amp; 2 + 4\pi^2 A \cos (2\pi x_2)
\end{pmatrix}
\]&lt;/span&gt;
We can construct these functions in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grad_f = function(x) {
  c(2*x[1] + 2*pi*A*sin(2*pi*x[1]),
    2*x[2] + 2*pi*A*sin(2*pi*x[2]) )
}
hess_f = function(x){
  H11 = 2 + 4*pi^2*A*sin(2*pi*x[1])
  H22 = 2 + 4*pi^2*A*sin(2*pi*x[2])
  return(matrix(c(H11,0,0,H22),2,2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These analytical forms of the gradient and hessian can be supplied to various optimisation algorithms to speed up convergence.&lt;/p&gt;
&lt;p&gt;Optimisation problems can be one or multi dimensional, where the dimension refers to the size of the parameter vector, in our case &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Generally, one-dimensional problems are easier to solve, as there is only one parameter value to optimise over. In statistics, we are often interested in multi-dimensional optimisation. For example, in maximum likelihood estimation we are trying to find parameter values that maximise a likelihood function, for any number of parameters. For the Rastrigin function in our example, we have taken the dimension &lt;span class=&#34;math inline&#34;&gt;\(n=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;optimisation-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimisation Methods&lt;/h2&gt;
&lt;div id=&#34;gradient-descent-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gradient Descent Methods&lt;/h3&gt;
&lt;p&gt;Iterative algorithms take the form
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{x}_{k+1} = \boldsymbol{x}_k + t \boldsymbol{d}_k, \: \: \text{ for iterations } k=0,1,\dots, 
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{d}_k \in \R^n\)&lt;/span&gt; is the descent direction, &lt;span class=&#34;math inline&#34;&gt;\(t_k\)&lt;/span&gt; is the stepsize.
&lt;span class=&#34;math display&#34;&gt;\[
f&amp;#39;(\boldsymbol{x}; \boldsymbol{d})=\nabla f(\boldsymbol{x})^T \boldsymbol{d} &amp;lt; 0.
\]&lt;/span&gt;
So moving &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt; in the descent direction for timestep &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; decreases the function, so we move towards a minimum.
The  is the negative gradient of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{d}_k = -\nabla f(\boldsymbol{x}_k)\)&lt;/span&gt;, or normalised &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{d}_k = {-\nabla f(\boldsymbol{x}_k)}/{\norm{\nabla f(\boldsymbol{x})}}\)&lt;/span&gt;. We can construct a general gradient descent method in R and evaluate performance on optimising the Rastrigin function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gradient_method = function(f, x, gradient, eps=1e-4, t=0.1, maxiter=1000){
  converged = TRUE
  iterations = 0
  while((!all(abs(gradient(x)) &amp;lt; eps))){
    if(iterations &amp;gt; maxiter){
      cat(&amp;quot;Not converged, stopping after&amp;quot;, iterations, &amp;quot;iterations \n&amp;quot;)
      converged = FALSE
      break
    }
    gradf = gradient(x)
    d = -gradf/abs(gradf)
    x = x - t*gradf
    iterations = iterations + 1
  } 
  if(converged) {cat(&amp;quot;Number of iterations:&amp;quot;, iterations, &amp;quot;\n&amp;quot;)
                 cat(&amp;quot;Converged!&amp;quot;)}
  return(list(f=f(x),x=x))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code essentially will continue running the while loop until the tolerance condition is satisfied, where the change in &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt; from one iteration to another is negligible. Now we can see in which cases this will provide a solution to the problem of the Rastrigin function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gradient_method(f, x = c(1, 1), grad_f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Not converged, stopping after 1001 iterations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $f
## [1] 20.44268
## 
## $x
## [1] -3.085353 -3.085353&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gradient_method(f, x = c(.01, .01), grad_f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Not converged, stopping after 1001 iterations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $f
## [1] 17.82949
## 
## $x
## [1] -2.962366 -2.962366&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even when the initial guess of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; was very close to zero, the true solution, this function did not converge. This shows that under a complex and highly varying function such as the Rastrigin function, the gradient method has problems. This can be improved by including a backtracking line search to dynamically change the value of the stepsize &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(t_k\)&lt;/span&gt; for each iteration &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. This method reduces the stepsize &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; for each iteration &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; via &lt;span class=&#34;math inline&#34;&gt;\(t_k = \beta t_k\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(\beta \in (0,1)\)&lt;/span&gt; while
&lt;span class=&#34;math display&#34;&gt;\[
f(\boldsymbol{x}_k) - f(\boldsymbol{x}_k + t_k \boldsymbol{d}_k) &amp;lt; -\alpha\nabla f(\boldsymbol{x}_k)^T \boldsymbol{d}_k.
\]&lt;/span&gt;
and for &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in (0,1)\)&lt;/span&gt;. We can add this to the gradient method function with the line &lt;code&gt;while( (f(x) - f(x + t*d) ) &amp;lt; (-alpha*t * t(gradf)%*%d)) t = beta*t&lt;/code&gt;. Meaning we need to specify &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. After this is added to the function, we have&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gradient_method(f, c(0.01,0.01), grad_f, maxiter = 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Number of iterations: 1255 
## Converged!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $f
## [1] 5.002503e-09
## 
## $x
## [1] 5.008871e-06 5.008871e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we finally have convergence! However, this is for when the initial guess was very close to the actual solution, and so in more realistic cases where we don’t know this true solution, this method is likely inefficient and inaccurate. The Newton method is an advanced form of the basic gradient descent method.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;newton-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Newton Methods&lt;/h3&gt;
&lt;p&gt;The Newton method seeks to solve the optimisation problem using evaluations of Hessians and a quadratic approximation of a function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; around &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}_k\)&lt;/span&gt;. This is under the assumption is that the Hessian &lt;span class=&#34;math inline&#34;&gt;\(\nabla^2 f(\boldsymbol{x}_k)\)&lt;/span&gt; is . The unique minimiser of the quadratic approximation is
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{x}_{k+1} = \boldsymbol{x}_k - (\nabla^2 f(\boldsymbol{x}_k))^{-1} \nabla f(\boldsymbol{x}_k),
\]&lt;/span&gt;
which is known as . Here you can consider &lt;span class=&#34;math inline&#34;&gt;\((\nabla^2 f(\boldsymbol{x}_k))^{-1} \nabla f(\boldsymbol{x}_k)\)&lt;/span&gt; as the descent direction in a scaled gradient method. The &lt;code&gt;nlm&lt;/code&gt; function from base R uses the Newton method. It is an expensive algorithm to run, because it involves inverting a matrix, the hessian matrix of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. Newton methods work a lot better if you can supply an algebraic expression for the hessian matrix, so that you do not need to numerically calculate the gradient on each iteration. We can use &lt;code&gt;nlm&lt;/code&gt; to test the Newton method on the Rastrigin function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_fornlm = function(x){
  out = f(x)
  attr(out, &amp;#39;gradient&amp;#39;) &amp;lt;- grad_f(x)
  attr(out, &amp;#39;hessian&amp;#39;) &amp;lt;-  hess_f(x)
  return(out)
}
nlm(f, c(-4, 4), check.analyticals = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $minimum
## [1] 3.406342e-11
## 
## $estimate
## [1] -4.135221e-07 -4.131223e-07
## 
## $gradient
## [1] 1.724132e-05 1.732303e-05
## 
## $code
## [1] 2
## 
## $iterations
## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this converged to the true solution in a surprisingly small number of iterations. The likely reason for this is due to Newton’s method using a quadratic approximation, and the Rastrigin function taking a quadratic form.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bfgs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;BFGS&lt;/h3&gt;
&lt;p&gt;In complex cases, the hessian cannot be supplied analytically. Even if it can be supplied analytically, in high dimensions the hessian is a very large matrix, which makes it computationally expensive to invert for each iteration. The BFGS method approximates the hessian matrix, increasing computability and efficiency. The BFGS method is the most common quasi-Newton method, and it is one of the methods that can be suppled to the &lt;code&gt;optim&lt;/code&gt; function. It approximates the hessian matrix with &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt;, and for iterations &lt;span class=&#34;math inline&#34;&gt;\(k=0,1,\dots\)&lt;/span&gt;, it has the following basic algorithm:&lt;/p&gt;
&lt;p&gt;Initialise &lt;span class=&#34;math inline&#34;&gt;\(B_0 = I\)&lt;/span&gt; and initial guess &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Obtain a direction &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{d}_k\)&lt;/span&gt; through the solution of &lt;span class=&#34;math inline&#34;&gt;\(B_k \boldsymbol{d}_k = - \nabla f(\boldsymbol{x}_k)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Obtain a stepsize &lt;span class=&#34;math inline&#34;&gt;\(t_k\)&lt;/span&gt; by line search &lt;span class=&#34;math inline&#34;&gt;\(t_k = \text{argmin} f(\boldsymbol{x}_k + t\boldsymbol{d}_k)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Set &lt;span class=&#34;math inline&#34;&gt;\(s_k = t_k \boldsymbol{d}_k\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}_{k+1} = \boldsymbol{x}_k + \boldsymbol{s}_k\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Set &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y}_k = \nabla f(\boldsymbol{x}_{k+1}) - \nabla f(\boldsymbol{x}_k)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update the hessian approximation &lt;span class=&#34;math inline&#34;&gt;\(B_{k+1} = B_k + \frac{\boldsymbol{y}_k\boldsymbol{y}_k^T}{\boldsymbol{y}_k^T \boldsymbol{s}_k} - \frac{B_k \boldsymbol{s}_k \boldsymbol{s}_k^T B_k}{\boldsymbol{s}_k^T B_k \boldsymbol{s}_k}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;BFGS is the fastest method that is guaranteed convergence, but has its downsides. BFGS stores the matrices &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt; in memory, so if your dimension is high (i.e. a large amount of parameters), these matrices are going to be large and storing them is inefficient. Another version of BFGS is the low memory version of BFGS, named ‘L-BGFS’, which only stores some of the vectors that &lt;em&gt;represent&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(B_k\)&lt;/span&gt;. This method is almost as fast. In general, you should use BFGS if you can, but if your dimension is too high, reduce down to L-BFGS.&lt;/p&gt;
&lt;p&gt;This is a very good but complicated method. Luckily, the function &lt;code&gt;optim&lt;/code&gt; from the &lt;code&gt;stats&lt;/code&gt; package in R has the ability to optimise with the BFGS method. Testing this on the Rastrigin function gives&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;optim(c(1,1), f, method=&amp;quot;BFGS&amp;quot;, gr = grad_f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $par
## [1] 0.9899629 0.9899629
## 
## $value
## [1] 1.979932
## 
## $counts
## function gradient 
##       19        3 
## 
## $convergence
## [1] 0
## 
## $message
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;optim(c(.1,.1), f, method=&amp;quot;BFGS&amp;quot;, gr = grad_f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $par
## [1] 4.61081e-10 4.61081e-10
## 
## $value
## [1] 0
## 
## $counts
## function gradient 
##       31        5 
## 
## $convergence
## [1] 0
## 
## $message
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the BFGS method actually didn’t find the true solution for an initial value of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x} = (1,1)\)&lt;/span&gt;, but did for when the initial value was &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x} = (0.1,0.1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;non-linear-least-squares-optimisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-Linear Least Squares Optimisation&lt;/h2&gt;
&lt;p&gt;The motivating example we have used throughout this section was concerned with optimising a two-dimensional function, of which we were only interested in two variables that controlled the value of the function &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{x})\)&lt;/span&gt;. In many cases, we have a dataset &lt;span class=&#34;math inline&#34;&gt;\(D = \{\boldsymbol{y},\boldsymbol{x}_i\}\)&lt;/span&gt;, where we decomopose the ‘observations’ as &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{y} = g(\boldsymbol{x}) + \epsilon\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is a random noise parameter. In this case we are interested in finding an approximation to the data generating function &lt;span class=&#34;math inline&#34;&gt;\(g(\boldsymbol{x})\)&lt;/span&gt;, which we call &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{x},\boldsymbol{\beta})\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; are some parameters of whose relationship with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt; we model to make this approximation, so we are interested in optimising over these parameters. The objective function we are minimising over is
&lt;span class=&#34;math display&#34;&gt;\[
\min_{\boldsymbol{\beta}} \sum^n_{i=1} r_i^2 = \min_{\boldsymbol{\beta}} \sum^n_{i=1} (y_i - f(x_i,\boldsymbol{\beta}))^2, 
\]&lt;/span&gt;
i.e. the squared difference between the observed dataset and the approximation to the data generating function that defines that dataset. Here, &lt;span class=&#34;math inline&#34;&gt;\(r_i = y_i - f(x_i,\boldsymbol{\beta})\)&lt;/span&gt; is known as the &lt;em&gt;residuals&lt;/em&gt;, and it is of the most interest in a least squares setting. Many optimisation methods are specifically designed to optimise the least squares problem, but all optimisation methods can be used (provided they find a minimum). Some of the most popular algorithms for least squares are the Gauss-Newton algorithm and the Levenberg-Marquardt algorithm. Both of these algorithms are extensions of Newton’s method for general optimisation. The general form of the Gauss-Newton method is
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{\beta} \leftarrow \boldsymbol{\beta} - (J_r^TJ_r)^{-1}J_r^Tr_i,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(J_r\)&lt;/span&gt; is the Jacobian matrix of the residue &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, defined as
&lt;span class=&#34;math display&#34;&gt;\[
J_r = \frac{\partial r}{\partial \boldsymbol{\beta}}.
\]&lt;/span&gt;
So this is defined as the matrix of partial derivatives with respect to each coefficient &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt;. The Levenberg-Marquardt algorithm extends this approach by including a diagonal matrix of small entries to the &lt;span class=&#34;math inline&#34;&gt;\(J_r^TJ_r\)&lt;/span&gt; term, to eliminate the possibility of this being a singular matrix. This has the update process of
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{\beta} \leftarrow \boldsymbol{\beta} - (J_r^TJ_r+\lambda I)^{-1}J_r^Tr_i,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is some small value. In the simple case where &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0\)&lt;/span&gt;, this reduces to the Gauss-Newton algorithm. This is a highly efficient method, but in the case where our dataset is large, we may want to use stochastic gradient descent.&lt;/p&gt;
&lt;div id=&#34;stochastic-gradient-descent&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;Stochastic Gradient Descent (SGD) is a stochastic approximation to the standard gradient descent method. Instead of calculating the gradient for an entire dataset (which can be extremely large) it calculates the gradient for a lower-dimensional subset of the dataset; picked randomly or deterministically. The form of this method is
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{x}_{k+1} = \boldsymbol{x}_k - t \nabla f_i(\boldsymbol{x}_k)
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is an index that refers to cycling through all points &lt;span class=&#34;math inline&#34;&gt;\(i \in D\)&lt;/span&gt;, the points in the dataset. This can be in different sizes of groups, so depending on the problem, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; can be large or small (relative to the size of the dataset). Stochastic gradient methods are useful in the setting where your dataset is very large, otherwise it could be unnecessary.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Numerical Integration</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc1/integration/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc1/integration/</guid>
      <description>


&lt;div id=&#34;numerical-integration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numerical Integration&lt;/h2&gt;
&lt;p&gt;Calculating a definite integral of the form
&lt;span class=&#34;math display&#34;&gt;\[
\int^b_a f(x) dx
\]&lt;/span&gt;
can be difficult when an analytical solution is not possible.
We are primarily interested in integration in statistics because we want to be able to compute expectations, i.e.
&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}(X) = \int xf(x)dx.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is easier in one dimension, but as the number of dimensions increases then the methods required become more complex.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-dimensional-case&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One-dimensional Case&lt;/h2&gt;
&lt;p&gt;In one dimension, we could use some method involving solving an ODE, of which many methods exist. Another way is &lt;strong&gt;quadrature&lt;/strong&gt;; approximating the integral using multiple points across the curve and taking areas at each point.&lt;/p&gt;
&lt;div id=&#34;quadrature&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quadrature&lt;/h3&gt;
&lt;p&gt;One of the most basic methods to approximate an integral involves collecting a series of shapes or ‘bins’ underneath the a curve, of which the area is known for each bin, and approximating the integral as the sum of the area of these shapes. To do this, we need to estimate the curve for which we are integrating, so we can get points at which to estimate these bins. This can be estimated with polynomial methods. The &lt;strong&gt;Weierstrass Approximation Theorem&lt;/strong&gt; states that there exists a polynomial which can be used to approximate a given continuous function, within a tolerance. More formally, for &lt;span class=&#34;math inline&#34;&gt;\(f \in C^0([a,b])\)&lt;/span&gt;, there exists a sequence of polynomials &lt;span class=&#34;math inline&#34;&gt;\(p_n\)&lt;/span&gt; that converges uniformly to &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; on the interval &lt;span class=&#34;math inline&#34;&gt;\([a,b]\)&lt;/span&gt;, i.e.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
||f-p_n||_{\infty} = \max_{x \in [a,b]} |f(x)-p_n(x)| \to 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are many ways to approximate this polynomial, and the obvious way of doing this is by uniformly sampling across the curve, and estimating the polynomial based on these points, but we will show that this is not accurate in most cases. For example, the function
&lt;span class=&#34;math display&#34;&gt;\[
f(x) = \frac{1}{50+25\sin{[(5x)^3]}}, \qquad x \in [-1,1]
\]&lt;/span&gt;
has a very complex integral to solve analytically. Wolfram Alpha gives this solution as
&lt;span class=&#34;math display&#34;&gt;\[
\int f(x) dx =  -\frac{2}{375}i \sum_{\omega:\: \omega^6 - 3\omega^4- 16i\omega^3 + 3\omega^2 - 1=0}\frac{2\omega \tan^{-1}\left( \frac{\sin 5x}{\cos 5x - \omega}\right) - i \omega \log(-2\omega \cos 5x + \omega^2 + 1)}{\omega^4 - 2\omega^2- 8 i \omega +1} ,
\]&lt;/span&gt;
which would be an extreme effort to solve without a computer. With quadrature methods, the integral in this one-dimensional case can be approximated with small error due to the Weierstrass Approximation theorem.&lt;/p&gt;
&lt;p&gt;We first start by approximating the polynomial &lt;span class=&#34;math inline&#34;&gt;\(p_n\)&lt;/span&gt; with a basic method of uniformly sampling across the range of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We can use Lagrange polynomials to approximate the polynomial across these uniform points. A Lagrange polynomial takes the form
&lt;span class=&#34;math display&#34;&gt;\[
p_{k-1}(x) := \sum^k_{i=1} \ell_i (x) f_i(x_i), \: \: \: \: \text{ where } \:\:\:\: \ell_i(x) = \prod^k_{j=1, j \neq i} \frac{x-x_j}{x_i-x_j},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\ell_i\)&lt;/span&gt; are the Lagrange basis polynomials. We start by setting up the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = seq(-1, 1, len=100)
f = function(x) 1/(50+25*sin(5*x)^3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A Lagrange polynomial function can be set up in R. This function will return an approximating function, of which values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; can be supplied, just like the original function &lt;code&gt;f&lt;/code&gt; is set up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_lagrange_polynomial = function(f, x_points){
  function(x){
    basis_polynomials = array(1, c(length(x), length(x_points)))
    for(j in 1:length(x_points)){
      for(m in 1:length(x_points)){
        if(m==j) next
        basis_polynomials[,j] = basis_polynomials[,j] * ((x-x_points[m])/(x_points[j]-x_points[m]))
      }
    }
    p = 0
    for(i in 1:length(x_points)){
      p = p + basis_polynomials[,i]*f(x_points[i])
    }
    return(p)
  }
}

x_points = seq(range(x)[1], range(x)[2], length=30)
lagrange_polynomial = get_lagrange_polynomial(f, x_points)
plot(x, f(x), type=&amp;quot;l&amp;quot;)
points(x_points, f(x_points), col=&amp;quot;red&amp;quot;, pch=20)
lines(x, lagrange_polynomial(x), col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/integration_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the Lagrange polynomial approximation approximates the function reasonably well in the middle areas, but the approximation is completely off at the ends. This large deviation is known as &lt;em&gt;Runge’s phenomenon&lt;/em&gt;, and this occurs when using polynomial interpolation and equally spaced interpolation points. This can be fixed by using &lt;strong&gt;Chebyshev points&lt;/strong&gt;, which take the form
&lt;span class=&#34;math display&#34;&gt;\[
\cos \left(\frac{2i-1}{2k}\pi\right),
\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots,k\)&lt;/span&gt;. This can be simply implented in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chebyshev_points = function(k) cos(((2*seq(1,k,by=1)-1)*pi)/(2*k))
c_points = sort(chebyshev_points(30))
lagrange_polynomial = get_lagrange_polynomial(f, c_points)
plot(x, f(x), type=&amp;quot;l&amp;quot;)
points(c_points, f(c_points), col=&amp;quot;red&amp;quot;, pch=20)
lines(x, lagrange_polynomial(x), col=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/integration_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
The function is approximated a lot better without any significant deviations from the function. Now that we have a more accurate approximation, we can estimate the area underneath the curve by using a ‘histogram’ approximation to the area. A basic method will simply sum over all small areas of the function evaluation at each point, and the distance between midpoints, i.e.
&lt;span class=&#34;math display&#34;&gt;\[
\sum^k_{i=1}w_k f(x_k),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th point (calculated with Chebyshev or uniform approximations), and &lt;span class=&#34;math inline&#34;&gt;\(w_k\)&lt;/span&gt; is the distance between the midpoints above and below point &lt;span class=&#34;math inline&#34;&gt;\(x_k\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist_approximation = function(f_points, x_points, xrange=c(-1,1)){
  midpoints = (x_points[2:(length(x_points))]+x_points[1:(length(x_points)-1)])/2
  midpoints = c(xrange[1] ,midpoints, xrange[2])
  diffs = diff(midpoints)
  
  sum(diffs*f_points)
}
x_points = seq(range(x)[1], range(x)[2], length=30)
lagrange_polynomial = get_lagrange_polynomial(f, x_points)
hist_approximation(lagrange_polynomial(x_points), x_points)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04426415&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which we can compare to the analytical solution to the integral.
&lt;span class=&#34;math display&#34;&gt;\[
\int^{1}_{-1} \frac{1}{50+25\sin{x}}dx \approx 0.0443078
\]&lt;/span&gt;
So this isn’t too far off, but can be more approximately calculated with &lt;strong&gt;Simpson’s rule&lt;/strong&gt;, given by
&lt;span class=&#34;math display&#34;&gt;\[
\int^b_a f(x) dx \approx \frac{b-a}{6} \left( f(a) + 4f\left(\frac{a+b}{2}\right) + f(b) \right).
\]&lt;/span&gt;
This can be coded into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpsons_approximation = function(f, x_points, xrange=c(-1,1)){
  midpoints = c(xrange[1], x_points, xrange[2])
  diffs = (midpoints[2:(length(midpoints))] - midpoints[1:(length(midpoints)-1)])/6 * (
    f(midpoints[1:(length(midpoints)-1)]) + 4*f((midpoints[2:(length(midpoints))] +
    midpoints[1:(length(midpoints)-1)])/2) + f(midpoints[2:(length(midpoints))])
  )
  
  sum(diffs)
}

c_points = sort(chebyshev_points(30))
simpsons_approximation(get_lagrange_polynomial(f, c_points), c_points)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.044304&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a closer estimate to the true value. Increasing the number of points increases the accuracy of the integral approximation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n=1000
ints = rep(NA, (n-10))
for(b in 10:n) {
  c_points = sort(chebyshev_points(b))
  ints[b-10] = simpsons_approximation(f(c_points), c_points)
}
plot(11:n, ints, xlab=&amp;quot;No. of Points&amp;quot;, log=&amp;quot;x&amp;quot;, type=&amp;quot;l&amp;quot;, ylab=&amp;quot;Approximated Area&amp;quot;)
abline(h=0.0443078, col=&amp;quot;red&amp;quot;)
legend(&amp;quot;topright&amp;quot;, col=&amp;quot;red&amp;quot;, legend=&amp;quot;True Integral&amp;quot;, lwd=2, lty=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/portfolios/sc1/integration_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
This plot shows that at around 35 points, the accuracy of the integral doesn’t increase significantly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-dimensional-case&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multi-dimensional Case&lt;/h2&gt;
&lt;p&gt;Quadrature methods don’t work as well with more than one dimension. Since quadrature is based around using polynomial approximation to the real curve and calculating the area under there, this is less simple in higher dimensions, and comes with an extremely large computational complexity. &lt;strong&gt;Monte-Carlo&lt;/strong&gt; algorithms provide more efficient convergence to the integral area in this case. This will not be covered in this portfolio.x&lt;/p&gt;
&lt;!-- ### Monte-Carlo Methods --&gt;
&lt;!-- Monte-Carlo methods are *non-deterministic*, as opposed to the deterministic approach of quadrature methods. This approach is based around computing areas on a non-regular grid, of which the sampling method is random. Due to the law of large numbers, enough random sampling will converge to the true value. This will be briefly explained in this portfolio. --&gt;
&lt;!-- The multi-dimensional integral --&gt;
&lt;!-- \[ --&gt;
&lt;!-- I = \int_{{\Omega}}f(\overline{\mathbf{x}}) \; d \overline{\mathbf{x}}, --&gt;
&lt;!-- \] --&gt;
&lt;!-- where $\Omega \subseteq \mathbb{R}^n$, has volume  --&gt;
&lt;!-- \[ --&gt;
&lt;!-- V = \int_{\Omega}d\overline{\boldsymbol{x}}. --&gt;
&lt;!-- \] --&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ParallelRcpp</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/parallelrcpp/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/parallelrcpp/</guid>
      <description>


&lt;p&gt;There are two primary methods for parallelisation in &lt;code&gt;Rcpp&lt;/code&gt;, the first being &lt;code&gt;OpenMP&lt;/code&gt; and the second being the &lt;code&gt;RcppParallel&lt;/code&gt; package for &lt;code&gt;Rcpp&lt;/code&gt;. The &lt;code&gt;RcppParallel&lt;/code&gt; package builds on existing methods and uses &lt;code&gt;OpenMP&lt;/code&gt;, but provides neat functionality and simple usage, and so will be focused on in this portfolio.&lt;/p&gt;
&lt;p&gt;The reader must have a working knowledge of C++ and &lt;code&gt;Rcpp&lt;/code&gt;. To begin, each &lt;code&gt;Rcpp&lt;/code&gt; file must include&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::depends(RcppParallel)]]
#include &amp;lt;RcppParallel.h&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in the header. This ensures that the settings for &lt;code&gt;RcppParallel&lt;/code&gt; are automatically included in the compilation of the C++ code.&lt;/p&gt;
&lt;p&gt;Regular parallel approaches to Cpp can cause crashes due to the single-threaded nature of R. This is due to multiple threads attempting to access and interact with the same data structure. &lt;code&gt;RcppParallel&lt;/code&gt; provides a straightforward way to account for this.&lt;/p&gt;
&lt;div id=&#34;basic-parallel-operations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basic Parallel Operations&lt;/h3&gt;
&lt;p&gt;There are two functions inbuilt which can provide the bulk of the parallel operations; &lt;code&gt;parallelFor&lt;/code&gt; and &lt;code&gt;parallelReduce&lt;/code&gt;. These are interfaces to a parallel for loop and reduce function (see &lt;code&gt;?Reduce&lt;/code&gt; in R).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RcppParallel&lt;/code&gt; also provides two accessor classes, &lt;code&gt;RVector&lt;/code&gt; and &lt;code&gt;RMatrix&lt;/code&gt;, which are thread-safe accessors for an Rcpp vector and matrix, helping to deal with the problem of accessing the same data structure across multiple threads.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-matrix-transformations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Matrix Transformations&lt;/h2&gt;
&lt;p&gt;Consider taking the log of every element in a large matrix. In R, this process is simple, since &lt;code&gt;log&lt;/code&gt; is a vectorised function, we can just run&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(A)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;A&lt;/code&gt; is a matrix. This would also be easy to implement in &lt;code&gt;Rcpp&lt;/code&gt; using the &lt;code&gt;std::transform&lt;/code&gt; operator:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp:export]]
NumericMatrix MatrixLog(NumericMatrix A)
{
  int n = A.nrow();
  int d = A.ncol();
  NumericMatrix out(n, d);
  std::transform(A.begin(), A.end(), out.begin(), ::log);
  return(out);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;std::transform&lt;/code&gt; function applies a given function across a range of values, here these are specified as all the elements in the matrix &lt;code&gt;A&lt;/code&gt;, where the starting and ending point are supplied by &lt;code&gt;A.begin()&lt;/code&gt; and &lt;code&gt;A.end()&lt;/code&gt;. These transformed values are saved in &lt;code&gt;out&lt;/code&gt;, starting at &lt;code&gt;out.begin()&lt;/code&gt;. We can compare the speed of this function to an R implementation by applying both the base R &lt;code&gt;log&lt;/code&gt; function and &lt;code&gt;MatrixLog&lt;/code&gt; to a large matrix (and check that they give the same result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
sourceCpp(&amp;quot;MatrixLog.cpp&amp;quot;)
d = 200
A = matrix(1:(d^2), d, d)
all.equal(log(A), MatrixLog(A))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, they are equal, this is a good first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
microbenchmark(log(A), MatrixLog(A), times = 1000, unit=&amp;quot;relative&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: relative
##          expr      min       lq     mean   median       uq      max neval
##        log(A) 1.008818 1.042968 0.980134 1.057816 0.979937 0.398702  1000
##  MatrixLog(A) 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000  1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So on average, the C++ implementation is actually around the same speed as the base R implementation. We can speed up the &lt;code&gt;Rcpp&lt;/code&gt; code through the use of parallel computing, using &lt;code&gt;parallelFor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Firstly, &lt;code&gt;parallelFor&lt;/code&gt; has four arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;begin&lt;/code&gt;: the beginning of the for loop&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end&lt;/code&gt;: the end of the for loop&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worker&lt;/code&gt;: an object of type &lt;code&gt;Worker&lt;/code&gt;, where the operations are specified&lt;/li&gt;
&lt;li&gt;&lt;code&gt;grainSize&lt;/code&gt;: minimal chunk size for parallelisation, minimum number of operations for each thread&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before defining the parallel code, &lt;code&gt;parallelFor&lt;/code&gt; needs a &lt;code&gt;Worker&lt;/code&gt; object to specify what processes to perform within the for loop. For this case, we need to create a worker that takes the log of each set of elements that are passed to each thread.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;struct Log : public RcppParallel::Worker
{
   const RcppParallel::RMatrix&amp;lt;double&amp;gt; input;
   RcppParallel::RMatrix&amp;lt;double&amp;gt; output;
  
   Log(const NumericMatrix input, NumericMatrix output) 
      : input(input), output(output) {}
   
   void operator()(std::size_t begin, std::size_t end) {
      std::transform(input.begin() + begin, 
                     input.begin() + end, 
                     output.begin() + begin, 
                     ::log);
   }
};&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s break down this structure. Firstly, two objects of type &lt;code&gt;RMatrix&lt;/code&gt; are specified, for the input and output (recall that an &lt;code&gt;RMatrix&lt;/code&gt; is a thread-safe object given by &lt;code&gt;RcppParallel&lt;/code&gt;). Since different chunks of the matrix will be passed between threads, they need to be converted to this safe &lt;code&gt;RMatrix&lt;/code&gt; object before they are interacted with. Secondly, the &lt;code&gt;Log&lt;/code&gt; function is defined, so that these inputs and outputs are passed through.&lt;/p&gt;
&lt;p&gt;Finally, the &lt;code&gt;operator()&lt;/code&gt; is the main part, which is what will natively be called by &lt;code&gt;parallelFor&lt;/code&gt;. This performs the same operation as what we saw before in &lt;code&gt;MatrixLog&lt;/code&gt;, with a few key differences. Namely the &lt;code&gt;begin&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; function inputs, which change the range that &lt;code&gt;std::transform&lt;/code&gt; is applied to based on the chunk of the matrix that &lt;code&gt;parallelFor&lt;/code&gt; will be giving this worker.&lt;/p&gt;
&lt;p&gt;Now that this is set up, we can rewrite &lt;code&gt;MatrixLog&lt;/code&gt; in parallel:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;RcppParallel.h&amp;gt;
using namespace Rcpp;
// [[Rcpp::depends(RcppParallel)]]

// [[Rcpp::export]]
NumericMatrix MatrixLogPar(NumericMatrix A) {
  
  int n = A.nrow();
  int d = A.ncol();
  NumericMatrix output(n, d);
  
  Log log_(A, output);
  parallelFor(0, A.length(), log_);
  
  return output;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is similar to the original &lt;code&gt;MatrixLog&lt;/code&gt;, however the &lt;code&gt;std::transform&lt;/code&gt; section has been replaced by the definition of the worker &lt;code&gt;log_&lt;/code&gt; (with class &lt;code&gt;Log&lt;/code&gt;), and then the call to &lt;code&gt;parallelFor&lt;/code&gt;. To reiterate, a worker is defined which has the pre-built operator that it will take the log of each element within the chunk that is specified to it. This worker is then spread out across multiple threads by the call to &lt;code&gt;parallelFor&lt;/code&gt;, and the output is saved in &lt;code&gt;output&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we can compare the speed!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;quot;MatrixLogPar.cpp&amp;quot;)
all.equal(log(A), MatrixLog(A), MatrixLogPar(A))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(log(A), MatrixLog(A), MatrixLogPar(A), unit=&amp;quot;relative&amp;quot;, times=1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: relative
##             expr      min       lq     mean   median       uq       max neval
##           log(A) 2.339494 2.318958 1.749330 2.048938 1.820713 0.7575495  1000
##     MatrixLog(A) 2.232776 2.197518 1.731507 1.977766 1.801228 0.8874397  1000
##  MatrixLogPar(A) 1.000000 1.000000 1.000000 1.000000 1.000000 1.0000000  1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So originally we had roughly the same processing time as the base R implementation. Now this is roughly twice as fast as base R!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://dannyjameswilliams.co.uk/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Analysis of the Lyrics of Kanye West</title>
      <link>https://dannyjameswilliams.co.uk/post/kanye/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/kanye/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/forceNetwork-binding/forceNetwork.js&#34;&gt;&lt;/script&gt;


&lt;em&gt;“2020 I’mma run the whole election”&lt;/em&gt; may not sound like the words of a lyrical genius, but I ask you to &lt;em&gt;“name one genius that ain’t crazy”&lt;/em&gt;. For the duration of your read of this post, take a minute to separate an artist from their art. It is undeniable that Kanye West has had serious influence over the musical industry during his career. He has single-handedly influenced hip hop since his first album, &lt;em&gt;The College Dropout&lt;/em&gt;, and that’s not to mention &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_awards_and_nominations_received_by_Kanye_West&#34;&gt;all of his awards&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;
&lt;center style=&#34;color:#717d7e;&#34;&gt;
&lt;em&gt;&amp;quot;I woke up early this mornin’ with a new state of mind&lt;/em&gt; &lt;br&gt;
&lt;em&gt;A creative way to rhyme without usin’ nines and guns&amp;quot;&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;
Controversy aside, what makes Kanye West so influential to the hip-hop industry? His lyrics must play a part to his success. The &lt;a href=&#34;https://cloud.google.com/natural-language/&#34;&gt;Google Natural Language API&lt;/a&gt; can perform a full language analysis of Kanye West’s lyrics for free. This uses a pre-trained natural language model by Google, and the API is available for use in Python.&lt;/p&gt;
&lt;p&gt;The data were mostly obtained by using the &lt;a href=&#34;https://cran.r-project.org/web/packages/genius/index.html&#34;&gt;genius&lt;/a&gt; package in R, but missing entries were copied and pasted manually from the Genius website. The analysis is restricted to his studio albums, but not solo albums, so that his collaborations with Kid Cudi and Jay Z are included.
All albums considered are &lt;em&gt;The College Dropout&lt;/em&gt;, &lt;em&gt;Late Registration&lt;/em&gt;, &lt;em&gt;Graduation&lt;/em&gt;, &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt;, &lt;em&gt;My Beautiful Dark Twisted Fantasy&lt;/em&gt;, &lt;em&gt;Watch the Throne&lt;/em&gt;, &lt;em&gt;Yeezus&lt;/em&gt;, &lt;em&gt;The Life of Pablo&lt;/em&gt;, &lt;em&gt;ye&lt;/em&gt;, &lt;em&gt;Kids See Ghosts&lt;/em&gt; and &lt;em&gt;Jesus is King&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This article is split into four sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entity Analysis&lt;/li&gt;
&lt;li&gt;Sentiment Analysis&lt;/li&gt;
&lt;li&gt;Sentiment and Magnitude against Album Reception&lt;/li&gt;
&lt;li&gt;Generating a New Kanye Song&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All code used for the analysis and graphics in this post can be found in the github repository &lt;a href=&#34;https://github.com/dannyjameswilliams/kanyenet&#34;&gt;kanyenet&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;entity-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Entity Analysis&lt;/h2&gt;
&lt;p&gt;The natural language API provided by Google includes entity analysis - the identification of &lt;em&gt;entities&lt;/em&gt;, which can be interpreted as the ‘important parts’ of the text. Each song is passed individually to the API, collating the number of entities for each song. Below is a network of word connections, where the links between nodes represent words that appear in the same song. The graph is interactive, so you can scroll around and zoom in and out to see all the connections. You can click on a node to view the total number of times it appears across all songs.&lt;/p&gt;
It might take a while to load, and is best viewed on desktop. &lt;strong&gt;Content warning:&lt;/strong&gt; An effort has been made to censor offensive words, but some may have slipped through the cracks.
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;forceNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;links&#34;:{&#34;source&#34;:[0,0,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,4,5,5,5,5,5,5,5,5,5,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,9,9,9,10,10,10,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,14,14,14,14,14,14,15,15,15,15,15,15,16,16,16,16,16,16,17,17,17,18,18,18,19,19,19,19,19,19,19,19,19,19,20,20,21,21,22,22,23,23,23,23,23,23,23,24,24,25,25,25,25,26,26,27,27,27,27,27,28,28,28,28,28,29,29,29,29,29,29,29,29,30,30,30,31,31,32,32,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,35,35,36,36,36,36,36,36,36,36,37,37,37,37,37,37,38,39,39,39,40,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,43,44,45,46,46,47,48,48,48,49,49,49,49,49,50,50,50,51,51,51,51,52,52,52,53,53,53,53,53,54,54,54,54,55,55,55,56,56,56,56,57,57,57,57,58,58,58,58,59,59,60,60,60,61,61,61,61,61,61,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,64,64,65,65,65,65,65,65,65,66,66,67,67,67,67,67,68,68,68,68,68,68,69,69,69,69,70,71,71,71,71,71,71,71,71,71,71,72,72,72,72,72,73,73,74,74,74,75,75,75,75,76,77,77,78,79,80,80,80,80,80,80,81,81,82,83,83,83,83,83,83,83,83,83,83,83,84,84,85,85,86,86,86,86,87,87,87,87,88,89,89,89,89,89,89,89,89,89,89,90,90,90,90,91,91,92,92,92,92,92,93,93,94,95,95,95,96,96,97,98,98,98,98,98,98,98,98,98,98,98,98,98,98,99,100,100,100,100,100,101,102,103,103,104,104,105,106,107,108,108,108,109,109,109,109,109,109,109,110,110,110,111,111,111,111,112,112,113,113,113,113,113,114,114,115,116,116,116,116,116,116,116,116,116,116,117,117,117,117,117,117,118,119,119,119,120,120,121,121,122,123,124,125,125,125,125,125,125,125,125,125,125,126,127,127,128,128,129,129,129,129,129,130,130,130,131,132,133,134,135,135,136,137,138,139,139,139,139,139,139,140,140,140,141,141,141,142,143,144,145,146,147,148,148,149,149,150,150,151,152,153,154,155,156,157,157,158,159,160,161,162,163,164,164,165,166,167,168],&#34;target&#34;:[0,124,1,95,121,167,2,15,71,116,125,139,155,3,7,90,116,161,4,5,36,61,83,98,117,140,164,165,6,17,59,80,7,77,79,86,90,109,125,127,132,154,156,161,8,30,96,101,9,112,168,10,73,89,11,43,48,66,90,12,33,34,62,63,83,98,116,125,129,139,144,13,20,76,14,53,75,89,90,97,15,71,116,125,139,155,16,113,116,117,139,154,17,59,80,18,26,142,19,58,89,92,98,125,130,131,149,153,20,76,21,61,22,126,23,37,49,57,108,150,151,24,45,25,50,98,162,26,142,27,65,119,120,125,28,54,89,104,166,29,42,68,71,80,110,116,134,30,96,101,31,115,32,89,33,34,62,63,83,98,116,125,129,139,144,34,62,63,83,98,116,125,129,139,144,35,70,36,61,83,98,117,140,164,165,37,49,57,108,150,151,38,39,42,125,40,41,61,67,72,109,116,125,42,68,71,80,110,116,125,134,157,163,43,48,66,90,98,125,129,133,139,44,45,46,82,47,48,66,90,49,57,108,150,151,50,98,162,51,52,141,147,52,141,147,53,75,89,90,97,54,89,104,166,55,99,102,56,74,103,139,57,108,150,151,58,130,149,153,59,80,60,84,146,61,67,72,83,98,106,109,116,117,125,137,140,164,165,62,63,83,98,116,125,129,139,144,63,83,98,116,125,129,139,144,64,159,65,81,85,107,119,120,125,66,90,67,72,109,116,125,68,71,80,110,116,134,69,139,141,145,70,71,80,89,110,116,125,134,136,139,155,72,109,116,125,158,73,89,74,103,139,75,89,90,97,76,77,79,78,79,80,89,105,110,116,134,81,160,82,83,98,116,117,125,129,139,140,144,164,165,84,146,85,107,86,109,127,132,87,98,109,123,88,89,90,92,97,98,104,105,125,131,166,90,91,97,161,91,152,92,98,122,125,131,93,94,94,95,121,167,96,101,97,98,109,116,117,123,125,129,131,139,140,144,162,164,165,99,100,111,129,148,154,101,102,103,139,104,166,105,106,107,108,150,151,109,116,123,125,127,129,132,110,116,134,111,129,148,154,112,168,113,116,117,139,154,114,126,115,116,117,125,129,134,139,143,144,154,155,117,139,140,154,164,165,118,119,120,125,120,125,121,167,122,123,124,125,129,131,136,137,139,144,154,155,156,126,127,132,128,166,129,139,144,148,154,130,149,153,131,132,133,134,135,154,136,137,138,139,141,144,145,154,155,140,164,165,141,145,147,142,143,144,145,146,147,148,154,149,153,150,151,151,152,153,154,155,156,157,163,158,159,160,161,162,163,164,165,165,166,167,168],&#34;value&#34;:[1,2,1,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,1,2,2,2,6,2,2,2,2,2,2,2,2,2,2,2,1,2,2,2,1,2,2,1,2,2,1,2,2,2,2,4,4,4,4,4,4,4,8,4,4,4,4,1,2,2,2,2,2,2,2,2,1,2,2,2,2,2,1,2,2,2,2,2,1,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,1,2,1,4,1,2,1,2,2,2,2,2,2,1,2,1,2,2,2,1,2,1,2,2,2,2,1,2,2,2,2,1,2,2,2,2,2,2,2,1,2,2,1,2,1,2,1,2,2,2,2,2,4,2,2,2,2,1,2,2,2,2,4,2,2,2,2,1,2,1,2,2,2,2,2,2,2,1,2,2,2,2,2,1,1,2,2,1,1,2,2,2,2,2,2,3,2,2,2,2,2,2,2,2,2,5,2,2,2,2,2,2,2,2,1,1,1,2,1,2,2,2,1,2,2,2,2,1,2,2,1,2,2,2,1,2,2,1,2,2,2,2,1,2,2,2,2,2,2,1,2,2,2,1,2,2,2,1,2,2,2,1,2,1,2,2,8,2,2,2,2,2,2,2,2,4,2,2,2,2,1,2,2,2,4,2,2,2,2,1,2,2,4,2,2,2,2,1,2,5,2,2,2,2,2,2,1,2,1,2,2,2,2,1,2,2,2,2,2,1,2,2,2,1,5,2,2,2,4,4,2,2,2,2,2,2,2,2,2,1,2,1,2,2,1,2,2,2,1,1,2,2,1,3,2,2,2,2,2,3,2,1,3,4,4,2,2,2,2,2,2,2,2,1,2,1,2,1,2,2,2,1,2,2,2,1,8,2,2,2,2,2,2,4,2,2,4,2,2,2,2,2,3,2,2,2,2,1,2,1,1,2,2,1,2,1,8,4,4,2,2,6,4,2,2,2,2,2,2,2,1,1,2,2,2,2,1,1,1,2,1,2,1,1,1,1,2,2,4,2,2,2,2,2,2,1,2,2,1,2,2,2,1,2,1,2,2,2,2,1,2,1,17,2,8,4,2,8,2,4,2,2,2,2,2,2,2,2,1,1,2,2,1,2,1,2,1,3,1,14,2,2,2,2,4,2,2,2,2,2,1,2,1,2,4,4,2,2,2,1,2,2,1,1,2,1,1,4,1,1,1,6,2,2,2,2,2,1,2,2,2,2,2,1,1,1,1,1,1,1,2,1,2,1,2,1,1,1,8,1,1,1,2,1,1,1,3,1,1,1,2,1,2,1,1],&#34;colour&#34;:[&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;]},&#34;nodes&#34;:{&#34;name&#34;:[&#34;300&#34;,&#34;a million&#34;,&#34;air&#34;,&#34;all&#34;,&#34;america&#34;,&#34;ass&#34;,&#34;ayy&#34;,&#34;baby&#34;,&#34;baby jesus&#34;,&#34;ball&#34;,&#34;bang&#34;,&#34;beam&#34;,&#34;beat&#34;,&#34;big brother&#34;,&#34;bitch&#34;,&#34;bitches&#34;,&#34;blame game&#34;,&#34;bottle&#34;,&#34;bout&#34;,&#34;boy&#34;,&#34;brother&#34;,&#34;bulls***&#34;,&#34;c&#39;mon homie&#34;,&#34;cab&#34;,&#34;champion&#34;,&#34;chance&#34;,&#34;chill&#34;,&#34;church&#34;,&#34;city&#34;,&#34;concert&#34;,&#34;coretta&#34;,&#34;crack music n****&#34;,&#34;dad&#34;,&#34;dame&#34;,&#34;deal&#34;,&#34;dem&#34;,&#34;dessert&#34;,&#34;destination&#34;,&#34;diamond&#34;,&#34;door&#34;,&#34;dream&#34;,&#34;eighteen&#34;,&#34;everybody&#34;,&#34;everything&#34;,&#34;ey &#39;ey &#39;ey&#34;,&#34;eye&#34;,&#34;f***&#34;,&#34;fadin&#34;,&#34;faith&#34;,&#34;fare&#34;,&#34;fire&#34;,&#34;fly&#34;,&#34;fore&#34;,&#34;freak&#34;,&#34;freedom&#34;,&#34;friend&#34;,&#34;frightenin&#34;,&#34;front&#34;,&#34;gangsta&#34;,&#34;genie&#34;,&#34;ghost&#34;,&#34;girl&#34;,&#34;glass&#34;,&#34;glasses&#34;,&#34;glory&#34;,&#34;god&#34;,&#34;god dream&#34;,&#34;gold digger&#34;,&#34;gossip&#34;,&#34;graveshift&#34;,&#34;gwaan&#34;,&#34;hand&#34;,&#34;head&#34;,&#34;hell&#34;,&#34;help&#34;,&#34;highlight&#34;,&#34;hip hop brother&#34;,&#34;hoe&#34;,&#34;home&#34;,&#34;homie&#34;,&#34;i&#39;ma&#34;,&#34;jesus&#34;,&#34;jungle&#34;,&#34;kanye&#34;,&#34;kid&#34;,&#34;king&#34;,&#34;l.a.&#34;,&#34;la la&#34;,&#34;lie&#34;,&#34;life&#34;,&#34;light&#34;,&#34;lord&#34;,&#34;love&#34;,&#34;love lock-down&#34;,&#34;love lockdown&#34;,&#34;luxury&#34;,&#34;malcolm&#34;,&#34;mama&#34;,&#34;man&#34;,&#34;many&#34;,&#34;mars&#34;,&#34;martin&#34;,&#34;memorie&#34;,&#34;menacin&#34;,&#34;mind&#34;,&#34;mine&#34;,&#34;mistake&#34;,&#34;mob&#34;,&#34;moment&#34;,&#34;money&#34;,&#34;monster&#34;,&#34;moon&#34;,&#34;mothaf***a&#34;,&#34;motherf***er&#34;,&#34;murder&#34;,&#34;music&#34;,&#34;n****&#34;,&#34;name&#34;,&#34;new&#34;,&#34;night sky&#34;,&#34;nightlife&#34;,&#34;nike&#34;,&#34;nobody&#34;,&#34;nothing&#34;,&#34;omen&#34;,&#34;one&#34;,&#34;paper&#34;,&#34;parties&#34;,&#34;party&#34;,&#34;people&#34;,&#34;pimp&#34;,&#34;pinocchio&#34;,&#34;place&#34;,&#34;power&#34;,&#34;profit&#34;,&#34;rain&#34;,&#34;reason&#34;,&#34;robocop&#34;,&#34;rosie&#34;,&#34;s***&#34;,&#34;salad&#34;,&#34;sky&#34;,&#34;slave&#34;,&#34;somebody&#34;,&#34;song&#34;,&#34;spaceship&#34;,&#34;spirit&#34;,&#34;spot&#34;,&#34;star&#34;,&#34;step&#34;,&#34;street&#34;,&#34;street light&#34;,&#34;stress&#34;,&#34;talk&#34;,&#34;thing&#34;,&#34;thirty&#34;,&#34;toast&#34;,&#34;two&#34;,&#34;vision&#34;,&#34;war&#34;,&#34;water&#34;,&#34;way&#34;,&#34;wire&#34;,&#34;word&#34;,&#34;work&#34;,&#34;workout plan&#34;,&#34;world&#34;,&#34;yeezy&#34;,&#34;zone&#34;],&#34;group&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169],&#34;nodesize&#34;:[6,5,12,14,7,7,8,49,6,6,5,6,14,14,20,5,11,6,5,15,6,9,11,5,9,6,8,11,6,9,5,19,5,7,7,12,5,5,8,5,5,5,20,42,12,7,5,8,10,5,9,8,8,6,8,16,6,5,6,6,22,64,8,9,8,52,9,8,6,8,6,45,21,5,6,6,6,8,19,22,23,29,7,41,21,6,13,9,8,50,42,27,25,12,9,11,6,6,62,6,5,6,5,6,5,6,10,6,5,27,12,9,7,6,17,12,84,21,9,13,5,5,15,27,5,97,10,12,10,29,6,5,5,16,7,6,6,8,7,54,5,19,7,5,5,10,6,8,9,24,6,5,9,6,72,5,15,15,7,8,12,18,5,12,6,8,11,16,9],&#34;size&#34;:[6,5,12,14,7,7,8,49,6,6,5,6,14,14,20,5,11,6,5,15,6,9,11,5,9,6,8,11,6,9,5,19,5,7,7,12,5,5,8,5,5,5,20,42,12,7,5,8,10,5,9,8,8,6,8,16,6,5,6,6,22,64,8,9,8,52,9,8,6,8,6,45,21,5,6,6,6,8,19,22,23,29,7,41,21,6,13,9,8,50,42,27,25,12,9,11,6,6,62,6,5,6,5,6,5,6,10,6,5,27,12,9,7,6,17,12,84,21,9,13,5,5,15,27,5,97,10,12,10,29,6,5,5,16,7,6,6,8,7,54,5,19,7,5,5,10,6,8,9,24,6,5,9,6,72,5,15,15,7,8,12,18,5,12,6,8,11,16,9]},&#34;options&#34;:{&#34;NodeID&#34;:&#34;name&#34;,&#34;Group&#34;:1,&#34;colourScale&#34;:&#34;d3.scaleOrdinal(d3.schemeCategory20);&#34;,&#34;fontSize&#34;:18,&#34;fontFamily&#34;:&#34;Calibri&#34;,&#34;clickTextSize&#34;:45,&#34;linkDistance&#34;:&#34;function(d){return d.value * 10}&#34;,&#34;linkWidth&#34;:&#34;function(d) { return Math.sqrt(d.value); }&#34;,&#34;charge&#34;:-150,&#34;opacity&#34;:0.9,&#34;zoom&#34;:true,&#34;legend&#34;:false,&#34;arrows&#34;:false,&#34;nodesize&#34;:true,&#34;radiusCalculation&#34;:&#34; Math.sqrt(d.nodesize)+6&#34;,&#34;bounded&#34;:false,&#34;opacityNoHover&#34;:0.15,&#34;clickAction&#34;:&#34;alert(\&#34;Total count of &#39;\&#34; + (d.name) + \&#34;&#39;: \&#34; + (d.size) + \&#34;\n\&#34;)&#34;}},&#34;evals&#34;:[&#34;options.linkDistance&#34;],&#34;jsHooks&#34;:{&#34;render&#34;:[{&#34;code&#34;:&#34;function(el, x) { \n    d3.selectAll(\&#34;.node text\&#34;).style(\&#34;fill\&#34;, \&#34;black\&#34;);\n  }&#34;,&#34;data&#34;:null}]}}&lt;/script&gt;
&lt;figcaption&gt;
Network of entities. Connections between nodes represent entities being in the same song.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br&gt;
To see a larger version of this network, with the ability to filter by album and minimum number of occurences of a word, &lt;a href=&#34;https://dannyjameswilliams.shinyapps.io/kanyenetwork/&#34;&gt;click here to view it as an R shiny app.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This graph only contains entities that appear more than 4 times across all songs, as the full network would contain far too much information, and nothing would be visible. The larger size of the node indicates a word being more frequent across all songs.&lt;/p&gt;
&lt;p&gt;Since this undirected graph is &lt;em&gt;not fully connected&lt;/em&gt;, then it is impossible to connect all entities to each other via other entities that appear in the same song. Here we can also see the most common words at the center of the graph, and as expected, the most frequent words also seem to have the most connections.&lt;/p&gt;
&lt;p&gt;We can also note a few interesting qualities; firstly that Kanye never talks about Jesus in a song without also talking about God. Secondly, Kanye only talks about mistakes when he also talks about girls, I wonder what could that mean?&lt;/p&gt;
&lt;p&gt;Themes are also apparent in different clusters of the graph, for example, a small section containing &lt;em&gt;star&lt;/em&gt;, &lt;em&gt;moon&lt;/em&gt; and &lt;em&gt;mars&lt;/em&gt;, or entities such as &lt;em&gt;workout plan&lt;/em&gt;, &lt;em&gt;dessert&lt;/em&gt; and &lt;em&gt;salad&lt;/em&gt; in another section, seemingly referring to exercise and health. Particular songs can also be isolated out at the outer sections of the graph, which explains some of the more uncommon words that appear with few connections.&lt;/p&gt;
&lt;p&gt;But how do the frequency of these entities change over time? We can inspect the top 10 entities separately for each album, which gives a good indication of their individual themes, as well as Kanye’s use of different lyricism throughout his career.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/kanye/top10words_notitles.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Top 10 entities by album in order of their release.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Kanye’s albums have a broad range of different themes, which correspond to different entities being more prevalent within them. Kanye’s most heartfelt album, &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt; (the fourth album) contained no curse words, but instead had more references to ‘love’ and ‘life’. It is easy to judge the theme of each album by their most common entities, such as &lt;em&gt;Jesus is King&lt;/em&gt; (the final one). Again there are no profanities, instead the album is heavily focused on religion, being a gospel album. Here, ‘Jesus’ is the most common entity, followed by god.&lt;/p&gt;
Some of the entities which might seem ‘irrelevant’ that can be found within these charts often correspond to a single song with purposeful repetition. ‘Toast’ appears in the top 10 entities for the album &lt;em&gt;My Beautiful Dark Twisted Fantasy&lt;/em&gt;, not because it is an amazing breakfast food, but because it is repeated in the chorus of &lt;em&gt;Runaway&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;
&lt;center style=&#34;color:#717d7e;&#34;&gt;
&lt;em&gt;“Let’s have a toast for the douche bags, let’s have a toast for the assholes,&lt;em&gt;&lt;br&gt;
&lt;/em&gt;Let’s have a toast for the scumbags, every one of them that I know”&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;As a side note, you may wonder why the word ‘amazing’ does not appear in the top 10 entities for &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt;, due to it appearing a whopping 55 times in the song &lt;em&gt;Amazing&lt;/em&gt;. Thankfully, Google’s API does not classify it as an entity, since it is actually an adjective. However, ‘love lockdown’ and ‘robocop’ still made it through, as both words are repeated many times in their own songs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;Natural language analysis can also classify the &lt;em&gt;sentiment&lt;/em&gt; of a piece of text, in this case, the sentiment of song lyrics, one song at a time. The sentiment ranges from -1 to 1, where a negative/positive value means the song has a lower/higher sentiment, generally referring to the mood of the song - whether it is more uplifting or sad.&lt;/p&gt;
&lt;p&gt;Firstly, the sentiment API extracts the sentiment from each sentence separately. We can take a look at the density of all sentence sentiments below.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/kanye/sentiment_density.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Density of sentence sentiment.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;So most of the sentence sentiments are negative, in general Kanye’s lyrics convey a more sombre tone than they do a positive one. Wording can play a key part in how the natural language analysis measures sentiment. The lowest sentiment sentences are at -0.9, so let’s take a look at some examples of these.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;“Oh, how could you be so heartless?”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“The devil is alive I feel him breathing.”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“And when I’m older, you ain’t gotta work no more.”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can see the first two generally are quite negative, but the last one has been misrepresented. In this line, Kanye talks about when he was a kid, he wanted to take care of his mother when she got older so that she wouldn’t have to go to work again, but the syntax of the sentence tricked the API into believing it was a generally negative sentence. This isn’t very common, but does highlight one of the limitations of this natural language API.&lt;/p&gt;
&lt;p&gt;A song’s &lt;em&gt;magnitude&lt;/em&gt; is defined as the sum of the absolute values of the sentiments for each sentence in the song. Quite a mouthful - consider it as the ‘emotionality’ of the song; the higher the sentiment is (in one way or another), the more emotional the song becomes. We can break down the sentiment and magnitude over all songs by album.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/kanye/sentiment_magnitude.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Sentiment (top) and magnitude (bottom), averaged for each song and split by album.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Generally, Kanye’s albums are all quite negative, with the exception of &lt;em&gt;Kids See Ghosts&lt;/em&gt; and &lt;em&gt;Jesus is King&lt;/em&gt;, his two latest albums. After the release of &lt;em&gt;Graduation&lt;/em&gt;, Kanye went through various personal traumas. We can see this reflected here, as the sentiment up to &lt;em&gt;Graduation&lt;/em&gt; was increasing, after which it began decreasing again. Only recently has the sentiment began to increase again.&lt;/p&gt;
&lt;p&gt;The album with the lowest sentiment is &lt;em&gt;Yeezus&lt;/em&gt;, which surprisingly does not have the largest magnitude, although the magnitude does not vary as much with album. The sentiment in &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt; has the largest variance, with certain songs reaching very low and (comparatively) high values. These two albums generally are considered quite emotional, so it is reassuring to see this reflected in the sentiment analysis.&lt;/p&gt;
&lt;p&gt;We might say that it’s nice that Kanye’s latest releases are more positive, but there is a general attitude of &lt;em&gt;“I miss the old Kanye”&lt;/em&gt;. Is that the lower mood Kanye? The always rude Kanye?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-and-magnitude-against-album-reception&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment and Magnitude against Album Reception&lt;/h2&gt;
&lt;p&gt;The next question in our heads should be, what can we infer from this? Some albums have a higher or lower sentiment than others, but does this have any relationship with the album itself?&lt;/p&gt;
&lt;p&gt;It turns out: well, maybe. Plotted below are the relationships between the mean sentiment and the mean magnitude, by album, against the aggregated critic reviews for each album collected from &lt;a href=&#34;https://www.metacritic.com/person/kanye-west&#34;&gt;Metacritic&lt;/a&gt;.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe width=&#34;700&#34; height=&#34;450&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;//plotly.com/~dannyjameswilliams/3.embed&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Linear regression for Album sentiment against metacritic score.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe width=&#34;700&#34; height=&#34;450&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;//plotly.com/~dannyjameswilliams/6.embed&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Linear regression for Album magnitude against metacritic score.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Linear regression models are chosen because of the sparsity of data. Any more complex model is likely unneeded, but would also lack sufficient degrees of freedom to perform any meaningful inference. Modelling these variables separately allowed for more succinct visualisation and interpretation of their effects in isolation. The goodness of fit for each model can be evaluated somewhat with &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; values:
&lt;span class=&#34;math display&#34;&gt;\[
R^2_{\text{sentiment}} \approx  0.270, \qquad R^2_{\text{magnitude}} \approx  0.675.
\]&lt;/span&gt;
So the model involving each album’s mean magnitude explains more of the variance than the sentiment, and there is some definite correlation there. Does this mean an album with a higher magnitude (i.e. contains more emotional lyrics) will be more critically acclaimed? I can’t say for sure, but it’s an interesting point to note.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-a-new-kanye-song&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating a New Kanye Song&lt;/h2&gt;
&lt;p&gt;Finally, I would like to end this post by creating the lyrics for a new Kanye song. &lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;OpenAI’s unsupervised GPT-2 language model&lt;/a&gt; was trained on 40GB of internet text, and tasked with predicting the next word given in the text. The model contains an amazing 1.5 billion parameters. The &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple&#34;&gt;gpt-2-simple&lt;/a&gt; package in Python provides straightforward access to finetuning the GPT-2 model to an additional dataset, so that text generations are based on the new data, but the language has already been learned from the 40GB of internet text (if you think GPT-2 is impressive, take a look at &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;GPT-3&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I input the formatted lyric text file to finetune the GPT-2 model to Kanye’s lyrics, and generated many large sized texts to be considered as songs. This was done in Google colab notebooks, and the code used to do so is &lt;a href=&#34;https://colab.research.google.com/drive/13tC0RIr_IpNNWr6zInYkyormDgKsyF18?usp=sharing&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of the songs generated, I picked a coherent and ‘song-like’ generation and changed the formatting (added spacing). I hope you enjoy the latest release!&lt;/p&gt;
&lt;center style=&#34;color:#717d7e;&#34;&gt;
My name is Bess, I’m 21 years old and I just want to be a real star&lt;br&gt;
I just want to be a superstar&lt;br&gt;
The city skyline, the planes flying overhead&lt;br&gt;
I’m groovy as f***, like Good Charlotte&lt;br&gt;
Uh, and I just want my daddy to be proud of me&lt;br&gt;
&lt;br&gt;
Cause I ain’t talkin’ about Kris when it comes to being in the club&lt;br&gt;
It’s Jay that I’m talkin’ about, man&lt;br&gt;
Even though he got the baby’s clothes on&lt;br&gt;
I done wore nothing but red until he tucked me in&lt;br&gt;
And when he woke up, I was still wearing everything buterin’&lt;br&gt;
S*** was very “The Big Lebowski”&lt;br&gt;
&lt;br&gt;
I was standin’ by myself writing this song&lt;br&gt;
And I just started to cry&lt;br&gt;
Because this s*** can’t get any worse&lt;br&gt;
This s*** can’t get any worse&lt;br&gt;
Oh, Lord, oh, Lord&lt;br&gt;
&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
&lt;br&gt;
And I came back, I came back, I came back&lt;br&gt;
And I looked in the mirror and I seen the biggest&lt;br&gt;
The guns are in the table, the weapons is in the air&lt;br&gt;
&lt;br&gt;
Yeah, make America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
&lt;br&gt;
&lt;/center&gt;
&lt;p&gt;Seems to match Kanye pretty well, we have mention of his mother-in-law, Kris Jenner, as well as his sort-of friend and collaborator Jay-Z. Then we finish with a classic controversial Kanye segment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactive-visualisations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactive Visualisations&lt;/h2&gt;
&lt;p&gt;Most of the visualisations that I have shown can be found in an R shiny app below.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dannyjameswilliams.shinyapps.io/kanyenet/&#34;&gt;The kanyenet interactive R shiny app.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also included are additional interactive plots, enabling the filtering by album for sentiment densities and viewing the sentiment and magnitude of each song. You can also view more generations from the GPT-2 model!&lt;/p&gt;
&lt;p&gt;Again, you can view all code used for this report in the github repository &lt;a href=&#34;https://github.com/dannyjameswilliams/kanyenet&#34;&gt;kanyenet&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hot Takes for R</title>
      <link>https://dannyjameswilliams.co.uk/post/hottakes/</link>
      <pubDate>Sun, 28 Jun 2020 15:39:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/hottakes/</guid>
      <description>


&lt;p&gt;The arrow assigment operator &lt;code&gt;&amp;lt;-&lt;/code&gt; is useless. Before I’m crucified by the R community, hear me out and read this post.&lt;/p&gt;
&lt;p&gt;Every time I read code written by an academic, lecturer or someone who uses R frequently, I come across the arrow symbol &lt;code&gt;&amp;lt;-&lt;/code&gt; used for assignment of variables. Never in my career have I seen someone systematically use the equals symbol &lt;code&gt;=&lt;/code&gt; across their code.&lt;/p&gt;
&lt;div id=&#34;benefits-of-the-arrow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Benefits of the arrow&lt;/h3&gt;
&lt;p&gt;A frequent association with &lt;code&gt;&amp;lt;-&lt;/code&gt; is in how assignment works in R. The variable on the right hand side of the operator is assigned to the one on the left. Hence the arrow makes a lot of sense. We can also do it the other way around, for instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;3 -&amp;gt; x
y &amp;lt;- 5
cat(&amp;quot;x is&amp;quot;, x, &amp;quot;and y is&amp;quot;, y, &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## x is 3 and y is 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the arrow has a benefit when teaching programming, so if you’re a beginner it is obvious which way around variables are assigned. If you’re not a beginner, it might reinforce this knowledge so that you don’t make mistakes.&lt;/p&gt;
&lt;p&gt;You can also use the arrow inside of functions to assign variables, for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(x &amp;lt;- solve(matrix(rnorm(100^2), 100, 100)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.009   0.007   0.004&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can view &lt;code&gt;x&lt;/code&gt; separately, even though it was assigned inside the &lt;code&gt;system.time&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x[1:5, 1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]         [,2]        [,3]        [,4]        [,5]
## [1,] -0.06462205 -0.020931424  0.11817438 -0.18430566 -0.03781733
## [2,]  0.03279143 -0.007782603 -0.12716072  0.03636665  0.10505115
## [3,]  0.02362454 -0.001848696 -0.05952053 -0.04676793 -0.01750361
## [4,] -0.05040604  0.047707725  0.02687982 -0.01738263  0.01374564
## [5,] -0.01643280 -0.040117354  0.12479274 -0.00396290 -0.05332341&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is perhaps its most useful application, which you cannot do with &lt;code&gt;=&lt;/code&gt;. The &lt;code&gt;=&lt;/code&gt; sign inside of a function argument is strictly used for matching the function argument with the variable you’re passing through.&lt;/p&gt;
&lt;p&gt;The arrow also has historical significance, since R’s predecessor, S, used &lt;code&gt;&amp;lt;-&lt;/code&gt; exclusively. This &lt;a href=&#34;https://www.r-bloggers.com/why-do-we-use-arrow-as-an-assignment-operator/&#34;&gt;R-bloggers post&lt;/a&gt; explains that S was based on an older language called APL, which was designed on a keyboard that had an arrow key exactly like &lt;code&gt;&amp;lt;-&lt;/code&gt;. But our keyboards now only have a key for &lt;code&gt;=&lt;/code&gt;, right?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-you-should-accept-the-equals-sign&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why you should accept the equals sign&lt;/h3&gt;
&lt;p&gt;But I’m here today to tell you to not use &lt;code&gt;&amp;lt;-&lt;/code&gt; and to use &lt;code&gt;=&lt;/code&gt; instead. Start by asking yourself why you use the arrow? Maybe you have historical reasons and used R before 2001, or more likely, you’re following convention for coding in R that even &lt;a href=&#34;https://google.github.io/styleguide/Rguide.html&#34;&gt;styling guides&lt;/a&gt; &lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34;&gt;recommend&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Firstly, no other programming language uses the arrows, at least, none of the most frequently used ones such as Python, MATLAB, C++, Julia, Javascript, etc. So if you’re like me and use R alongside other programming languages, why would you bother using &lt;code&gt;&amp;lt;-&lt;/code&gt; instead of &lt;code&gt;=&lt;/code&gt;? Wouldn’t you like consistency across the languages you write in, at least so that your muscle memory doesn’t have to change depending on whether you’re fitting a Neural network in Python, or a GAM in R?&lt;/p&gt;
&lt;p&gt;Okay fair enough, maybe you don’t mind switching coding styles depending on what language you’re writing in, after all, you are going to be changing a lot more than just the assignment operator. So what other benefits does &lt;code&gt;=&lt;/code&gt; have?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a button for it on the keyboard.&lt;/li&gt;
&lt;li&gt;Consistency between function arguments and assignment.&lt;/li&gt;
&lt;li&gt;Increased readability and neatness since it has fewer character (admittedly, this is subjective).&lt;/li&gt;
&lt;li&gt;Similarity with equality operator (&lt;code&gt;==&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;No confusion between for example &lt;code&gt;x&amp;lt;-2&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(x=2\)&lt;/span&gt;) and &lt;code&gt;x &amp;lt; -2&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; -2\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Consistency with &lt;em&gt;mathematics itself&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, I prefer to use the equals assigment operator over the arrow, because I like to code in more than just one language.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-neat-full-stop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The neat full stop&lt;/h3&gt;
&lt;p&gt;While I’m on the subject of the arrow, using a full stop in a variable name brings a lot of confusion. This one is a lot less controversial than disregarding the arrow in my opinion. We can name a variable in R as&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;some.variable = 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks neat! But in other languages, this would throw an error. Why is that? Languages like Python use &lt;code&gt;.&lt;/code&gt; as a class operator, and you use it to access elements of a class exclusively, so you cannot use it in variable names. But R doesn’t have this problem, right?&lt;/p&gt;
&lt;p&gt;When defining an S3 class in R, you can overwrite some default functions (such as &lt;code&gt;print&lt;/code&gt; or &lt;code&gt;plot&lt;/code&gt;) with a new function that handles these default operations in a different way for your specific S3 class. To do this for an S3 class called &lt;code&gt;mys3class&lt;/code&gt;, you would write a new functions as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.mys3class = function(x, ...){
  ...
}
plot.mys3class = function(x, ...){
  ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look familiar? So full stops &lt;em&gt;do&lt;/em&gt; have a purpose in R apart from assigning neat variable names. For me, I don’t like using full stops for the main reason I don’t like using the &lt;code&gt;&amp;lt;-&lt;/code&gt; operator: &lt;strong&gt;consistency&lt;/strong&gt;. If I’m using &lt;code&gt;&amp;lt;-&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt;, it will be for a specific purpose where I cant use &lt;code&gt;=&lt;/code&gt; or &lt;code&gt;_&lt;/code&gt; (however, these are rules that I’ve broken myself, and you can probably find instances of it in my portfolios).&lt;/p&gt;
&lt;p&gt;So whilst neither the arrow (&lt;code&gt;&amp;lt;-&lt;/code&gt;) for assignment nor the full stop (&lt;code&gt;.&lt;/code&gt;) for variable naming are completely useless, better alternatives &lt;em&gt;do&lt;/em&gt; exist. However, if you value your code looking neat above all else, and aren’t bothered by cross platform consistency; then you can use R’s exclusive &lt;code&gt;&amp;lt;-&lt;/code&gt;, or its inconsistent &lt;code&gt;.&lt;/code&gt; without issue.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Process Classification</title>
      <link>https://dannyjameswilliams.co.uk/projects/gpc/</link>
      <pubDate>Sat, 27 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/projects/gpc/</guid>
      <description>&lt;p&gt;The second group project I worked on at COMPASS mainly involved learning how Gaussian process classification worked, as it is a complicated procedure, and not as straight forward as Gaussian process classification.&lt;/p&gt;
&lt;p&gt;Our work involved a number of aspects that have improved on Gaussian process classification in recent literatures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pseudo-Marginal Likelihood:&lt;/strong&gt; An importance sampling procedure to approximate the marginal likelihood in MCMC sampling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Subset Selection&lt;/strong&gt;: An entropy based measure that chooses a subset of a full dataset that maximises information across the dataset, referred to as the &lt;em&gt;Information Vector Machine (IVM)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Laplace Approximation:&lt;/strong&gt; An approximation of the posterior of the latent variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Together, these approximations makes Gaussian process classification feasibile. Without approximations such as these, the procedure would have an incredible runtime.&lt;/p&gt;
&lt;p&gt;Finally, we compared the results on an e-mail spam dataset, and had a higher prediction accuracy than a JAGS implementation of logistic regression. We combined our code, written in Rcpp, into an R package, available 
&lt;a href=&#34;https://github.com/dannyjameswilliams/gpc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Electricity Demand Forecasting Hackathon</title>
      <link>https://dannyjameswilliams.co.uk/post/hackathon/</link>
      <pubDate>Mon, 17 Feb 2020 15:39:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/hackathon/</guid>
      <description>


&lt;p&gt;In February I participated in a COMPASS hackathon, where me and my fellow students fit statistical models to try to improve predictions in forecasting electricity demand based on weather related variables.&lt;/p&gt;
&lt;p&gt;We were fortunate to be visited by Dr Jethro Browell, a Research Fellow at the University of Strathclyde, who gave a brief lecture on how electricity demand was calculated, and how much it has changed over the last decade. After the lecture, Dr Mateo Fasiolo, a lecturer who works with us, explained a basic Generalised Additive Model (GAM) which can be used to forecast electricity demand for a particular dataset.&lt;/p&gt;
&lt;p&gt;Our task was to output a set of predictions for a testing dataset and submit them to the group git repository. We only had access to the predictor variables of this dataset, so we wouldn’t know how well our model was doing until it was submit and checked by Dr Fasiolo, who then put all submitted scores on the projector at the front of the room. The team with the lowest root mean squared error at the end would be crowned the winner.&lt;/p&gt;
&lt;p&gt;Me and my team “Jim” (named so because we went to the gym) performed well at the start, extending the basic GAM to include additional covariates and interactions, as well as including some feature engineering. The second team “AGang” (because all of their names began with “A”) took the edge over us by removing a single variable that we didn’t realise was actually making our model worse, and their GAM produced better predictions overall by a small margin. The third team “D &amp;amp; D” (because both their names began with a D) was having no luck at all, trying to implement a random forest model as opposed to a GAM, but their predictions were significantly off, and their code took much longer to run than ours, leaving them with little time to troubleshoot.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/hackathon.jpg&#34; alt=&#34;The COMPASS cohort after participating in the hackathon, with Dr Jethro Browell and Dr Mateo Fasiolo in the front.&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The COMPASS cohort after participating in the hackathon, with Dr Jethro Browell and Dr Mateo Fasiolo in the front.
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;Try as we did, we were unable to do any better than our original model; but we limited our scope to a GAM, and did not try anything out-of-the-box compared to the other two teams.&lt;/p&gt;
&lt;p&gt;The “AGang” were set to win it, until a surprise twist of fate sent “D&amp;amp;D” soaring into the lead, with predictions that had a far smaller error than anyone elses. The random forest model they were fitting before had an error, and they managed to fix the error, finish running the model and submit their predictions with only moments to spare. Thus, we came last.&lt;/p&gt;
&lt;p&gt;This was a fun competition, even though we lost. I realise that our mistake now was that we did not include anything special in our model that accounted for different weather patterns in different regions. Our model would have done very well if it was more variable; so that certain predictors were included in some areas that had more solar power, for instance. The way which we fit the model was the same for all regions, even though they were all quite different.&lt;/p&gt;
&lt;p&gt;You can read the article from the Bristol school of mathematics &lt;a href=&#34;https://www.bristol.ac.uk/maths/news/2020/compass-hackathon.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chicago Crime Classification</title>
      <link>https://dannyjameswilliams.co.uk/projects/chicago/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/projects/chicago/</guid>
      <description>&lt;p&gt;My most recent project was a group project as part of the first term of the COMPASS CDT. It involved modelling different aspects of a large data set detailing crimes in Chicago. Pictured is a kernel density estimate of both the location of crimes in the city as well as the population.&lt;/p&gt;
&lt;p&gt;My input to this project was to use logistic regression methods to classify whether an arrest would be successful or not, given other covariates in the crime set; detailing the location, the area, the type of crime type, amongst others. This involved feature engineering and using LASSO coefficient paths to judge the most &amp;lsquo;important&amp;rsquo; covariates.&lt;/p&gt;
&lt;p&gt;The package that was created for this project can be found as a Github repository 
&lt;a href=&#34;https://github.com/jakespiteri/chicagocrime&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Python for Statistics</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/pythonstats/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/pythonstats/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Python is a powerful programming language that is very popular for data analysis as well as many other applications in the real world. This portfolio will go over some intermediary processes in Python 3.7.3, before  implementing basic statistical models; including linear models, regression and classification.&lt;/p&gt;
&lt;h2 id=&#34;python-overview&#34;&gt;Python Overview&lt;/h2&gt;
&lt;h3 id=&#34;functions-and-modules&#34;&gt;Functions and Modules&lt;/h3&gt;
&lt;p&gt;Similar to R, functions can be defined and called in the same script. The function definition syntax is as follows&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def function_name(input1, input2):
    variable = function_operation
    second_variable = second_function_operation
    return output
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There&amp;rsquo;s a couple things to note here. Firstly, indentation is essential to defining the function. Unlike R or MATLAB, there is nothing to write that marks the beginning or end of the function as this is all handled by where the indentation begins and ends. Secondly, the function name comes after the &lt;code&gt;def&lt;/code&gt;, instead of the other way around (as in R). Finally, the colon (&lt;code&gt;:&lt;/code&gt;) is necessary, as it is in loops and if statements.&lt;/p&gt;
&lt;p&gt;Functions can be written in one script, and called in another, and the way this is achieved is through &lt;em&gt;modules&lt;/em&gt;. Modules are a collection of code that you can load from another file, similar to how &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;library&lt;/code&gt; work in R. You load modules with any of these lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import module_name
import module_name as short_name
from module_name import function_name
from module_name import *
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All these commands load the module (or parts of it), but do it slightly differently. In Python, all modules loaded have to be accessed through their name to use their functions. For example, to use a function from NumPy, you would have to use &lt;code&gt;numpy.function_name&lt;/code&gt;. This name can be shortened if you used the second line above to load the module, commonly, NumPy is abbreviated to &lt;code&gt;np&lt;/code&gt; so that you would only have to type &lt;code&gt;np.function_name&lt;/code&gt; to access it.&lt;/p&gt;
&lt;p&gt;You can also load specific functions from a module by using the third and fourth line above, where the asterisk &lt;code&gt;*&lt;/code&gt; indicates to load all functions from the module. Note that if the module is loaded this way, you do &lt;em&gt;not&lt;/em&gt; need to specify the name of the module preceding the function. It is considered bad practice to load functions this way, as declaring the module name removes the chance of overlapping function names within modules.&lt;/p&gt;
&lt;h4 id=&#34;example-morse-code&#34;&gt;Example: Morse Code&lt;/h4&gt;
&lt;p&gt;As an example of basic operations in Python, consider writing two functions to encode and decode morse code respectively. The encoding function will take a string, or a piece of text, and convert it into morse code. The decoding function does the opposite: converts morse code into plain english text. To write these functions, we first must define a dictionary:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;letter_to_morse = {&#39;a&#39;:&#39;.-&#39;, &#39;b&#39;:&#39;-...&#39;, &#39;c&#39;:&#39;-.-.&#39;, &#39;d&#39;:&#39;-..&#39;, &#39;e&#39;:&#39;.&#39;, 
        &#39;f&#39;:&#39;..-.&#39;, &#39;g&#39;:&#39;--.&#39;, &#39;h&#39;:&#39;....&#39;, &#39;i&#39;:&#39;..&#39;, &#39;j&#39;:&#39;.---&#39;, &#39;k&#39;:&#39;-.-&#39;, 
        &#39;l&#39;:&#39;.-..&#39;, &#39;m&#39;:&#39;--&#39;, &#39;n&#39;:&#39;-.&#39;, &#39;o&#39;:&#39;---&#39;, &#39;p&#39;:&#39;.--.&#39;, &#39;q&#39;:&#39;--.-&#39;, 
        &#39;r&#39;:&#39;.-.&#39;, &#39;s&#39;:&#39;...&#39;, &#39;t&#39;:&#39;-&#39;, &#39;u&#39;:&#39;..-&#39;, &#39;v&#39;:&#39;...-&#39;, &#39;w&#39;:&#39;.--&#39;, 
        &#39;x&#39;:&#39;-..-&#39;, &#39;y&#39;:&#39;-.--&#39;, &#39;z&#39;:&#39;--..&#39;, &#39;0&#39;:&#39;-----&#39;, &#39;1&#39;:&#39;.----&#39;, 
        &#39;2&#39;:&#39;..---&#39;, &#39;3&#39;:&#39;...--&#39;, &#39;4&#39;:&#39;....-&#39;,&#39;5&#39;:&#39;.....&#39;, &#39;6&#39;:&#39;-....&#39;, 
        &#39;7&#39;:&#39;--...&#39;, &#39;8&#39;:&#39;---..&#39;, &#39;9&#39;:&#39;----.&#39;, &#39; &#39;:&#39;/&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this dictionary can be accessed by indexing with the letter that we want a translation for. For example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;letter_to_morse[&amp;quot;6&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, we need to define the reverse; a dictionary that takes morse code and outputs an english letter. We can reverse this dictionary with a loop.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;morse_to_letter = {}
for letter in letter_to_morse:
    morse = letter_to_morse[letter]
    morse_to_letter[morse] = letter
morse_to_letter[&amp;quot;.-&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can define the two functions &lt;code&gt;encode&lt;/code&gt; and &lt;code&gt;decode&lt;/code&gt; that use these two dictionaries. For the encoding function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def encode(x): 
    morse = []
    for letter in x:
        letter = letter.lower()
        morse.append(letter_to_morse[letter])

    morse_message = &amp;quot; &amp;quot;.join(morse)
    return morse_message
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function initialises an array &lt;code&gt;morse&lt;/code&gt;, and loops over all letters in the input &lt;code&gt;x&lt;/code&gt; and appends to the &lt;code&gt;morse&lt;/code&gt; array the translation in morse code. After the loop, the &lt;code&gt;morse&lt;/code&gt; array is joined together. For the decoding function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def decode(x):
    english = []
    morse_letters = x.split(&amp;quot; &amp;quot;)
    for letter in morse_letters:
        english.append(morse_to_letter[letter])
        
    english_message = &amp;quot;&amp;quot;.join(english)
    return english_message
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has a similar process as with &lt;code&gt;encode&lt;/code&gt;. First the input &lt;code&gt;x&lt;/code&gt; is split, and then each split morse symbol is looped over and converted to english using the &lt;code&gt;morse_to_letter&lt;/code&gt; dictionary. Before we run these functions, we can save them in a separate file called &lt;code&gt;morse.py&lt;/code&gt; and put it in our working directory. Then using another script (which will be called &lt;code&gt;run_morse.py&lt;/code&gt;), we can import the &lt;code&gt;morse.py&lt;/code&gt; functions as a module.&lt;/p&gt;
&lt;p&gt;So we can import &lt;code&gt;morse&lt;/code&gt; and run the &lt;code&gt;decode&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import morse
morse_message = &amp;quot;.... . .-.. .--. / .. -- / - .-. .- .--. .--. . -.. / .. -. / ... --- -- . / -- --- .-. ... . / -.-. --- -.. .&amp;quot;
morse.decode(morse_message)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, we can encode a secret message using &lt;code&gt;morse.encode&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;morse.encode(&amp;quot;please dont turn me into morse code&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing to note here is that the dictionaries &lt;code&gt;morse_to_letter&lt;/code&gt; and &lt;code&gt;letter_to_morse&lt;/code&gt; were defined outside of the functions. Since this module was imported, we can actually access this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;morse.letter_to_morse
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although oftentimes we would want variables to remain hidden, this is not possible if we are importing all the contents of a script. Importing Python files as modules is usually done so that functions can be imported across the working directory, as opposed to having one big file that contains all the code.&lt;/p&gt;
&lt;h3 id=&#34;classes&#34;&gt;Classes&lt;/h3&gt;
&lt;p&gt;Object oriented programming in Python is done through &lt;em&gt;classes&lt;/em&gt;, the general form of which is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class class_name:
      def __init__(self):
          self.variable_I_want_to_define = variable
          self.another_variable_I_want_to_define = variable_2
      def some_class_function(self, x):
          something_I_can_do_with_my_variables = x
          return some_output
      def some_other_function(self):
          self.an_internal_variable = 3
          return another_output
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The functions can be run as a member of the class, and the &lt;code&gt;__init__(self)&lt;/code&gt; section of the class is a function that is run when the class is &lt;em&gt;initialised&lt;/em&gt;. Any variables that are within the &lt;code&gt;__init__&lt;/code&gt; section of the class become defined, sometimes depending on the inputs to the &lt;code&gt;__init__&lt;/code&gt; function. These could be variables such as initial conditions, or initial estimates. We could add more inputs to the &lt;code&gt;__init__&lt;/code&gt; function after &lt;code&gt;self&lt;/code&gt;, if needed.&lt;/p&gt;
&lt;p&gt;The other functions can be called, and these could change some of the internal &lt;code&gt;self.&lt;/code&gt; variables, e.g. updating the initial conditions, running a certain loop or adding additional variables.&lt;/p&gt;
&lt;h4 id=&#34;example-morse-code-again&#34;&gt;Example: Morse Code Again&lt;/h4&gt;
&lt;p&gt;We can update the morse code example to write a class called &lt;code&gt;morse_code_translator&lt;/code&gt; that has an encode and decode function as part of the class. This is written as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class morse_code_translator:
      def __init__(self):
          self.letter_to_morse = {&#39;a&#39;:&#39;.-&#39;, &#39;b&#39;:&#39;-...&#39;, &#39;c&#39;:&#39;-.-.&#39;, &#39;d&#39;:&#39;-..&#39;, 
            &#39;e&#39;:&#39;.&#39;, &#39;f&#39;:&#39;..-.&#39;,  &#39;g&#39;:&#39;--.&#39;, &#39;h&#39;:&#39;....&#39;, &#39;i&#39;:&#39;..&#39;, &#39;j&#39;:&#39;.---&#39;, 
            &#39;k&#39;:&#39;-.-&#39;, &#39;l&#39;:&#39;.-..&#39;, &#39;m&#39;:&#39;--&#39;, &#39;n&#39;:&#39;-.&#39;, &#39;o&#39;:&#39;---&#39;, &#39;p&#39;:&#39;.--.&#39;, 
            &#39;q&#39;:&#39;--.-&#39;, &#39;r&#39;:&#39;.-.&#39;, &#39;s&#39;:&#39;...&#39;, &#39;t&#39;:&#39;-&#39;, &#39;u&#39;:&#39;..-&#39;, &#39;v&#39;:&#39;...-&#39;, 
            &#39;w&#39;:&#39;.--&#39;, &#39;x&#39;:&#39;-..-&#39;, &#39;y&#39;:&#39;-.--&#39;, &#39;z&#39;:&#39;--..&#39;, &#39;0&#39;:&#39;-----&#39;, 
            &#39;1&#39;:&#39;.----&#39;, &#39;2&#39;:&#39;..---&#39;, &#39;3&#39;:&#39;...--&#39;, &#39;4&#39;:&#39;....-&#39;, &#39;5&#39;:&#39;.....&#39;, 
            &#39;6&#39;:&#39;-....&#39;, &#39;7&#39;:&#39;--...&#39;, &#39;8&#39;:&#39;---..&#39;, &#39;9&#39;:&#39;----.&#39;, &#39; &#39;:&#39;/&#39;}
          self.morse_to_letter = {}
          for letter in self.letter_to_morse:
              morse = self.letter_to_morse[letter]
              self.morse_to_letter[morse] = letter
          self.english_message_history = []
          self.morse_message_history = []
      def encode(self, x):
          morse = []
          for letter in x:
              letter = letter.lower()
              morse.append(self.letter_to_morse[letter])
          morse_message = &amp;quot; &amp;quot;.join(morse)       
          self.english_message_history.append(x)
          self.morse_message_history.append(morse_message)
          return morse_message
      def decode(self, x):
          english = []
          morse_letters = x.split(&amp;quot; &amp;quot;)
          for letter in morse_letters:
              english.append(self.morse_to_letter[letter])
          english_message = &amp;quot;&amp;quot;.join(english)
          self.english_message_history.append(english_message)
          self.morse_message_history.append(x)
          return english_message
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this has simply moved the &lt;code&gt;decode&lt;/code&gt; and &lt;code&gt;encode&lt;/code&gt; functions over inside a class. The dictionaries are now written as &lt;code&gt;self.letter_to_morse&lt;/code&gt; and &lt;code&gt;self.morse_to_letter&lt;/code&gt;, with the &lt;code&gt;self.&lt;/code&gt; prefix. Other than that, I have also added two more variables: &lt;code&gt;english_message_history&lt;/code&gt; and &lt;code&gt;morse_message_history&lt;/code&gt;, to exemplify how variables can be updated by calling the functions. Every time &lt;code&gt;encode&lt;/code&gt; or &lt;code&gt;decode&lt;/code&gt; is run, these arrays keep track of all messages that have passed through either function. We can test to see if this works, by importing this class from &lt;code&gt;morse_class.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from morse_class import morse_code_translator
translator = morse_code_translator()
translator.encode(&amp;quot;please not again&amp;quot;)
translator.decode(&amp;quot;.--. .-.. . .- ... . / -.. --- -. - / - ..- .-. -. / -- . / .. -. - --- / . -. --. .-.. .. ... ....&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this has worked as expected, and the &lt;code&gt;translator&lt;/code&gt; object now can both decode and encode morse code messages. Now that these two messages have passed through the class, we can see if the history has worked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;translator.english_message_history
translator.morse_message_history
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the internal &lt;code&gt;self&lt;/code&gt; variables have been updated with the two messages that were passed through the class.&lt;/p&gt;
&lt;h2 id=&#34;statistical-models&#34;&gt;Statistical Models&lt;/h2&gt;
&lt;p&gt;Python has a lot of support for fitting statistical models and machine learning, this is mostly as part of the &lt;code&gt;sklearn&lt;/code&gt; package. Other modules must also be imported that provide different funtionality. Statistical models are handled by &lt;code&gt;sklearn&lt;/code&gt;, dataframes are handled by &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt;, randomness is handled by &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;random&lt;/code&gt;, and plots are handled by &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sklearn as sk
import sklearn.linear_model as lm
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;linear-regression&#34;&gt;Linear Regression&lt;/h3&gt;
&lt;p&gt;A basic linear regression model in python is handled as part of the &lt;code&gt;sklearn.linear_model&lt;/code&gt; module. The use of a linear model in Python can be explained through an example. Some simulated data can be set up as follows
$$
\boldsymbol{x} \sim \text{Unif}(0,1), \qquad y \sim N(\boldsymbol{\mu}, .5), \qquad \boldsymbol{\mu} = \beta_0 + \beta_1 \boldsymbol{x},
$$
where both $\boldsymbol{x}$ and $\boldsymbol{y}$ are of length $n$. Since this is simulated data, we pre-define $\beta_0 = 5$ and $\beta_1 = -2$ as the intercept and the gradient. We can use &lt;code&gt;numpy&lt;/code&gt; to sample this data, and the &lt;code&gt;linear_model&lt;/code&gt; module in &lt;code&gt;sklearn&lt;/code&gt; to fit a linear model to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n = 200
beta0 = 5
beta1 = -2
x = np.random.uniform(0, 1, n)
mu = beta0 + beta1*x
y = np.random.normal(mu, .5, n)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data can be converted into a dataframe with the &lt;code&gt;DataFrame&lt;/code&gt; function in &lt;code&gt;pandas&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = pd.DataFrame({&amp;quot;x&amp;quot;:x, &amp;quot;y&amp;quot;:y})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To set up the linear model, a model object must be set up in advance, and then the fitting routine can be called to it. This can be done in one line by nesting operations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = lm.LinearRegression(fit_intercept=True).fit(data[[&amp;quot;x&amp;quot;]], data[&amp;quot;y&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the model has been fit by first creating a linear model object with &lt;code&gt;lm.LinearRegression&lt;/code&gt; (and specifying that we &lt;em&gt;do&lt;/em&gt; want an intercept). Secondly, the &lt;code&gt;.fit()&lt;/code&gt; function has been called on that object to obtain parameter estimates $\beta_0$ and $\beta_1$ by passing the covariates &lt;code&gt;x&lt;/code&gt; and response variable &lt;code&gt;y&lt;/code&gt; to it. Now we can see whether these estimates bare resemblance to the true parameter values used to simulate the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Intercept:&amp;quot;, model.intercept_, &amp;quot;,&amp;quot;, &amp;quot;Gradient:&amp;quot;, model.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept: 4.89676478340633 , Gradient: [-1.90845212]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is reasonably close to the true values. Note that here there is only one covariate vector, but this method is applicable to multiple covariates which would give multiple estimates of the gradient for each covariate. Now we can plot the data and the fit, a &lt;code&gt;pandas&lt;/code&gt; data frame has an inbuilt method used for plotting the data, which produces a scatter plot from &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.plot.scatter(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;, figsize=(14, 11));
plt.plot(x, model.predict(data[[&amp;quot;x&amp;quot;]]), &#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/pythonstats_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;multiple-linear-regression&#34;&gt;Multiple Linear Regression&lt;/h3&gt;
&lt;p&gt;In the previous section, we fit a linear model with a single covariate vector. This is good for exemplary purposes as a two dimensional linear model is easy to visualise. Often in data analysis we can have multiple explanatory variables $\boldsymbol{x}_1, \dots, \boldsymbol{x}_p$ that give information about the respone variable $\boldsymbol{y}$.&lt;/p&gt;
&lt;p&gt;For an example of multiple linear regression, we will be using the Boston house prices dataset, which is part of the freely provided datasets from &lt;code&gt;sklearn&lt;/code&gt;. This dataset describes the median value of owner-occupied homes (per $1000), as well as 13 predictors that could provide information about the house value. We load this dataset and convert it to a &lt;code&gt;pandas&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.datasets import load_boston
data = load_boston()
boston = pd.DataFrame(data.data, columns = data.feature_names)
boston[&amp;quot;MEDV&amp;quot;] = data.target
boston.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;CRIM&lt;/th&gt;
      &lt;th&gt;ZN&lt;/th&gt;
      &lt;th&gt;INDUS&lt;/th&gt;
      &lt;th&gt;CHAS&lt;/th&gt;
      &lt;th&gt;NOX&lt;/th&gt;
      &lt;th&gt;RM&lt;/th&gt;
      &lt;th&gt;AGE&lt;/th&gt;
      &lt;th&gt;DIS&lt;/th&gt;
      &lt;th&gt;RAD&lt;/th&gt;
      &lt;th&gt;TAX&lt;/th&gt;
      &lt;th&gt;PTRATIO&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
      &lt;th&gt;LSTAT&lt;/th&gt;
      &lt;th&gt;MEDV&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.00632&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;2.31&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.538&lt;/td&gt;
      &lt;td&gt;6.575&lt;/td&gt;
      &lt;td&gt;65.2&lt;/td&gt;
      &lt;td&gt;4.0900&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;296.0&lt;/td&gt;
      &lt;td&gt;15.3&lt;/td&gt;
      &lt;td&gt;396.90&lt;/td&gt;
      &lt;td&gt;4.98&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.02731&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7.07&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.469&lt;/td&gt;
      &lt;td&gt;6.421&lt;/td&gt;
      &lt;td&gt;78.9&lt;/td&gt;
      &lt;td&gt;4.9671&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;242.0&lt;/td&gt;
      &lt;td&gt;17.8&lt;/td&gt;
      &lt;td&gt;396.90&lt;/td&gt;
      &lt;td&gt;9.14&lt;/td&gt;
      &lt;td&gt;21.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.02729&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7.07&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.469&lt;/td&gt;
      &lt;td&gt;7.185&lt;/td&gt;
      &lt;td&gt;61.1&lt;/td&gt;
      &lt;td&gt;4.9671&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;242.0&lt;/td&gt;
      &lt;td&gt;17.8&lt;/td&gt;
      &lt;td&gt;392.83&lt;/td&gt;
      &lt;td&gt;4.03&lt;/td&gt;
      &lt;td&gt;34.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.03237&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.458&lt;/td&gt;
      &lt;td&gt;6.998&lt;/td&gt;
      &lt;td&gt;45.8&lt;/td&gt;
      &lt;td&gt;6.0622&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;222.0&lt;/td&gt;
      &lt;td&gt;18.7&lt;/td&gt;
      &lt;td&gt;394.63&lt;/td&gt;
      &lt;td&gt;2.94&lt;/td&gt;
      &lt;td&gt;33.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.06905&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.458&lt;/td&gt;
      &lt;td&gt;7.147&lt;/td&gt;
      &lt;td&gt;54.2&lt;/td&gt;
      &lt;td&gt;6.0622&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;222.0&lt;/td&gt;
      &lt;td&gt;18.7&lt;/td&gt;
      &lt;td&gt;396.90&lt;/td&gt;
      &lt;td&gt;5.33&lt;/td&gt;
      &lt;td&gt;36.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The variable we are interested in predicting is &lt;code&gt;MEDV&lt;/code&gt;, and all other variables will be used to give information to predicting &lt;code&gt;MEDV&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before fitting any models, we can perform some exploratory data analysis to help the decision as to what predictor variables need to be included in the model fitting. Luckily, there are easy methods available that inspect how strongly correlated variables are with other variables in a &lt;code&gt;pandas&lt;/code&gt; data frame. Firstly, we will be using the inbuilt &lt;code&gt;corr&lt;/code&gt; function and plotting it as a heatmap. This uses the &lt;code&gt;seaborn&lt;/code&gt; module to create a heatmap.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import seaborn
corr = boston.corr()
cmap = seaborn.diverging_palette(-500, -720, as_cmap=True)
plt.figure(figsize=(14, 11))
seaborn.heatmap(corr, cmap=cmap);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/pythonstats_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are two things to infer from this plot: how correlated the predictor variables are from one another, and how correlated the predictor variables are from the response. We are mostly interested in how correlated the predictors are with &lt;code&gt;MEDV&lt;/code&gt;, as if they influence the value of &lt;code&gt;MEDV&lt;/code&gt; significantly, then they are likely to be important to include in a model. Another plot we can use to explore the data is a multi variable scatter plot. That is, a scatter plot for each pair of variables. &lt;code&gt;pandas&lt;/code&gt; has a function for this, as part of the &lt;code&gt;plotting&lt;/code&gt; submodule.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pandas.plotting import scatter_matrix
scatter_matrix(boston, figsize=(16, 16));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./pythonstats_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There is a lot to process in this plot, but you can restrict your attention to the final column on the right. This column will show scatter plots with all predictor variables and the response &lt;code&gt;MEDV&lt;/code&gt;. We are looking for obvious relationships between the two variables. &lt;code&gt;CRIM&lt;/code&gt; seems to have an effect at lower values, &lt;code&gt;ZN&lt;/code&gt; seems to have a small effect, &lt;code&gt;INDUS&lt;/code&gt; has a non-linear relationship,&lt;code&gt;RM&lt;/code&gt; has a clear linear relationship, &lt;code&gt;AGE&lt;/code&gt; seems to have a significant effect at lower values, &lt;code&gt;B&lt;/code&gt; has an effect at larger values and &lt;code&gt;LSTAT&lt;/code&gt; seems to have a quadratic relationship. These variables will be important to consider, but that does not mean all others are not. Since some of these predictors are categorical (even binary), it can be hard to infer their relationship with &lt;code&gt;MEDV&lt;/code&gt; from a scatter plot.&lt;/p&gt;
&lt;p&gt;We can now fit the model using the information above. The predictors we want to include are &lt;code&gt;CRIM&lt;/code&gt;, &lt;code&gt;ZN&lt;/code&gt;, &lt;code&gt;INDUS&lt;/code&gt;, &lt;code&gt;RM&lt;/code&gt;, &lt;code&gt;AGE&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;LSTAT&lt;/code&gt;. We use the linear model from &lt;code&gt;sklearn&lt;/code&gt; again, but first create the new data frame of the reduced dataset. Additionally we can include a quadratic effect for certain predictors by creating a new column in the data frame with the squared values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;boston_x = boston[[&amp;quot;CRIM&amp;quot;, &amp;quot;ZN&amp;quot;, &amp;quot;INDUS&amp;quot;, &amp;quot;RM&amp;quot;, &amp;quot;AGE&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;LSTAT&amp;quot;]]
boston_x.insert(len(boston_x.columns), &amp;quot;LSTAT2&amp;quot;, [i**2 for i in boston_x[&amp;quot;LSTAT&amp;quot;] ]) 
boston_x.insert(len(boston_x.columns), &amp;quot;AGE2&amp;quot;, [i**2 for i in boston_x[&amp;quot;AGE&amp;quot;] ]) 
boston_x.insert(len(boston_x.columns), &amp;quot;B2&amp;quot;, [i**2 for i in boston_x[&amp;quot;B&amp;quot;] ]) 

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now using the linear model with this modified data frame to fit the full model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mv_model = lm.LinearRegression(fit_intercept=True).fit(boston_x, data.target)
print(&amp;quot;Intercept: \n&amp;quot;, mv_model.intercept_, &amp;quot;\n&amp;quot;)
print(&amp;quot;Coefficients:&amp;quot;)
print(&amp;quot; \n&amp;quot;.join([str(boston_x.columns[i]) + &amp;quot; &amp;quot; + str(mv_model.coef_[i]) for i in np.arange(len(mv_model.coef_))]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept: 
 5.916804277809813 

Coefficients:
CRIM -0.11229848683846888 
ZN 0.0037476008537828498 
INDUS -0.0021805395766587208 
RM 4.0621316253156285 
AGE 0.07875634757295602 
B 0.042639346099976716 
LSTAT -2.063750521016358 
LSTAT2 0.042240400122337596 
AGE2 -0.0002509463690499019 
B2 -7.788232883181183e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;penalised-regression&#34;&gt;Penalised Regression&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;linear_model&lt;/code&gt; submodule of &lt;code&gt;sklearn&lt;/code&gt; doesn&amp;rsquo;t &lt;em&gt;just&lt;/em&gt; contain the &lt;code&gt;LinearRegression&lt;/code&gt; object, it contains many other variations of linear models. Some examples of these are penalised regression models, such as the Lasso model or ridge regression model. You can view the documentation for these online, and they work similarly to &lt;code&gt;LinearRegression&lt;/code&gt;. Below I will briefly go through an example of using a Lasso path algorithm on the dataset provided above. Firstly, the model object can be created immediately using the &lt;code&gt;linear_model.lars_path&lt;/code&gt; object.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;_,_,coefs = lm.lars_path(data.data, data.target)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is using the full dataset provided in &lt;code&gt;data.data&lt;/code&gt;, as the Lasso path plot can be used for feature selection, similar to the scatter matrix approach above.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xx = np.sum(np.abs(coefs.T), axis=1)
xx /= xx[-1]
plt.figure(figsize=(16, 9))
plt.plot(xx, coefs.T)
ymin, ymax = plt.ylim()
plt.vlines(xx, ymin, ymax, linestyle=&#39;dashed&#39;)
plt.xlabel(&#39;|coef| / max|coef|&#39;)
plt.ylabel(&#39;Coefficients&#39;)
plt.title(&#39;LASSO Path&#39;)
plt.axis(&#39;tight&#39;)
plt.legend(data.feature_names)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/pythonstats_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The coefficients for covariates that provide a lot of information to the response variable are likely going to take a very large value of regularisation parameter to shrink them to zero. These are the lines that are more separated in the plot above, i.e. &lt;code&gt;CRIM&lt;/code&gt;, &lt;code&gt;RM&lt;/code&gt;, &lt;code&gt;NOX&lt;/code&gt;, and &lt;code&gt;CHAS&lt;/code&gt;. Interestingly, these were not the variables that were selected in the scatter matrix approach earlier in this report, highlighting the variation in parameter selection depending on the method.&lt;/p&gt;
&lt;h3 id=&#34;classification&#34;&gt;Classification&lt;/h3&gt;
&lt;p&gt;The final statistical model introduced will be one interested in classifying. Classification is interested in prediction of a usually non-numeric &lt;em&gt;class&lt;/em&gt;, i.e. assign the output to a pre-defined class based on some inputs. The classification model explained here will be &lt;strong&gt;Logistic Regression&lt;/strong&gt; (ignore the name, it&amp;rsquo;s not a regression method).&lt;/p&gt;
&lt;p&gt;Logistic regression is called so because it uses a &lt;em&gt;logistic function&lt;/em&gt; to map fitted values to probabilities. The basic form of a classification model is the same of that of a regression method, except the output from the model corresponds to latent, unobserved fitted values which decide the class of each set of inputs.&lt;/p&gt;
&lt;p&gt;We will start by simulating some data $\boldsymbol{y}$ that need to be classified, which will be generated by a mathematical combination of some inputs $\boldsymbol{x}$. Each element of $\boldsymbol{y}$ will be given one of the class labels $\mathcal{C}_1$, $\mathcal{C}_2$ or $\mathcal{C}_3$. The goal of the statistical model is parameter estimation, similar to the regression methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate x
n = 1000
x1 = np.random.uniform(0, 1, n)
x2 = np.random.uniform(0, 1, n)

# Create parameters and mean
beta0 = 3
beta1 = 1.5
beta2 = -4.8
beta3 = 0.5
mu = beta0 + beta1*x1 + beta2*x2 + beta3*x1**2

# Set class based on value of mean
y = np.repeat(&amp;quot;Class 1&amp;quot;, n)
y[mu &amp;gt; 0] = &amp;quot;Class 2&amp;quot;
y[mu &amp;gt; 2] = &amp;quot;Class 3&amp;quot;

# Combine into DataFrame
clsf_dat = pd.DataFrame({&amp;quot;y&amp;quot;:y,&amp;quot;x1&amp;quot;:x1,&amp;quot;x2&amp;quot;:x2 })
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;colors = y
colors[y == &amp;quot;Class 1&amp;quot;] = &amp;quot;red&amp;quot;
colors[y == &amp;quot;Class 2&amp;quot;] = &amp;quot;blue&amp;quot;
colors[y == &amp;quot;Class 3&amp;quot;] = &amp;quot;green&amp;quot;
clsf_dat.plot.scatter(&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;, c=colors, figsize=(14, 11));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/pythonstats_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that the data is set up, we can fit the logistic regression model to it. &lt;code&gt;LogisticRegression&lt;/code&gt; is a function as part of the &lt;code&gt;linear_model&lt;/code&gt; submodule of &lt;code&gt;sklearn&lt;/code&gt;, just as before, so this next step should be familiar to you.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lr_model = lm.LogisticRegression(fit_intercept=True, solver=&#39;newton-cg&#39;, multi_class = &#39;auto&#39;)
lr_model = lr_model.fit(clsf_dat[[&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;]], clsf_dat[&amp;quot;y&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Intercept: \n&amp;quot;, lr_model.intercept_, &amp;quot;\n&amp;quot;)
print(&amp;quot;Coefficients:&amp;quot;)
print(&amp;quot; \n&amp;quot;.join([str(clsf_dat.columns[i]) + &amp;quot; &amp;quot; + str(lr_model.coef_[i]) for i in np.arange(len(lr_model.coef_))]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept: 
 [-4.45432353  0.92640783  3.52791569] 

Coefficients:
y [-4.06694321  9.66044839] 
x1 [0.18794764 0.76440687] 
x2 [  3.87899558 -10.42485527]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that there are multiple coefficients for each input! This is because even though the data were simulated using one parameter each, the logistic regression model has a different input combination for each class. The first class has both coefficients aliased to the intercept, and anything significantly different (in line with the combination of the remaining parameters) is given a different class.&lt;/p&gt;
&lt;p&gt;We can roughly evaluate the model&amp;rsquo;s performance by inspecting a plot of its predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = lr_model.predict(clsf_dat[[&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;colors = predictions
colors[predictions == &amp;quot;Class 1&amp;quot;] = &amp;quot;red&amp;quot;
colors[predictions == &amp;quot;Class 2&amp;quot;] = &amp;quot;blue&amp;quot;
colors[predictions == &amp;quot;Class 3&amp;quot;] = &amp;quot;green&amp;quot;
clsf_dat.plot.scatter(&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;, c=colors, figsize=(14, 11));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/pythonstats_60_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we can see an extremely similar plot to the one above, although it is not quite exactly the same! At the point of overlap between classes, some points are mislabelled as the wrong class, due to uncertainty around the decision boundaries.&lt;/p&gt;
&lt;p&gt;More complex models exist for classification and regression, but that&amp;rsquo;s for another time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Network for Identifying Cats and Dogs in Python</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/neuralnet/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/neuralnet/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We fit a neural network in Python to a dataset of cat and dog images, in an attempt to distinguish features that can correctly identify whether the image is of a cat or of a dog.&lt;/p&gt;
&lt;p&gt;This work uses the &lt;code&gt;keras&lt;/code&gt; library from &lt;code&gt;tensorflow&lt;/code&gt;. This portfolio will go over the following processes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Loading the data and formatting it be fit by a neural network in &lt;code&gt;tensorflow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fitting a neural network to the images in a training set&lt;/li&gt;
&lt;li&gt;Identifying and evaluating predictive performance on a testing set&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
import numpy as np
import IPython.display as display
import matplotlib.pyplot as plt

import os
import pathlib

from tensorflow import keras
from PIL import Image

tf.__version__
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;2.1.0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;loading-and-formatting-the-dataset&#34;&gt;Loading and Formatting the Dataset&lt;/h2&gt;
&lt;p&gt;The data are from the Kaggle competition &amp;ldquo;Dogs vs Cats&amp;rdquo;. These images are loaded into the working directory and defined below. These are pre-separated into a training set, validation set and a testing set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir = &amp;quot;/home/fs19144/Documents/SC2/Section10/data/train/&amp;quot;
data_dir = pathlib.Path(data_dir)
class_names = np.array([item.name for item in data_dir.glob(&#39;*&#39;)])
image_count = len(list(data_dir.glob(&#39;*/*.jpg&#39;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are pre-downloaded and saved separately. This code aboves load the data, retrieves the class names and the total number of images in the training set. The class names are retrieved from the folder names, being &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;dog&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;cat&#39;, &#39;dog&#39;], dtype=&#39;&amp;lt;U3&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The image count is calculated by reading the length of the list containing all elements in the data directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image_count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To format the data for a neural network, a few things must be accomplished. Firstly, the images need to have a lower resolution that normal, to avoid large file sizes and to maintain consistency across the dataset. Secondly, the images need to be loaded in batches, to avoid storing all pixel values for all images in a large array. Consider doing this, we have 6002 images, each having a corresponding pixel value for red, green and blue. If the image height and width is 60 pixels, then in total the number of elements in the training set would be&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;60 * 60 * 3 * 6002
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;64821600
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is extremely large. This size increases greatly with a higher image resolution, or a larger dataset, highlighting the need to load images in batches. We define these variables to go into the model fitting below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batch_size = 32
image_height = 60
image_width = 60
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use functionality from tensorflow to list the data objects in a &lt;code&gt;tf.Dataset&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_ds = tf.data.Dataset.list_files(str(data_dir/&#39;*/*&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This training set contains the path to all files in the &lt;code&gt;train&lt;/code&gt; directory, which can be iterated over every time we want to create a batch. Below are some functions that we will be applying to this data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_label(file_path):
  parts = tf.strings.split(file_path, os.path.sep)
  return parts[-2] == class_names

def decode_img(img):
  img = tf.image.decode_jpeg(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)
  return tf.image.resize(img, [image_width, image_height])

def process_path(file_path):
  label = get_label(file_path)
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;process_path&lt;/code&gt; function takes the &lt;code&gt;file_path&lt;/code&gt; (given when we map across &lt;code&gt;train_ds&lt;/code&gt;), retrieves the label from &lt;code&gt;get_label&lt;/code&gt; and decodes the image into pixel values in &lt;code&gt;decode_img&lt;/code&gt;. &lt;code&gt;get_label&lt;/code&gt; simply reads the folder name which the file being iterated over is stored in, whilst &lt;code&gt;decode_img&lt;/code&gt; uses &lt;code&gt;tensorflow&lt;/code&gt; functionality to read the image in the required format. The &lt;code&gt;tf.data.Dataset&lt;/code&gt; structure which &lt;code&gt;train_ds&lt;/code&gt; is saved as has inbuilt mapping functionality, so we apply &lt;code&gt;process_path&lt;/code&gt; to get a labelled training set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;labelled_ds = train_ds.map(process_path, num_parallel_calls=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we create a function that prepares the dataset for training iterations. This shuffles the dataset, and takes a batch of the specified size we defined earlier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def prepare_for_training(ds, shuffle_buffer_size=1000):

  ds = ds.shuffle(buffer_size=shuffle_buffer_size)
  ds = ds.repeat()
  ds = ds.batch(batch_size)
  ds = ds.prefetch(buffer_size=2)

  return ds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_ds_2 = prepare_for_training(labelled_ds)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;validation-set&#34;&gt;Validation Set&lt;/h4&gt;
&lt;p&gt;A neural network model can be improved by using a validation set to reduce overfitting. The neural network model will be fit using the validation set to check the loss on data that it is not training on. The process for formatting the data above can be repeated for different images in a &lt;code&gt;validation&lt;/code&gt; folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir_val = &amp;quot;/home/fs19144/Documents/SC2/Section10/data/validation/&amp;quot;
data_dir_val = pathlib.Path(data_dir_val)
class_names_val = np.array([item.name for item in data_dir_val.glob(&#39;*&#39;)])
image_count_val = len(list(data_dir_val.glob(&#39;*/*.jpg&#39;)))
val_ds = tf.data.Dataset.list_files(str(data_dir_val/&#39;*/*&#39;))
labelled_ds_val = val_ds.map(process_path, num_parallel_calls=2)
val_ds_2 = prepare_for_training(labelled_ds_val)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;example-batch&#34;&gt;Example Batch&lt;/h4&gt;
&lt;p&gt;We can iterate once over &lt;code&gt;train_ds_2&lt;/code&gt; to view an example image batch, along with their labels:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image_batch, label_batch = next(iter(train_ds_2))
plt.figure(figsize=(10,12))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(image_batch[i], cmap=plt.cm.binary)
    ind = [bool(i) for i in (label_batch[i])]
    plt.xlabel(class_names[ind])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/neuralnet_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;fitting-the-neural-network&#34;&gt;Fitting the Neural Network&lt;/h2&gt;
&lt;p&gt;We use the &lt;code&gt;keras&lt;/code&gt; library, a part of &lt;code&gt;tensorflow&lt;/code&gt;, to fit a neural network to classify these images. Most importantly, the &lt;code&gt;Sequential&lt;/code&gt; function to build the model and different layer functions to create the neural network layers within the &lt;code&gt;Sequential&lt;/code&gt; model build.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is the code used to specify the model using &lt;code&gt;Sequential&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Sequential([
    Conv2D(16, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, 
           input_shape=(image_height, image_width ,3)),
    MaxPooling2D(),
    Dropout(0.2),
    Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;),
    MaxPooling2D(),
    Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;),
    MaxPooling2D(),
    Dropout(0.2),
    Flatten(),
    Dense(512, activation=&#39;relu&#39;),
    Dense(2)
])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This convolutional neural network contains 8 layers, with a final layer to output a classification.  The convolutional layers, given by &lt;code&gt;Conv2D&lt;/code&gt;, apply convolutional operations to the image and output a single value based on the operations. The pooling layers, given by &lt;code&gt;MaxPooling2D&lt;/code&gt;, extract key features of the image and reduce the dimensionality to reduce computational time. In this case, the max pooling approach extracts the maximum value over subregions of the image. &lt;code&gt;Dropout&lt;/code&gt; is another method that helps to reduce overfitting, dropping out a proportion of the dataset, and makes the distribution of weight values in the neural network more regular.&lt;/p&gt;
&lt;p&gt;Finally, the &lt;code&gt;Flatten&lt;/code&gt; function reformats the data, so that it is vector form and not in a high dimensional format. The &lt;code&gt;Dense&lt;/code&gt; layer provides most of the information to the model. These fully connected layers perform the classification on the output to all the other layers. The final &lt;code&gt;Dense(2)&lt;/code&gt; will output two nodes, giving probabilities for each one. This will be classifying either a cat or a dog.&lt;/p&gt;
&lt;p&gt;This model needs to be compiled. We use the &lt;code&gt;adam&lt;/code&gt; optimiser, which is a variation on gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(optimizer=&#39;adam&#39;,
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=[&#39;accuracy&#39;])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we fit the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;history = model.fit(
    train_ds_2,
    steps_per_epoch = image_count // batch_size,
    epochs = 10,
    validation_data = val_ds_2,
    validation_steps = image_count_val // batch_size
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train for 187 steps, validate for 62 steps
Epoch 1/10
187/187 [==============================] - 22s 119ms/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6906 - val_accuracy: 0.5514
Epoch 2/10
187/187 [==============================] - 22s 119ms/step - loss: 0.6578 - accuracy: 0.5639 - val_loss: 0.6227 - val_accuracy: 0.6127
Epoch 3/10
187/187 [==============================] - 21s 112ms/step - loss: 0.6024 - accuracy: 0.6527 - val_loss: 0.6277 - val_accuracy: 0.6235
Epoch 4/10
187/187 [==============================] - 21s 113ms/step - loss: 0.5647 - accuracy: 0.6868 - val_loss: 0.5571 - val_accuracy: 0.6920
Epoch 5/10
187/187 [==============================] - 21s 114ms/step - loss: 0.5284 - accuracy: 0.7214 - val_loss: 0.5474 - val_accuracy: 0.7054
Epoch 6/10
187/187 [==============================] - 22s 116ms/step - loss: 0.4949 - accuracy: 0.7411 - val_loss: 0.5419 - val_accuracy: 0.7114
Epoch 7/10
187/187 [==============================] - 21s 114ms/step - loss: 0.4685 - accuracy: 0.7585 - val_loss: 0.5114 - val_accuracy: 0.7384
Epoch 8/10
187/187 [==============================] - 21s 114ms/step - loss: 0.4392 - accuracy: 0.7807 - val_loss: 0.4992 - val_accuracy: 0.7523
Epoch 9/10
187/187 [==============================] - 22s 117ms/step - loss: 0.4091 - accuracy: 0.7996 - val_loss: 0.5721 - val_accuracy: 0.7256
Epoch 10/10
187/187 [==============================] - 21s 113ms/step - loss: 0.3819 - accuracy: 0.8182 - val_loss: 0.4976 - val_accuracy: 0.7573
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;evaluating-the-model&#34;&gt;Evaluating the Model&lt;/h2&gt;
&lt;p&gt;Now that the model is fit, how do we know that it&amp;rsquo;s doing a good job? Firstly, by the final lines in the model fitting, we can see that the prediction accuracy ended at around 0.82, so 82% of training set predictions were correct, and around 75% of validation set predicctions were.&lt;/p&gt;
&lt;p&gt;We can also plot the loss on the training set and the validation set below. We want this to decrease in both cases, as a smaller loss means a better fit. We also want the predictiona accuracy to increase, so that predictions are more correct. If the validation set loss has also decreased to a low value, and the accuracy has increased, then the model has avoided overfitting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;acc  = history.history[&#39;accuracy&#39;]
loss = history.history[&#39;loss&#39;]
val_loss = history.history[&#39;val_loss&#39;]
val_acc  = history.history[&#39;val_accuracy&#39;]

epochs_range = range(10)

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;)
plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;)
plt.legend(loc=&#39;lower right&#39;)
plt.title(&#39;Training and Validation Accuracy&#39;)

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;)
plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;)
plt.legend(loc=&#39;upper right&#39;)
plt.title(&#39;Training and Validation Loss&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/neuralnet_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;These plots show that the loss steadily decreases, and the accuracy increases in both cases. Now we can evaluate some predictions on the test set. The test set do not have specific class labels, and we are treating these as unknown.&lt;/p&gt;
&lt;h4 id=&#34;test-set-predictions&#34;&gt;Test Set Predictions&lt;/h4&gt;
&lt;p&gt;To further test the performance of the model, and inspect it ourselves, we can see how well the model predicts on a test set (which the model has never seen). This is a separate dataset to the validation set which was part of the model fitting process.&lt;/p&gt;
&lt;p&gt;Firstly, we load the image using the &lt;code&gt;PIL.Image&lt;/code&gt; module, and load the images into a list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import PIL.Image as Image
import glob


pred_list = []
for filename in glob.glob(&#39;/home/fs19144/Documents/SC2/Section10/data/test/*.jpg&#39;): 
    x = Image.open(filename).resize((image_height, image_width))
    pred_list.append(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we loop over the 20 elements in this small test set to inspect the model predictions. Each image is loaded as a &lt;code&gt;numpy&lt;/code&gt; array, which automatically converts it into pixel format. These pixel values need to be divided by 255, to get RGB values in $[0,1]$. We obtain model predictions using the simple &lt;code&gt;predict&lt;/code&gt; function as part of the &lt;code&gt;model&lt;/code&gt; object. These predictions come in terms of fitted values (not strictly probabilities). The predicted classes are then given by taking the largest fitted values output by the model. We can plot the images and their predictions below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred_class = np.empty([len(pred_list)], dtype=&amp;quot;S3&amp;quot;)

plt.figure(figsize=(10,12))
for i in np.arange(len(pred_list)):
    x = np.array(pred_list[i])/255.0
    pred = model.predict(x[np.newaxis, ...])     
    pred_class[i] = class_names[np.argmax(pred[0])]
    
    # Plot
    plt.subplot(4,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x)
    plt.xlabel(pred_class[i])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/portfolios/neuralnet_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are some funny looking dogs in this sample, and some funny looking cats. Clearly we can see that oftentimes the model calls a cat a dog, and a dog a cat (I imagine the dogs are more offended by that), but for the most part the predictions look accurate.&lt;/p&gt;
&lt;p&gt;There is a lot more that can go into improving this model, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-processing the image data&lt;/li&gt;
&lt;li&gt;Increasing the dataset size&lt;/li&gt;
&lt;li&gt;Increasing the image resolution&lt;/li&gt;
&lt;li&gt;Adding more layers to the neural network&lt;/li&gt;
&lt;li&gt;Adding &amp;lsquo;null&amp;rsquo; images, that show neither a cat nor dog, so that the model does not always predict a cat or dog, it can be neither&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://dannyjameswilliams.co.uk/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Downscaling Extremes of Precipitation in the South West</title>
      <link>https://dannyjameswilliams.co.uk/projects/precip/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/projects/precip/</guid>
      <description>&lt;p&gt;For my Masters dissertation project, I used extreme value theory to model the extremes of precipitation across the South West of the UK, and downscale sparse areas using a numerical weather model on a gridded scale. Pictured is the high resolution elevation levels across the South West, with a grid that samples the same elevation levels every 0.25 degrees in longitude and latitude. This highlights one of the problems with downscaling in such a scenario.&lt;/p&gt;
&lt;p&gt;Extreme values of rainfall can be categorised by their maxima, and using a generalised extreme value (GEV) distribution, these maxima can be modelled. I modelled two sets of maxima separately; for the observations (point locations) and the model output (gridded locations). A separate downscaling generalised additive model (GAM) was used to compare predictions from both models, and link them together using various covariates.&lt;/p&gt;
&lt;p&gt;You can read more about this project in my final dissertation, which can be found 
&lt;a href=&#34;https://dannyjameswilliams.co.uk/pdfs/proj/mastersdiss.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Section 5: High Performance Computing with Bluecrystal</title>
      <link>https://dannyjameswilliams.co.uk/portfolios/sc2/hpc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/portfolios/sc2/hpc/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We’ve all been in the situation where we want to perform some extremely complicated computing process, such as MCMC or manipulating an extremely large data frame a lot of times, but our laptops just aren’t good enough. They don’t have enough cores, no dedicated GPU and only a small amount of memory. Luckily for academics and PhD students, universities in general sympathise with us. A high performance computing (HPC) cluster is a collection of highly powerful computers located somewhere that can be accessed remotely, and used to run terminal scripts for coding.&lt;/p&gt;
&lt;p&gt;This portfolio will detail the use of &lt;strong&gt;Bluecrystal&lt;/strong&gt;, the super computers available at the University of Bristol. To access these computers, one needs to do so through the terminal, and so a basic knowledge of manipulating and using file structures in &lt;code&gt;bash&lt;/code&gt; is required. Section 4 details the fundamentals of &lt;code&gt;bash&lt;/code&gt; if the reader is not already familiar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bluecrystal-phase-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bluecrystal Phase 3&lt;/h2&gt;
&lt;p&gt;This tutorial will focus on &lt;em&gt;Bluecrystal Phase 3&lt;/em&gt;, that is the third generation of Bluecrystal machines. The cluster is made up of 312 compute &lt;em&gt;nodes&lt;/em&gt;, which are where the processes are run. The basic nodes have specifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;64GB RAM&lt;/li&gt;
&lt;li&gt;300TB storage&lt;/li&gt;
&lt;li&gt;16 Cores&lt;/li&gt;
&lt;li&gt;Infiniband High Speed Network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As well as the large memory nodes, which have 256GB of RAM, and the GPU nodes which each have an NVIDIA Tesla K20 - an extremely powerful GPU.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;connecting-to-the-hpc-cluster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Connecting to the HPC Cluster&lt;/h2&gt;
&lt;p&gt;To access the HPC cluster, you would need to log in via SSH (secure shell) from a University of Bristol connection (or a VPN). The &lt;code&gt;ssh&lt;/code&gt; command in bash is your friend here. To log in (assuming you have an account), you perform the following command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh your_username@bluecrystalp3.bris.ac.uk&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It will then ask for your password, which you supply immediately after writing this command. From here you will be in the &lt;em&gt;log-in node&lt;/em&gt;, note that you should &lt;strong&gt;not&lt;/strong&gt; run any code on the log-in node, as this node is only purposed for connecting to the compute nodes. If you run any large code on the log-in node you will slow down the HPC cluster for everyone else.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;file-system&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;File System&lt;/h2&gt;
&lt;p&gt;Within the log-in node, you will have your own personal file directory where you can store your files and your code. By default, after logging in you will be in this directory, so if you use the &lt;code&gt;ls&lt;/code&gt; command you will see the contents of your personal directory immediately. You can write code here, through the terminal, or copy it into your directory from your own personal computer. You are free to make directories and files here, as the contents of your directory will be read by the compute nodes when you want to run a job.&lt;/p&gt;
&lt;p&gt;To copy a file from your own computer to your file system in the HPC cluster, you can use the &lt;code&gt;scp&lt;/code&gt; command in &lt;code&gt;bash&lt;/code&gt;, a command that is run from your &lt;em&gt;own&lt;/em&gt; computer only and looks like&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;scp path_to_my_file/file.txt your_username@bluecrystalp3.bris.ac.uk::path_inside_hpc/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Performing this operation would copy &lt;code&gt;file.txt&lt;/code&gt; in folder &lt;code&gt;path_to_my_file&lt;/code&gt; into the &lt;code&gt;path_inside_hpc&lt;/code&gt; folder on your directory in the HPC cluster. If you want to do this the other way around, and copy something from your HPC cluster file system to your personal computer, just switch the order of the arguments to &lt;code&gt;scp&lt;/code&gt;, but always do it from your own machine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-jobs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running Jobs&lt;/h2&gt;
&lt;p&gt;To run a job, you must write a &lt;code&gt;bash&lt;/code&gt; script that tells the compute node what to do. This &lt;code&gt;bash&lt;/code&gt; script will be interpreted at the HPC node and run accordingly. Below is a general template for what this &lt;code&gt;bash&lt;/code&gt; script would look like to be passed across:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#!/bin/bash
#
#PBS -l nodes=2:ppn=1,walltime=24:00:00

# working directory
export WORK_DIR=$HOME
cd $WORK_DIR

# print to output
echo JOB ID: $PBS_JOBID
echo Working Directory `pwd`

# run something
/bin/hostname&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line &lt;code&gt;#!/bin/bash&lt;/code&gt; tells the compiler to read this as &lt;code&gt;bash&lt;/code&gt;, and the third line &lt;code&gt;#PBS -l nodes=2:ppn=1,walltime=24:00:00&lt;/code&gt; gives information to the HPC cluster as to what you want for the job. You can change these arguments to your suiting, e.g. increase the &lt;code&gt;walltime&lt;/code&gt; if you think your code will run for more than 24 hours.&lt;/p&gt;
&lt;p&gt;You must save this &lt;code&gt;bash&lt;/code&gt; script as something like &lt;code&gt;run_R.sh&lt;/code&gt;, and then when logged into Bluecrystal, use the command &lt;code&gt;qsub&lt;/code&gt; - meaning to submit this job to the queue, i.e.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;qsub run_R.sh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which would add this job to the queue. Since there are many people that use the HPC cluster, your job may not start immediately, and you might have to wait. You may have to wait longer if your &lt;code&gt;walltime&lt;/code&gt; is particularly high, as you will be waiting for enough nodes to become available.&lt;/p&gt;
&lt;div id=&#34;other-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Functions&lt;/h3&gt;
&lt;p&gt;As well as &lt;code&gt;qsub&lt;/code&gt;, there are other commands that you can use to play with the HPC cluster. Some notable ones are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;qstat&lt;/code&gt; gives a list of current jobs being run and those in the queue&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qstat -u user_name&lt;/code&gt; gives a list of current jobs queued and running by &lt;code&gt;user_name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qstat job_id&lt;/code&gt; gives information about the job &lt;code&gt;job_id&lt;/code&gt; being run&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qdel job_id&lt;/code&gt; deletes a job with a given &lt;code&gt;job_id&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-different-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running Different Code&lt;/h2&gt;
&lt;p&gt;To run other programming languages on the HPC cluster, it can be a bit of faff. To run code such as Python or R on the cluster, you must first load the &lt;em&gt;module&lt;/em&gt; associated with the particular language. On the log-in node, you can run&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;module avail&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to get a list of all available modules. There will be a lot. Choose one that you like, for example, I am a personal fan of &lt;code&gt;languages/R-3.6.2-gcc9.1.0&lt;/code&gt;, and you can load this with &lt;code&gt;module load module_name&lt;/code&gt;, for example&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;module load languages/R-3.6.2-gcc9.1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which will allow you to run R and R scripts. To submit a job that runs an R script, you must add this line to the job script before you run the code. To run an R script from &lt;code&gt;bash&lt;/code&gt;, you use&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Rscript script_name.R&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;using-r-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using R Packages&lt;/h3&gt;
&lt;p&gt;Since packages cannot be installed globally on the log-in node, you can install them locally instead. You first type the command &lt;code&gt;R&lt;/code&gt; into bash, and then &lt;code&gt;install.packages(&amp;quot;package_name&amp;quot;)&lt;/code&gt;. It will ask you if you want the package to be installed locally, which you say yes to.&lt;/p&gt;
&lt;p&gt;After this, all packages installed on your local file system on the log-in node will be accessible as normal when running job scripts.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
