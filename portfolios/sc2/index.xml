<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Computing 2 | Danny James Williams</title>
    <link>http://dannyjameswilliams.co.uk/portfolios/sc2/</link>
      <atom:link href="http://dannyjameswilliams.co.uk/portfolios/sc2/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistical Computing 2</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><lastBuildDate>Sun, 09 Sep 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://dannyjameswilliams.co.uk/images/icon_hu6de9a8f7dd4e8a8bd7c2613cf2ad59bf_37670_512x512_fill_lanczos_center_2.png</url>
      <title>Statistical Computing 2</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/</link>
    </image>
    
    <item>
      <title>Intro to C&#43;&#43;</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/intro/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/intro/</guid>
      <description>


&lt;div id=&#34;basic-c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic C++&lt;/h2&gt;
&lt;p&gt;C++ programs are written in ‘chunks’, and each chunk can be a function which contains some code, or a &lt;em&gt;main&lt;/em&gt; chunk, which is the program that is run when the code is compiled. Here is an example of a &lt;code&gt;main&lt;/code&gt; chunk that runs some code.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// include &amp;#39;iostream&amp;#39; package
#include &amp;lt;iostream&amp;gt;          

// int means this will output an integer, main() signifies the primary code to run
int main()                               
{
  // pipe operator, using std library, cout is to output something, endl is to end line
  std::cout &amp;lt;&amp;lt; &amp;quot;Hello&amp;quot; &amp;lt;&amp;lt; std::endl;    
  
  // return integer 0 to show all is okay
  return 0;                             
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This chunk of code will output the string &lt;code&gt;Hello&lt;/code&gt; when run, with comments (starting with &lt;code&gt;//&lt;/code&gt;) describing what each line does. However, this cannot be run by itself, since C++ is a &lt;em&gt;compiled&lt;/em&gt; programming language; so we need a compiler. Firstly, we save this file as &lt;code&gt;hello.cpp&lt;/code&gt;, and using the &lt;code&gt;g++&lt;/code&gt; compiler in the terminal will compile this code into an executable program.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ hello.cpp -o hello&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This starts with &lt;code&gt;g++&lt;/code&gt;, telling the terminal to use the &lt;code&gt;g++&lt;/code&gt; program, then chooses the file &lt;code&gt;hello.cpp&lt;/code&gt;, &lt;code&gt;-o hello&lt;/code&gt; specifies that the name of the output is &lt;code&gt;hello&lt;/code&gt;. This has compiled a program into the current working directory which can also be run in the terminal.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;./hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the code has run succesfully! The only output was &lt;code&gt;Hello&lt;/code&gt; as that was all that was specified to be output. Note that in C++ every integer, string, float, etc. needs to be defined in advance. Instead of in programming languages like R and Python, where nothing really needs to be specified in advance, variables need to be defined each time. For example,&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;int a = 3
float b = 14.512231
double c = 4e200
std::string d = &amp;quot;What up&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To further exemplify the usage of C++, consider the following example.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
int main()
{
  for (int i=1; i&amp;lt;=100; i++)
  {
    if (i&amp;gt;=95)
    {
      std::cout &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
    }
    else if (i&amp;lt;5)
    {
      for(float j=-1.5; j&amp;gt;=-3.5; j--)
      {
        std::cout &amp;lt;&amp;lt; i*j &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has used a range of different code, including &lt;code&gt;for&lt;/code&gt; loops and conditional statements &lt;code&gt;if&lt;/code&gt; and &lt;code&gt;else if&lt;/code&gt;. These are implemented in C++ similarly to the way they are implemented in R. In fact, the &lt;code&gt;if&lt;/code&gt; statements almost have the exact same formatting as R. Loops are a bit different. The syntax that C++ uses for basic loops are &lt;code&gt;for(initialise; condition; increment)&lt;/code&gt;, where &lt;code&gt;initialise&lt;/code&gt; refers to the first element that is being looped through, &lt;code&gt;condition&lt;/code&gt; is the stopping condition of the loop and &lt;code&gt;increment&lt;/code&gt; is how much the initaliser increases on each iteration.&lt;/p&gt;
&lt;p&gt;In this case, for the increment we have used &lt;code&gt;i++&lt;/code&gt; and &lt;code&gt;j--&lt;/code&gt;, which is shorthand for &lt;code&gt;i = i + 1&lt;/code&gt; and &lt;code&gt;j = j - 1&lt;/code&gt;. The stopping conditions are when &lt;code&gt;i&lt;/code&gt; is less than or equal to 100, and when &lt;code&gt;j&lt;/code&gt; is greater than or equal to -3.5. Each iteration of the loop checks two conditions, the value of &lt;code&gt;i&lt;/code&gt;, and different operations happen when &lt;code&gt;i&lt;/code&gt; is less than 5, or greater than or equal to 95. We can run this code to see the output.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ loopif.cpp -o loopif
./loopif&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -1.5 -2.5 -3.5 -3 -5 -7 -4.5 -7.5 -10.5 -6 -10 -14 95 96 97 98 99 100&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;times-table-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Times Table Example&lt;/h3&gt;
&lt;p&gt;Consider for a further example writing a function to print the times tables for a given number, a given number of times. For this, we can exemplify the use of &lt;em&gt;multi-file&lt;/em&gt; programs. By creating a header file, with extension &lt;code&gt;.h&lt;/code&gt;, and including this in our main program, we can specify functions in a different file. This helps navigate large code files, and separating different functions in different files can be extremely useful.&lt;/p&gt;
&lt;p&gt;Let’s start by creating a times table function in a &lt;code&gt;timestable.cpp&lt;/code&gt; file. This takes two integer inputs, the first being the times table we want to print, and the second being the maximum number we want to multiply to the first input.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;

void timestable(int a, int b)
{
  for(int i = 1 ; i &amp;lt;= b; i++)
  {
    std::cout &amp;lt;&amp;lt; a*i &amp;lt;&amp;lt; &amp;quot; &amp;quot;;
  }
  std::cout &amp;lt;&amp;lt; std::endl;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this loops from &lt;code&gt;i=1&lt;/code&gt; to &lt;code&gt;b&lt;/code&gt;, and multiplies &lt;code&gt;a&lt;/code&gt; by &lt;code&gt;i&lt;/code&gt; on each iteration. Now we create a header file, called &lt;code&gt;timestable.h&lt;/code&gt;, containing the following&lt;/p&gt;
&lt;pre class=&#34;h&#34;&gt;&lt;code&gt;#ifndef _TIMESTABLE_H
#define _TIMESTABLE_H

void timestable(int, int);
  
#endif &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This header file checks and defines the token &lt;code&gt;_TIMESTABLE_H&lt;/code&gt;, and then simply declares the function &lt;code&gt;timestable&lt;/code&gt;. When reading this function into another C++ file, it will declare the &lt;code&gt;timestable&lt;/code&gt; function that is defined in &lt;code&gt;timestable.cpp&lt;/code&gt;. A main file, called &lt;code&gt;main.cpp&lt;/code&gt; to read the header file, which will define the &lt;code&gt;timestable&lt;/code&gt; function, and run it for for two examples.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;quot;timestable.h&amp;quot;

int main()
{
  std::cout &amp;lt;&amp;lt; &amp;quot;Five times table&amp;quot; &amp;lt;&amp;lt; std::endl;
  timestable(5, 10);

  std::cout &amp;lt;&amp;lt; &amp;quot;Twelve times table&amp;quot; &amp;lt;&amp;lt; std::endl;
  timestable(12, 12);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we are expected the output of the five times table, up to 5 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 10, and the twelve times table, up to 12 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; 12. Let’s compile and run this program.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ main.cpp timestable.cpp -o timestable
./timestable&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Five times table
## 5 10 15 20 25 30 35 40 45 50 
## Twelve times table
## 12 24 36 48 60 72 84 96 108 120&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it has worked as expected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Integrating R and C</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/rnc/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/rnc/</guid>
      <description>


&lt;p&gt;R can be interfaced with both C and C++, coming with a significant speed up in computation time and efficiency. R has an inbuilt system for calling C with the &lt;code&gt;.Call&lt;/code&gt; function, and RCpp can be used to integrate more easily with C++, enabling the use of pre-existing functions similar to those in base R. This portfolio will detail how this integration between R and C is possible effectively through the use of examples in a statistical sense.&lt;/p&gt;
&lt;div id=&#34;adaptive-kernel-regression-smoothing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adaptive Kernel Regression Smoothing&lt;/h2&gt;
&lt;p&gt;In the situation where we have some data which can not fit into a conventional linear model, we can employ the use of kernel smoothing to fit a more flexible model. More specifically, suppose the data has been generated from the following model
&lt;span class=&#34;math display&#34;&gt;\[
y_i = \sin(\alpha \pi x^3) + \epsilon_i, \qquad \text{ with } \qquad \epsilon_i \sim N(0, \sigma^2),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a uniformly random sample, and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; are fixed parameters. This simulated data can be generated in R with parameter values &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 4\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma=0.2\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\dots, n\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(998)
n = 200
x = runif(n)
y = sin(4*pi*x^3) + rnorm(n, 0, 0.2)
plot(x, y, pch = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
From the plot it is clear that the data are non-linear, so a simple linear model would not be suitable. A kernel regression smoother can be used to estimate the conditional expectation &lt;span class=&#34;math inline&#34;&gt;\(\mu(x) = \mathbb{E}(y|x)\)&lt;/span&gt; more flexibly using
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}(x) = \frac{\sum^n_{i=1}\kappa_\lambda (x, x_i)y_i}{\sum^n_{i=1}\kappa_\lambda(x, x_i)},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\kappa\)&lt;/span&gt; is a kernel with bandwidth &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt;. This kernel regression can be written in R as well as C, so it provides opportunity for comparison between the two languages.&lt;/p&gt;
&lt;div id=&#34;writing-a-function-in-c&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Writing a function in C&lt;/h3&gt;
&lt;p&gt;We can write a C function to implement a Gaussian kernel with variance &lt;span class=&#34;math inline&#34;&gt;\(\lambda^2\)&lt;/span&gt;, shown below.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;#include &amp;lt;R.h&amp;gt;
#include &amp;lt;Rinternals.h&amp;gt;
#include &amp;lt;Rmath.h&amp;gt;

SEXP meanKRS(SEXP y, SEXP x, SEXP x0, SEXP lambda)
{
  
  int n, n0;
  double lambda0;
  double *outy, *x00, *xy, *y0;
  
  n0 = length(x0);
  
  x = PROTECT(coerceVector(x, REALSXP));
  y = PROTECT(coerceVector(y, REALSXP));
  SEXP out = PROTECT(allocVector(REALSXP, n0));
  
  n = length(x);
  
  outy = REAL(out);
  lambda0 = REAL(lambda)[0];
  x00 = REAL(x0);
  xy = REAL(x);
  y0 = REAL(y);
  
  for(int i=0; i&amp;lt;n0; i++)
  {
    double num = 0, den = 0;
    for(int j=0; j&amp;lt;n; j++)
    {
      num += dnorm(xy[j], x00[i], lambda0, 0)*y0[j];
      den += dnorm(xy[j], x00[i], lambda0, 0);
    }
    outy[i] = num/den;
  }
  
  UNPROTECT(3);
  
  return out;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a large, and quite complicated function to what would normally be implemented in R. The main reason for the length of the function is due to the amount of declarations that have to be made in C. At the start, all integers and doubles are declared, with a &lt;code&gt;*&lt;/code&gt; signifying that these are pointers, which instead point to a location in memory rather than being a variable themselves. &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; are all defined as vectors; either defining the inputs or allocating a new vector, with &lt;code&gt;PROTECT&lt;/code&gt; signifying that these locations in memory are protected. Then the pointers are set up with the &lt;code&gt;REAL&lt;/code&gt; function deciding where to point, and the &lt;code&gt;[0]&lt;/code&gt; index meaning to take the value of where is being pointed to instead of setting up a pointer.&lt;/p&gt;
&lt;p&gt;After this, the actual kernel smoothing is calculated within two &lt;code&gt;for&lt;/code&gt; loops, as there is no global definition for &lt;code&gt;sum&lt;/code&gt; in C. The numerator &lt;code&gt;num&lt;/code&gt; and denominator &lt;code&gt;den&lt;/code&gt; are defined on each iteration of the outer loop, and they are added to themselves in the inner loop. Once the inner loop iterations are complete, the value of &lt;code&gt;out&lt;/code&gt; is assigned as the division of these two (which is pointed to by &lt;code&gt;outy&lt;/code&gt;). Finally, the protected vectors are unprotected, to free up memory.&lt;/p&gt;
&lt;p&gt;The headers at the beginning of the file, written as &lt;code&gt;#include &amp;lt;header.h&amp;gt;&lt;/code&gt; allow the inclusion of certain pre-written functions that make writing C code simpler. Using the &lt;code&gt;R&lt;/code&gt;, &lt;code&gt;Rinternals&lt;/code&gt; and &lt;code&gt;Rmath&lt;/code&gt; were not all necessary, however &lt;code&gt;Rmath&lt;/code&gt; provided the &lt;code&gt;dnorm&lt;/code&gt; function that was necessary for the Gaussian kernel.&lt;/p&gt;
&lt;p&gt;Also note that the use of a double &lt;code&gt;for&lt;/code&gt; loop is not inefficient in C as it would be in R, so there is no significant slowdown coming from nested loops. Now the function is written, it can be saved into the current working directory and loaded into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(&amp;quot;R CMD SHLIB meanKRS.c&amp;quot;)
dyn.load(&amp;quot;meanKRS.so&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A wrapper function can be set up to clean up code, i.e. an R function can be made which explicitly calls the C function with the same arguments. After this, we can plot the results to see how the smoothing looks with an arbitrary value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda^2 = 0.02\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanKRS = function(y, x, x0, lambda) .Call(&amp;quot;meanKRS&amp;quot;, y, x, x0, lambda)
plot(x, y, pch = 20)
lines(seq(0, 1, len=1000), meanKRS(y, x, seq(0,1,len=1000), lambda = 0.02), col=&amp;quot;blue&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;528&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So this kernel smoothing approach has provided a nice fit to the simulated data, and appears to be going through the centre of the points across the plot.&lt;/p&gt;
&lt;p&gt;To compare the computational efficiency, we can compare this C function with a simpler function written in R, called &lt;code&gt;meanKRS_R&lt;/code&gt; (function not shown here for brevity, the R function performs the same operation whilst making use of R’s &lt;code&gt;sum&lt;/code&gt; function instead of having a loop).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(meanKRS_R(y,x,seq(0,1,len=1000),0.06), meanKRS(y,x,seq(0,1,len=1000),0.06))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the values are exactly the same, and the function is accurate. How much quicker is it? We can use &lt;code&gt;microbenchmark&lt;/code&gt; to display summary statistics of timing both functions 100 times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
microbenchmark(C_KRS = meanKRS(y,x,seq(0,1,len=1000),0.06),
               R_KRS = meanKRS_R(y,x,seq(0,1,len=1000),0.06), times = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##   expr      min       lq     mean   median       uq      max neval
##  C_KRS 14.32545 16.17882 16.36242 16.27690 16.42271 20.37999   100
##  R_KRS 30.25504 32.08685 33.45924 32.42864 34.38512 43.47946   100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the C code is significantly faster, as expected, by about a factor of two.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementing cross-validation&lt;/h3&gt;
&lt;p&gt;The value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; used here was chosen arbitrarily, but in practice it is common to use &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-fold cross-validation to choose a more optimal value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. We can implement a cross-validation routine in C. Implementing a function within a function is difficult in C, but it is relatively straightforward in Rcpp. This example goes through the use of Rcpp to implement cross-validation.&lt;/p&gt;
&lt;p&gt;Rcpp is a package in R which provides tools to implement C++ code in R. It works in a similar way to how the &lt;code&gt;.Call&lt;/code&gt; interface works for C code, but with a more accessible interface. Firstly, we create a file in the working directory called &lt;code&gt;KRS_cv.cpp&lt;/code&gt;, where the Rcpp functions will be stored. This file contains a few functions, the first of which being the &lt;code&gt;meanKRS&lt;/code&gt; function re-written for Rcpp.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;// [[Rcpp::export(name = &amp;quot;meanKRS&amp;quot;)]]
NumericVector meanKRS_I(const NumericVector y, const NumericVector x, 
                        const NumericVector x0, const double lambda)
{
  int n, n0;
  n0 = x0.size();
  n = x.size();
  NumericVector out(n0);
  for(int i=0; i&amp;lt;n0; i++)
  {
    double num = 0, den = 0;
    NumericVector dval = dnorm(x, x0[i], lambda, 1);
    double max_dval = max(dval);
    for(int j=0; j&amp;lt;n; j++)
    {
      num = num + exp(dval[j]-max_dval)*y[j];
      den = den + exp(dval[j]-max_dval);
    }
    out[i] = num/den;
  }
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The cross-validation function &lt;code&gt;cvKRS&lt;/code&gt; is written as:&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;// [[Rcpp::export(name = &amp;quot;cvKRS&amp;quot;)]]
NumericVector cvKRS_I(const NumericVector y, const NumericVector x, 
                      const int k, const NumericVector lambdas)
{
  // declare variables and vectors
  int n = y.size();
  NumericVector mse_lambda(lambdas.size());
  NumericVector out(1);
  NumericVector sorted_x(n);
  NumericVector sorted_y(n);
  
  // sort x and y according to the order of x
  IntegerVector order_x = stl_order(x);
  for(int kk=0; kk &amp;lt; n; kk++)
  {
    int ind = order_x[kk]-1;
    sorted_y[kk] = y[ind];  
    sorted_x[kk] = x[ind];
  }
  
  // set up indices to cross-validate for
  IntegerVector idxs = seq_len(k);
  IntegerVector all_idxs = rep_each(idxs, n/k);

  // different lambdas
  for(int jj = 0; jj &amp;lt; lambdas.size(); jj++)
  {
    double lambda = lambdas[jj];
    NumericVector mse(k);
    
    // cross-validation loop
    for(int ii=1; ii &amp;lt;= k; ii++)
    {
      const LogicalVector kvals = all_idxs != ii;
      
      NumericVector y_t = clone(sorted_y);
      NumericVector x_t = clone(sorted_x);
      NumericVector y_cross = y_t[kvals];
      NumericVector x_cross = x_t[kvals];
      NumericVector fit = meanKRS_I(y_cross, x_cross, sorted_x, lambda);
      
      // calculate mean squared error
      NumericVector error = pow((fit[!kvals] - sorted_y[!kvals]), 2);
      mse[ii-1] = mean(error);
    }
    
    // average mean squared error for each value of lambda
    mse_lambda[jj] = mean(mse);
  }
  
  // output lambda which gave the smallest mean squared error
  int best_pos = which_min(mse_lambda);
  out[0] = lambdas[best_pos];
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comments within the function (succeeding a &lt;code&gt;//&lt;/code&gt;) give explanation of each section of the function. This function also calls the other function &lt;code&gt;meanKRS&lt;/code&gt; by using &lt;code&gt;meanKRS_I&lt;/code&gt;. The function was defined as &lt;code&gt;meanKRS_I&lt;/code&gt; in the &lt;code&gt;.cpp&lt;/code&gt; file, but when exported into R it is defined as &lt;code&gt;meanKRS&lt;/code&gt;, based on the comment preceeding the function definition. Both functions were defined within the same file, and so can call one another, provided the function being called is defined first. There is a function used within &lt;code&gt;cvKRS&lt;/code&gt; that is not part of base Rcpp functionality: &lt;code&gt;stl_order&lt;/code&gt;. This does the same thing as &lt;code&gt;order&lt;/code&gt; in base R, but needed to be defined separately.&lt;/p&gt;
&lt;p&gt;Now that the function is defined, we can call it within R by first loading the &lt;code&gt;Rcpp&lt;/code&gt; library and then using the &lt;code&gt;sourceCpp&lt;/code&gt; function to source the C++ file, located in the same working directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
sourceCpp(&amp;quot;KRS_cv.cpp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the stage where we would get compilation errors, if there were any. Now we can run the function over a series of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values and plot the kernel regression over the simulated data for the optimal &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; selected by cross-validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda_seq = exp(seq(log(1e-6),log(100),len=50))
best_lambda = cvKRS(y, x, k = 20, lambdas = lambda_seq)
plot(x, y, pch = 20)
lines(seq(0, 1, len=1000), meanKRS(y, x, seq(0,1,len=1000), best_lambda), 
      col=&amp;quot;deeppink&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
To compare computational efficiency, we can benchmark speeds against a function written in R (not shown here).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(cvKRS(y, x, k = 20, lambdas = lambda_seq),
               cvKRS_R(y, x, k = 20, lambdas = lambda_seq), 
               times = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: seconds
##                                         expr      min       lq     mean
##    cvKRS(y, x, k = 20, lambdas = lambda_seq) 2.289333 2.293236 2.660317
##  cvKRS_R(y, x, k = 20, lambdas = lambda_seq) 6.390262 6.629278 7.100683
##    median       uq      max neval
##  2.380029 3.141275 3.197711     5
##  7.481269 7.486666 7.515939     5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this further exemplifies the significant speed increase that comes with writing in C, or Rcpp. In fact, this function had an average of three times the speed of that of the R function, against the two times speed up the previous function had. This is likely due to using two functions written in C++ (as cross validation calls the original kernel regression function).&lt;/p&gt;
&lt;p&gt;This has improved the fit, but no single value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; leads to a satisfactory fit, due to the first half of the function (for &lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; 0.5\)&lt;/span&gt;) wanting a smooth &lt;span class=&#34;math inline&#34;&gt;\(\mu(x)\)&lt;/span&gt; as it is quite linear, and the second half wanting a less smooth one, as it is more ‘wiggly’. We can let the smoothness depend on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by constructing &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt;, which will improve the fit. This method is based around modelling the residuals and varying &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; more when the residuals are larger. Thus in practice &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt; is a sequence of values instead of a single value. Below are the contents of the file that contains the functions to implement this written in Rcpp.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

NumericVector fitKRS(const NumericVector x, const NumericVector x0, 
                     const double lambda, const NumericVector y, 
                     const NumericVector lambda_vec)
{
  NumericVector copy_y = clone(y);
  int n = x.size();
  int n0 = x0.size();
  NumericVector out(n0);
  for(int ii=0; ii&amp;lt;n0; ii++)
  {
    NumericVector dval = dnorm(x, x0[ii], lambda*lambda_vec[ii], 1);
    double max_dval = max(dval);
    double num=0, den=0;
    for(int jj=0; jj&amp;lt;n; jj++)
    {
      num = num + exp(dval[jj] - max_dval)*copy_y[jj];
      den = den + exp(dval[jj] - max_dval);
    }
    out[ii] = num/den;
  }
  return out;
}

// [[Rcpp::export(name = &amp;quot;mean_var_KRS&amp;quot;)]]
NumericVector mean_var_KRS_I(const NumericVector y, const NumericVector x, 
                             const NumericVector x0, const double lambda)
{
  int n = x.size();
  int n0 = x0.size();
  NumericVector res(n);
  NumericVector lambda_1sn(n, 1.0);
  NumericVector lambda_1sn0(n0, 1.0);
  NumericVector mu = fitKRS(x, x, lambda, y, lambda_1sn);
  NumericVector resAbs = abs(y - mu);
  NumericVector madHat = fitKRS(x, x0, lambda, resAbs, lambda_1sn0);
  NumericVector w = 1 / madHat;
  w = w / mean(w);
  NumericVector out = fitKRS(x, x0, lambda, y, w);

  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first function, &lt;code&gt;fitKRS&lt;/code&gt; was used to save space, since the same operation is performed multiple times with different parameters. Different weightings &lt;code&gt;w&lt;/code&gt; get added to the vector of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; values in the final stage, resulting in the varied &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt; parameter. We can plot this to show this change, using the initial value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; selected by cross-validation earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;quot;meanvarKRS.cpp&amp;quot;)
varied_mu = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = best_lambda)
plot(x, y, pch=20)
lines(seq(0, 1, len=1000), varied_mu, col = &amp;quot;darkgoldenrod&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
This looks like a good fit, and the function is working as intended. How well does the speed of the function written in Rcpp compare to one written in R?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(C = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), 
                                lambda = best_lambda),
               R = mean_var_KRS_R(y = y, x = x, x0 = seq(0,1,len=1000), 
                                  lam = best_lambda), times = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##  expr      min       lq     mean   median       uq       max neval
##     C 25.56793 35.80579 41.67632 39.84551 45.09670  84.63745   500
##     R 35.03011 54.59121 64.87276 61.94020 70.77707 122.68250   500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has an expected speed up again. And to make sure both functions are outputting the same thing, we can use &lt;code&gt;all.equal&lt;/code&gt; again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), 
                              lambda = best_lambda), 
          mean_var_KRS_R(y = y, x = x, x0 = seq(0,1,len=1000), 
                             lam = best_lambda))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation-again&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cross Validation Again&lt;/h3&gt;
&lt;p&gt;The value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; used for fitting this local regression is that picked from cross-validation where &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; does not vary with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, in the previous section. To improve this fit further, we can use cross-validation again, but fitting the model with the new &lt;code&gt;mean_var_KRS&lt;/code&gt; function on each iteration instead of the basic kernel regression. The function is written in Rcpp below.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;// [[Rcpp::export(name = &amp;quot;mean_var_cv_KRS&amp;quot;)]]
NumericVector mean_var_cv_KRS_I(const NumericVector y, const NumericVector x, 
                      const int k, const NumericVector lambdas)
{
  int n = y.size();
  NumericVector mse_lambda(lambdas.size());
  NumericVector out(1);
  NumericVector sorted_x(n);
  IntegerVector order_x = stl_order(x);
  NumericVector test;
  NumericVector sorted_y(n);
  for(int kk=0; kk &amp;lt; n; kk++)
  {
    int ind = order_x[kk]-1;
    sorted_y[kk] = y[ind];  
    sorted_x[kk] = x[ind];
  }
  IntegerVector idxs = seq_len(k);
  IntegerVector all_idxs = rep_each(idxs, n/k);
  
  for(int jj = 0; jj &amp;lt; lambdas.size(); jj++)
  {
    double lambda = lambdas[jj];
    NumericVector mse(k);
    for(int ii=1; ii &amp;lt;= k; ii++)
    {
      const LogicalVector kvals = all_idxs != ii;
      NumericVector y_t = clone(sorted_y);
      NumericVector x_t = clone(sorted_x);
      NumericVector y_cross = y_t[kvals];
      NumericVector x_cross = x_t[kvals];
      NumericVector fit = mean_var_KRS_I(y_cross, x_cross, sorted_x, lambda);
      NumericVector error = pow((fit[!kvals] - sorted_y[!kvals]), 2);
      mse[ii-1] = mean(error);
    }
    mse_lambda[jj] = mean(mse);
  }
  int best_pos = which_min(mse_lambda);
  out[0] = lambdas[best_pos];
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This functions is very similar to the cross-validation function implemented earlier. It loops over different values of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, but instead uses these as a starting point to fit a series of points using &lt;span class=&#34;math inline&#34;&gt;\(\lambda(x)\)&lt;/span&gt;. The error is calculated against the cross-validated points and the starting &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; that results in the smallest error is returned. Now we can call this in R, and re-run the &lt;code&gt;mean_var_KRS&lt;/code&gt; function with the newly selected &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and see how it compares.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;quot;meanvarcvKRS.cpp&amp;quot;)
cv_best_lambda = mean_var_cv_KRS(y, x, k=20, exp(seq(log(0.01), log(1), len=50)))
cv_varied_mu = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = cv_best_lambda)
plot(x, y, pch=20)
lines(seq(0, 1, len=1000), cv_varied_mu, col = &amp;quot;darkorchid&amp;quot;, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/portfolios/sc2/rnc_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
This looks a lot smoother for lower values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; than before, which looks like a better fit overall!&lt;/p&gt;
&lt;p&gt;A way to improve this model might involve a &lt;em&gt;local&lt;/em&gt; regression approach, which would be fitting parameter values at different (possibly uniform) intervals across the data set. Local regression will be covered in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Local Polynomial Regression with Rcpp</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/rcpp/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/rcpp/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Rcpp &lt;em&gt;sugar&lt;/em&gt; and &lt;em&gt;Armadillo&lt;/em&gt; are libraries included in Rcpp that allow different processes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Sugar&lt;/em&gt; provides an array of basic functions in Rcpp that are similar to inbuilt base R functions. These include functions such as &lt;code&gt;cbind&lt;/code&gt;, &lt;code&gt;sum&lt;/code&gt;, &lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;sample&lt;/code&gt; and many more. These are essential for writing simple code in Rcpp.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Armadillo&lt;/em&gt; is a package used for linear algebra operations in Rcpp. It allows specification of matrices, 3D matrices, vectors and others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This portfolio will explain the use of key functions and features of &lt;em&gt;Armadillo&lt;/em&gt; and &lt;em&gt;sugar&lt;/em&gt; by implementing local polynomial regression on a data set on solar electricity production in Sydney. We can load this data into the R environment first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;solarAU.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(solarAU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       prod          toy tod   logprod
## 8832 0.019 0.000000e+00   0 -3.540459
## 8833 0.032 5.708088e-05   1 -3.170086
## 8834 0.020 1.141618e-04   2 -3.506558
## 8835 0.038 1.712427e-04   3 -3.036554
## 8836 0.036 2.283235e-04   4 -3.079114
## 8837 0.012 2.854044e-04   5 -3.816713&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column &lt;code&gt;prod&lt;/code&gt; is a production measure, and &lt;code&gt;logprod&lt;/code&gt; is the log of the production measure, which will be the response variable. The covariates are &lt;code&gt;toy&lt;/code&gt; - the time of year, within 0 and 1 as a percentage, and &lt;code&gt;tod&lt;/code&gt; - the time of day, from 0 to 47, measured in half an hour intervals. A simple polynomial regression model is of the form
&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}(y|\bm{x}) = \beta_0 + \beta_1\text{tod} + \beta_2\text{tod}^2 + \beta_3\text{toy} + \beta_4 \text{toy}^2 = \tilde{\bm{x}}\bm{\beta}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-a-linear-regression-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a linear regression model&lt;/h1&gt;
&lt;p&gt;We can use &lt;em&gt;Armadillo&lt;/em&gt; to fit a linear regression model, and to solve the least squares optimisation problem
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\bm{\beta}} := \argmin_{\bm{\beta}}\norm{\bm{y}-\bm{X}\bm{\beta}}^2,
\]&lt;/span&gt;
which has solution
&lt;span class=&#34;math display&#34;&gt;\[
\bm{\hat{\beta}} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}.
\]&lt;/span&gt;
This can be implemented in C using QR decomposition of &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
sourceCpp(code=&amp;#39;
// [[Rcpp::depends(RcppArmadillo)]]
#include &amp;lt;RcppArmadillo.h&amp;gt;
#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp::export(name=&amp;quot;lm_cpp&amp;quot;)]]
arma::vec lm_cpp_I(arma::vec&amp;amp; y, arma::mat&amp;amp; X)
{
  arma::mat Q, R;
  arma::qr_econ(Q, R, X);
  arma::vec beta = solve(R, (trans(Q) * y));
  return beta;
}&amp;#39;)
ls = function(formula, data){
  y = data[,all.vars(formula)[1]]
  x = model.matrix(formula, data)
  lm_cpp(y, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;sourceCpp&lt;/code&gt; function has a differing argument &lt;code&gt;code&lt;/code&gt;, where instead of reading code from a file in the directory, the code is supplied directly here. Running &lt;code&gt;sourceCpp&lt;/code&gt; adds the &lt;code&gt;lm_cpp&lt;/code&gt; function to the environment. An R function is set up which takes the inputs &lt;code&gt;formula&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt; and runs the Rcpp function with the given inputs. This makes it comparable to &lt;code&gt;lm&lt;/code&gt;. Firstly though, we can see that both functions output the same parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]
## [1,] -6.26275685
## [2,]  0.86440391
## [3,] -0.01757599
## [4,] -5.91806924
## [5,]  6.14298863&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)         tod    I(tod^2)         toy    I(toy^2) 
## -6.26275685  0.86440391 -0.01757599 -5.91806924  6.14298863&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But which is faster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(
  R_lm = lm(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU),
  C_lm = ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU),
  times = 500
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##  expr      min       lq     mean   median       uq      max neval
##  R_lm 3.531021 4.047208 5.079068 4.205777 5.222140 57.54048   500
##  C_lm 2.522135 2.927327 3.615145 3.026044 3.654492 37.26725   500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the C code is approximately twice as fast with this method, and would be even faster without the R wrapper function. However, the R function &lt;code&gt;lm&lt;/code&gt; performs a number of checks and computes a number of statistics that the C++ code does not, which explains part of the performance gap.&lt;/p&gt;
&lt;p&gt;For a more fair comparison, we can set up the model matrices in advance, and perform the same operations in R and in RCpp.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R_ls_solve = function(y,X){
  QR = qr(X)
  beta = solve(qr.R(QR), (t(qr.Q(QR)) %*% y))
  return(beta)
}
y = solarAU$logprod
X = with(solarAU, cbind(1, tod, tod^2, toy, toy^2))
microbenchmark(R_solve = R_ls_solve(y,X),
               C_solve = lm_cpp(y, X), 
               times = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: microseconds
##     expr      min       lq      mean   median       uq       max neval
##  R_solve 1942.200 2176.820 4659.2470 2511.320 4010.905 64206.810   500
##  C_solve  440.409  510.482  688.3188  573.852  689.738  3547.974   500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this has come with an approximate speed up of ten times, exemplifying how much more computationally efficient code written in C++ is over R. However, there is a (non-computational) problem with the model. A plot of the residuals from the linear model shows this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta = ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU)
res = y - X %*% beta
predplot = ggplot(solarAU, mapping = aes(x=toy, y=tod, z= X%*%beta)) +
  stat_summary_hex() +  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) +
  theme(legend.title = element_blank())
resplot = ggplot(solarAU, mapping=aes(x=toy, y = tod, z = res)) +
  stat_summary_hex() +  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) +
  theme(legend.title = element_blank())
grid.arrange(predplot, resplot, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/portfolios/sc2/rcpp_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
There is a pattern in the residuals! This means that there is a feature not included in the model that can explain some of the noise. We need to extend this model to account for this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;local-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Local Regression&lt;/h1&gt;
&lt;p&gt;We can improve the model fit by adopting a local regression approach, that is, making the parameter estimates depend on the covariates &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\hat{\bm{\beta}} = \hat{\bm{\beta}}(\bm{x})\)&lt;/span&gt;. This is achieved by minimising &lt;span class=&#34;math inline&#34;&gt;\(\hat{\bm{\beta}}(\bm{x}_0)\)&lt;/span&gt; for a fixed &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}_0\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\bm{\beta}}(\bm{x}_0) = \argmin_{\bm{\beta}} \sum^n_{i=1}\kappa_{\bm{H}}(\bm{x}_0 - \bm{x}_i)(y_i-\tilde{\bm{x}}_i^T\bm{\beta})^2,
\]&lt;/span&gt;
for a density kernel &lt;span class=&#34;math inline&#34;&gt;\(\kappa_{\bm{H}}\)&lt;/span&gt; with positive definite bandwidth matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{H}\)&lt;/span&gt;. Fitting this model involves re-fitting the linear model once for each row in the data set, which for large data sets is not viable, but shows the need of computationally efficient code in C++. Now we can write the local regression function in RCpp.&lt;/p&gt;
&lt;pre class=&#34;c&#34;&gt;&lt;code&gt;vec local_lm_I(vec&amp;amp; y, rowvec x0, rowvec X0, mat&amp;amp; x, mat&amp;amp; X, mat&amp;amp; H)
{
  mat Hstar = chol(H, &amp;quot;lower&amp;quot;); 
  vec w = dmvnInt(x, x0, Hstar);
  vec beta = lm_cpp_I(y % sqrt(w), X.each_col() % sqrt(w));
  return X0 * beta;
}

// [[Rcpp::export(name=&amp;quot;local_lm_fit&amp;quot;)]]
vec local_lm_fit_I(vec&amp;amp; y, mat x0, mat X0, mat&amp;amp; x, mat&amp;amp; X, mat&amp;amp; H)
{
  int n0 = x0.n_rows;
  vec out(n0);
  for(int ii=0; ii &amp;lt; n0; ii++)
  {
    rowvec x00 = x0.row(ii);
    rowvec X00 = X0.row(ii);
    out(ii) = as_scalar(local_lm_I(y, x00, X00, x, X, H));
    if(ii % 50 == 0) {R_CheckUserInterrupt();}
  }
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These two functions, as well as &lt;code&gt;lm_cpp&lt;/code&gt; from earlier all combine to implement local regression. &lt;code&gt;local_lm_fit_I&lt;/code&gt; loops over all rows in &lt;code&gt;x0&lt;/code&gt; and &lt;code&gt;X0&lt;/code&gt; and fits linear regression for a constant &lt;span class=&#34;math inline&#34;&gt;\(\bm{x}_0\)&lt;/span&gt;, using weights from a Gaussian kernel, imeplemented by a multivariate normal density (given by &lt;code&gt;dmvnInt&lt;/code&gt;, defined separately). &lt;code&gt;local_lm_I&lt;/code&gt; simply calculates the weights and pre-multiplies them by &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}\)&lt;/span&gt; to go into the fitting function. We can source these functions and run these for a subsetted data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;#39;lm_cpp.cpp&amp;#39;)
nsub = 2000

sub = sample(1:nrow(solarAU), nsub)
y = solarAU$logprod
x = as.matrix(solarAU[c(&amp;quot;tod&amp;quot;, &amp;quot;toy&amp;quot;)])
X = model.matrix(~tod+toy+I(tod^2)+I(toy^2),data=solarAU)
x0 = x[sub, ]
X0 = X[sub, ]
H = diag(c(1,0.01))

cpp_pred_local = local_lm_fit(y, x0, X0, x, X, H)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of which we can plot, for both the predictions and the residuals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predPlot = ggplot(mapping=aes(x=x0[,2], y = x0[,1], z = cpp_pred_local)) + stat_summary_hex() +
  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) + theme(legend.title = element_blank())
resPlot = ggplot(mapping=aes(x=x0[,2], y = x0[,1], z = y[sub] - cpp_pred_local)) + stat_summary_hex() +
  xlab(&amp;quot;Time of Year&amp;quot;) + ylab(&amp;quot;Time of Day&amp;quot;) + theme(legend.title = element_blank())
grid.arrange(predPlot, resPlot, ncol=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/portfolios/sc2/rcpp_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
These look a lot better! There isn’t a systematic pattern in these residuals which show that the model isn’t missing anything important. Now we can check this C++ code against the basic R code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm)
lmLocal &amp;lt;- function(y, x0, X0, x, X, H){
  w &amp;lt;- dmvnorm(x, x0, H)
  fit &amp;lt;- lm(y ~ -1 + X, weights = w)
  return( t(X0) %*% coef(fit) )
}
R_pred_local &amp;lt;- sapply(1:nsub, function(ii){
  lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H = diag(c(1, 0.1)^2))
})

all.equal(R_pred_local, as.vector(cpp_pred_local))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since all these predictions are equal in R and Rcpp, how does the speed difference compare? This function takes a long time to run, so we will only time each of them once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(sapply(1:nsub, function(ii){
  lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H = diag(c(1, 0.1)^2))
}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##  23.527   0.076  23.606&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(
  local_lm_fit(y, x0, X0, x, X, H)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
## 872.113   0.309 145.901&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the Rcpp code has come with an approximate ten times speed up again. However, this model could still be improved. We have chosen the bandwidth matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{H}\)&lt;/span&gt; arbitrarily, but this could be improved with cross-validation. Whilst this is not shown here, a similar approach to that given in section 2 could be implemented.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Intro to OpenMP in C&#43;&#43;</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/openmp/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/openmp/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;‘Normal’ programming can deal with performing operations one at a time, i.e. writing code in such a way that it can be seen as a set of instructions to be performed sequentially. A more efficient process will be maximising the use of the cores in your processor so that multiple operations are performed at once, with different processes happening on different cores.&lt;/p&gt;
&lt;p&gt;OpenMP is a way of performing parallel computation for C, C++ and Fortran, but this portfolio will go over the use of OpenMP in C++.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;A simple C++ script which uses OpenMP for parallel computing will look like this&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;

#ifdef _OPENMP
    #include &amp;lt;omp.h&amp;gt;
#else
    #define omp_get_num_threads() 0
    #define omp_get_thread_num() 0
#endif

int main(int argc, const char **argv)
{
    std::cout &amp;lt;&amp;lt; &amp;quot;Hello I am here safe and sound home in the main thread.\n&amp;quot;;

    #pragma omp parallel
    {
        int nthreads = omp_get_num_threads();
        int thread_id = omp_get_thread_num();

        std::cout &amp;lt;&amp;lt; &amp;quot;Help I am trapped in thread number &amp;quot; &amp;lt;&amp;lt; thread_id
                  &amp;lt;&amp;lt; &amp;quot; out of a total &amp;quot; &amp;lt;&amp;lt; nthreads 
                  &amp;lt;&amp;lt; std::endl;
    }

    std::cout &amp;lt;&amp;lt; &amp;quot;Thank god I&amp;#39;m safe back home now.\n&amp;quot;;

    return 0;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important parts here are the &lt;code&gt;ifdef _OPENMP&lt;/code&gt; section at the start, and the &lt;code&gt;#pragma omp parallel&lt;/code&gt; line before the process of the script. The comments succeeding the hash symbol can be seen as a ‘hint’ to the compiler of what to do. The compiler is free to ignore this if need be.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;if_def _OPENMP&lt;/code&gt; line checks if we are using OpenMP or not, and if so, includes the &lt;code&gt;omp.h&lt;/code&gt; header file. The &lt;code&gt;pragma omp parallel&lt;/code&gt; line tells the compiler to run the section enclosed in braces &lt;code&gt;{}&lt;/code&gt; a certain amount of times, depending on the input number of threads.&lt;/p&gt;
&lt;p&gt;We can compile this code (which is saved in &lt;code&gt;basic_openmp.cpp&lt;/code&gt;) to see what happens.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp basic_openmp.cpp -o basic_openmp
export OMP_NUM_THREADS=1
./basic_openmp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello I am here safe and sound home in the main thread.
## Help I am trapped in thread number 0 out of a total 1
## Thank god I&amp;#39;m safe back home now.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp basic_openmp.cpp -o basic_openmp
export OMP_NUM_THREADS=8
./basic_openmp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello I am here safe and sound home in the main thread.
## Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number 462 out of a total 8 out of a total  out of a total 8
## 
## 8
## 3 out of a total 8
## Help I am trapped in thread number 0 out of a total 8
## Help I am trapped in thread number 5 out of a total 8
## 7 out of a total 8
## Help I am trapped in thread number 1 out of a total 8
## Thank god I&amp;#39;m safe back home now.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This person really got trapped in a time loop. This is because the threads do not run sequentially, so each thread is printing out what it is asked as soon as it can, and these commands are being run at the same time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sections-and-loops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sections and Loops&lt;/h2&gt;
&lt;p&gt;OpenMP sections are ways of telling the compiler that each section can be operated on different threads. This is done by adding the line &lt;code&gt;#pragma omp section&lt;/code&gt; and braces &lt;code&gt;{}&lt;/code&gt; to each section that can be operated on individually, but only once. This is a way of breaking your code into parallel ‘chunks’ without executing the same code a lot of times by each thread.&lt;/p&gt;
&lt;p&gt;A loop can be run in parallel by writing the line &lt;code&gt;#pragma omp for&lt;/code&gt; above the loop. This specifies that each iteration in the loop is independent and can be run separately. Then each thread runs on a different iteration of the loop at the same time. Below is an example of a loop running in parallel.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;

#ifdef _OPENMP
    #include &amp;lt;omp.h&amp;gt;
#else
    #define omp_get_thread_num() 0
#endif

int main(int argc, const char **argv)
{
    #pragma omp parallel
    {
        int nloops = 0;
        
        #pragma omp for
        for (int i=0; i&amp;lt;1000; ++i)
        {
            ++nloops;
        }

        int thread_id = omp_get_thread_num();
        #pragma omp critical
        {
          std::cout &amp;lt;&amp;lt; &amp;quot;Thread &amp;quot; &amp;lt;&amp;lt; thread_id &amp;lt;&amp;lt; &amp;quot; performed &amp;quot;
                    &amp;lt;&amp;lt; nloops &amp;lt;&amp;lt; &amp;quot; iterations of the loop.\n&amp;quot;;
        }
    }

    return 0;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code will display how many iterations of the loop each thread is performing. Note that the &lt;code&gt;#pragma omp critical&lt;/code&gt; line is specifying that only one thread can enter the code in braces &lt;code&gt;{}&lt;/code&gt; at a time, just so that the printed output does not get jumbled up like it has done previously. In most code, this line will not be added as it will come with a significant slow down, I have only included it here for illustration purposes. We can compile and then run this code.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp loop_openmp.cpp -o loop_openmp
export OMP_NUM_THREADS=4
./loop_openmp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Thread 0 performed 250 iterations of the loop.
## Thread 1 performed 250 iterations of the loop.
## Thread 2 performed 250 iterations of the loop.
## Thread 3 performed 250 iterations of the loop.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So these threads have split the job evenly, but in some cases (maybe for a large amount of threads) the job would not be split evenly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo Exercise&lt;/h2&gt;
&lt;p&gt;For an exercise, consider using an OpenMP parallel program to calculate &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; using a Monte Carlo algorithm. We can do this by simulating a unit circle and a unit square, since &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is the ratio of the area of a circle to the area of a square. By simulating random points for a circle and for a square, provided we have a large enough number of simulations we can estimate this proportion and hence &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;cmath&amp;gt;
#include &amp;lt;cstdlib&amp;gt;
#include &amp;lt;iostream&amp;gt;

#ifdef _OPENMP
    #include &amp;lt;omp.h&amp;gt;
#else
    #define omp_get_thread_num() 0
#endif

double rand_one()
{
    return std::rand() / (RAND_MAX + 1.0);
}

int main()
{
    
    // declare variables
    int circle_points = 0;
    int square_points = 0;
    
    int circle_points_loop = 0;
    int square_points_loop = 0;

    // set up parallel OpenMP
    #pragma omp parallel
    {   

        // run for loop in parallel
        #pragma omp for
        for(int ii=0; ii &amp;lt; 100000; ii++)
        {

            // get random x and y coordinates
            double x_coord = (2*rand_one() - 1);
            double y_coord = (2*rand_one() - 1);

            // calculate radius
            double r = std::sqrt(pow(x_coord,2) + pow(y_coord,2));

            // if r is less than or equal to 1 then it is within the circle
            if(r &amp;lt; 1.0)
            {
                ++circle_points_loop;
            } else 
            {
                ++square_points_loop;
            }
        
        }

        // use critical when counting the final number of counts for each thread
        #pragma omp critical
        {
            circle_points += circle_points_loop;
            square_points += square_points_loop;
        }
    }

    // calculate final value of pi using ratios
    double pi = (4.0*circle_points)/(square_points+circle_points);
    
    // print pi
    std::cout &amp;lt;&amp;lt; pi &amp;lt;&amp;lt; std::endl;
    return 0;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The comments in the code above will explain why each section of code is run at each point in time. To check this result is valid, we can compile and run it.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;g++ -fopenmp pi.cpp -o pi
export OMP_NUM_THREADS=8
./pi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3.13372&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is not a bad approximation!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ParallelRcpp</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/parallelrcpp/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/parallelrcpp/</guid>
      <description>


&lt;p&gt;There are two primary methods for parallelisation in &lt;code&gt;Rcpp&lt;/code&gt;, the first being &lt;code&gt;OpenMP&lt;/code&gt; and the second being the &lt;code&gt;RcppParallel&lt;/code&gt; package for &lt;code&gt;Rcpp&lt;/code&gt;. The &lt;code&gt;RcppParallel&lt;/code&gt; package builds on existing methods and uses &lt;code&gt;OpenMP&lt;/code&gt;, but provides neat functionality and simple usage, and so will be focused on in this portfolio.&lt;/p&gt;
&lt;p&gt;The reader must have a working knowledge of C++ and &lt;code&gt;Rcpp&lt;/code&gt;. To begin, each &lt;code&gt;Rcpp&lt;/code&gt; file must include&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;// [[Rcpp::depends(RcppParallel)]]
#include &amp;lt;RcppParallel.h&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in the header. This ensures that the settings for &lt;code&gt;RcppParallel&lt;/code&gt; are automatically included in the compilation of the C++ code.&lt;/p&gt;
&lt;p&gt;Regular parallel approaches to Cpp can cause crashes due to the single-threaded nature of R. This is due to multiple threads attempting to access and interact with the same data structure. &lt;code&gt;RcppParallel&lt;/code&gt; provides a straightforward way to account for this.&lt;/p&gt;
&lt;div id=&#34;basic-parallel-operations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basic Parallel Operations&lt;/h3&gt;
&lt;p&gt;There are two functions inbuilt which can provide the bulk of the parallel operations; &lt;code&gt;parallelFor&lt;/code&gt; and &lt;code&gt;parallelReduce&lt;/code&gt;. These are interfaces to a parallel for loop and reduce function (see &lt;code&gt;?Reduce&lt;/code&gt; in R).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RcppParallel&lt;/code&gt; also provides two accessor classes, &lt;code&gt;RVector&lt;/code&gt; and &lt;code&gt;RMatrix&lt;/code&gt;, which are thread-safe accessors for an Rcpp vector and matrix, helping to deal with the problem of accessing the same data structure across multiple threads.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-matrix-transformations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Matrix Transformations&lt;/h2&gt;
&lt;p&gt;Consider taking the log of every element in a large matrix. In R, this process is simple, since &lt;code&gt;log&lt;/code&gt; is a vectorised function, we can just run&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(A)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;A&lt;/code&gt; is a matrix. This would also be easy to implement in &lt;code&gt;Rcpp&lt;/code&gt; using the &lt;code&gt;std::transform&lt;/code&gt; operator:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
using namespace Rcpp;

// [[Rcpp:export]]
NumericMatrix MatrixLog(NumericMatrix A)
{
  int n = A.nrow();
  int d = A.ncol();
  NumericMatrix out(n, d);
  std::transform(A.begin(), A.end(), out.begin(), ::log);
  return(out);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;std::transform&lt;/code&gt; function applies a given function across a range of values, here these are specified as all the elements in the matrix &lt;code&gt;A&lt;/code&gt;, where the starting and ending point are supplied by &lt;code&gt;A.begin()&lt;/code&gt; and &lt;code&gt;A.end()&lt;/code&gt;. These transformed values are saved in &lt;code&gt;out&lt;/code&gt;, starting at &lt;code&gt;out.begin()&lt;/code&gt;. We can compare the speed of this function to an R implementation by applying both the base R &lt;code&gt;log&lt;/code&gt; function and &lt;code&gt;MatrixLog&lt;/code&gt; to a large matrix (and check that they give the same result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
sourceCpp(&amp;quot;MatrixLog.cpp&amp;quot;)
d = 200
A = matrix(1:(d^2), d, d)
all.equal(log(A), MatrixLog(A))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, they are equal, this is a good first step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
microbenchmark(log(A), MatrixLog(A), times = 1000, unit=&amp;quot;relative&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: relative
##          expr      min       lq     mean   median       uq      max neval
##        log(A) 1.008818 1.042968 0.980134 1.057816 0.979937 0.398702  1000
##  MatrixLog(A) 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000  1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So on average, the C++ implementation is actually around the same speed as the base R implementation. We can speed up the &lt;code&gt;Rcpp&lt;/code&gt; code through the use of parallel computing, using &lt;code&gt;parallelFor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Firstly, &lt;code&gt;parallelFor&lt;/code&gt; has four arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;begin&lt;/code&gt;: the beginning of the for loop&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end&lt;/code&gt;: the end of the for loop&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worker&lt;/code&gt;: an object of type &lt;code&gt;Worker&lt;/code&gt;, where the operations are specified&lt;/li&gt;
&lt;li&gt;&lt;code&gt;grainSize&lt;/code&gt;: minimal chunk size for parallelisation, minimum number of operations for each thread&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before defining the parallel code, &lt;code&gt;parallelFor&lt;/code&gt; needs a &lt;code&gt;Worker&lt;/code&gt; object to specify what processes to perform within the for loop. For this case, we need to create a worker that takes the log of each set of elements that are passed to each thread.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;struct Log : public RcppParallel::Worker
{
   const RcppParallel::RMatrix&amp;lt;double&amp;gt; input;
   RcppParallel::RMatrix&amp;lt;double&amp;gt; output;
  
   Log(const NumericMatrix input, NumericMatrix output) 
      : input(input), output(output) {}
   
   void operator()(std::size_t begin, std::size_t end) {
      std::transform(input.begin() + begin, 
                     input.begin() + end, 
                     output.begin() + begin, 
                     ::log);
   }
};&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s break down this structure. Firstly, two objects of type &lt;code&gt;RMatrix&lt;/code&gt; are specified, for the input and output (recall that an &lt;code&gt;RMatrix&lt;/code&gt; is a thread-safe object given by &lt;code&gt;RcppParallel&lt;/code&gt;). Since different chunks of the matrix will be passed between threads, they need to be converted to this safe &lt;code&gt;RMatrix&lt;/code&gt; object before they are interacted with. Secondly, the &lt;code&gt;Log&lt;/code&gt; function is defined, so that these inputs and outputs are passed through.&lt;/p&gt;
&lt;p&gt;Finally, the &lt;code&gt;operator()&lt;/code&gt; is the main part, which is what will natively be called by &lt;code&gt;parallelFor&lt;/code&gt;. This performs the same operation as what we saw before in &lt;code&gt;MatrixLog&lt;/code&gt;, with a few key differences. Namely the &lt;code&gt;begin&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; function inputs, which change the range that &lt;code&gt;std::transform&lt;/code&gt; is applied to based on the chunk of the matrix that &lt;code&gt;parallelFor&lt;/code&gt; will be giving this worker.&lt;/p&gt;
&lt;p&gt;Now that this is set up, we can rewrite &lt;code&gt;MatrixLog&lt;/code&gt; in parallel:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;RcppParallel.h&amp;gt;
using namespace Rcpp;
// [[Rcpp::depends(RcppParallel)]]

// [[Rcpp::export]]
NumericMatrix MatrixLogPar(NumericMatrix A) {
  
  int n = A.nrow();
  int d = A.ncol();
  NumericMatrix output(n, d);
  
  Log log_(A, output);
  parallelFor(0, A.length(), log_);
  
  return output;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is similar to the original &lt;code&gt;MatrixLog&lt;/code&gt;, however the &lt;code&gt;std::transform&lt;/code&gt; section has been replaced by the definition of the worker &lt;code&gt;log_&lt;/code&gt; (with class &lt;code&gt;Log&lt;/code&gt;), and then the call to &lt;code&gt;parallelFor&lt;/code&gt;. To reiterate, a worker is defined which has the pre-built operator that it will take the log of each element within the chunk that is specified to it. This worker is then spread out across multiple threads by the call to &lt;code&gt;parallelFor&lt;/code&gt;, and the output is saved in &lt;code&gt;output&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we can compare the speed!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceCpp(&amp;quot;MatrixLogPar.cpp&amp;quot;)
all.equal(log(A), MatrixLog(A), MatrixLogPar(A))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(log(A), MatrixLog(A), MatrixLogPar(A), unit=&amp;quot;relative&amp;quot;, times=1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: relative
##             expr      min       lq     mean   median       uq       max neval
##           log(A) 2.339494 2.318958 1.749330 2.048938 1.820713 0.7575495  1000
##     MatrixLog(A) 2.232776 2.197518 1.731507 1.977766 1.801228 0.8874397  1000
##  MatrixLogPar(A) 1.000000 1.000000 1.000000 1.000000 1.000000 1.0000000  1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So originally we had roughly the same processing time as the base R implementation. Now this is roughly twice as fast as base R!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Python for Statistics</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/pythonstats/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/pythonstats/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Python is a powerful programming language that is very popular for data analysis as well as many other applications in the real world. This portfolio will go over some intermediary processes in Python 3.7.3, before  implementing basic statistical models; including linear models, regression and classification.&lt;/p&gt;
&lt;h2 id=&#34;python-overview&#34;&gt;Python Overview&lt;/h2&gt;
&lt;h3 id=&#34;functions-and-modules&#34;&gt;Functions and Modules&lt;/h3&gt;
&lt;p&gt;Similar to R, functions can be defined and called in the same script. The function definition syntax is as follows&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def function_name(input1, input2):
    variable = function_operation
    second_variable = second_function_operation
    return output
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There&amp;rsquo;s a couple things to note here. Firstly, indentation is essential to defining the function. Unlike R or MATLAB, there is nothing to write that marks the beginning or end of the function as this is all handled by where the indentation begins and ends. Secondly, the function name comes after the &lt;code&gt;def&lt;/code&gt;, instead of the other way around (as in R). Finally, the colon (&lt;code&gt;:&lt;/code&gt;) is necessary, as it is in loops and if statements.&lt;/p&gt;
&lt;p&gt;Functions can be written in one script, and called in another, and the way this is achieved is through &lt;em&gt;modules&lt;/em&gt;. Modules are a collection of code that you can load from another file, similar to how &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;library&lt;/code&gt; work in R. You load modules with any of these lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import module_name
import module_name as short_name
from module_name import function_name
from module_name import *
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All these commands load the module (or parts of it), but do it slightly differently. In Python, all modules loaded have to be accessed through their name to use their functions. For example, to use a function from NumPy, you would have to use &lt;code&gt;numpy.function_name&lt;/code&gt;. This name can be shortened if you used the second line above to load the module, commonly, NumPy is abbreviated to &lt;code&gt;np&lt;/code&gt; so that you would only have to type &lt;code&gt;np.function_name&lt;/code&gt; to access it.&lt;/p&gt;
&lt;p&gt;You can also load specific functions from a module by using the third and fourth line above, where the asterisk &lt;code&gt;*&lt;/code&gt; indicates to load all functions from the module. Note that if the module is loaded this way, you do &lt;em&gt;not&lt;/em&gt; need to specify the name of the module preceding the function. It is considered bad practice to load functions this way, as declaring the module name removes the chance of overlapping function names within modules.&lt;/p&gt;
&lt;h4 id=&#34;example-morse-code&#34;&gt;Example: Morse Code&lt;/h4&gt;
&lt;p&gt;As an example of basic operations in Python, consider writing two functions to encode and decode morse code respectively. The encoding function will take a string, or a piece of text, and convert it into morse code. The decoding function does the opposite: converts morse code into plain english text. To write these functions, we first must define a dictionary:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;letter_to_morse = {&#39;a&#39;:&#39;.-&#39;, &#39;b&#39;:&#39;-...&#39;, &#39;c&#39;:&#39;-.-.&#39;, &#39;d&#39;:&#39;-..&#39;, &#39;e&#39;:&#39;.&#39;, 
        &#39;f&#39;:&#39;..-.&#39;, &#39;g&#39;:&#39;--.&#39;, &#39;h&#39;:&#39;....&#39;, &#39;i&#39;:&#39;..&#39;, &#39;j&#39;:&#39;.---&#39;, &#39;k&#39;:&#39;-.-&#39;, 
        &#39;l&#39;:&#39;.-..&#39;, &#39;m&#39;:&#39;--&#39;, &#39;n&#39;:&#39;-.&#39;, &#39;o&#39;:&#39;---&#39;, &#39;p&#39;:&#39;.--.&#39;, &#39;q&#39;:&#39;--.-&#39;, 
        &#39;r&#39;:&#39;.-.&#39;, &#39;s&#39;:&#39;...&#39;, &#39;t&#39;:&#39;-&#39;, &#39;u&#39;:&#39;..-&#39;, &#39;v&#39;:&#39;...-&#39;, &#39;w&#39;:&#39;.--&#39;, 
        &#39;x&#39;:&#39;-..-&#39;, &#39;y&#39;:&#39;-.--&#39;, &#39;z&#39;:&#39;--..&#39;, &#39;0&#39;:&#39;-----&#39;, &#39;1&#39;:&#39;.----&#39;, 
        &#39;2&#39;:&#39;..---&#39;, &#39;3&#39;:&#39;...--&#39;, &#39;4&#39;:&#39;....-&#39;,&#39;5&#39;:&#39;.....&#39;, &#39;6&#39;:&#39;-....&#39;, 
        &#39;7&#39;:&#39;--...&#39;, &#39;8&#39;:&#39;---..&#39;, &#39;9&#39;:&#39;----.&#39;, &#39; &#39;:&#39;/&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this dictionary can be accessed by indexing with the letter that we want a translation for. For example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;letter_to_morse[&amp;quot;6&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, we need to define the reverse; a dictionary that takes morse code and outputs an english letter. We can reverse this dictionary with a loop.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;morse_to_letter = {}
for letter in letter_to_morse:
    morse = letter_to_morse[letter]
    morse_to_letter[morse] = letter
morse_to_letter[&amp;quot;.-&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can define the two functions &lt;code&gt;encode&lt;/code&gt; and &lt;code&gt;decode&lt;/code&gt; that use these two dictionaries. For the encoding function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def encode(x): 
    morse = []
    for letter in x:
        letter = letter.lower()
        morse.append(letter_to_morse[letter])

    morse_message = &amp;quot; &amp;quot;.join(morse)
    return morse_message
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function initialises an array &lt;code&gt;morse&lt;/code&gt;, and loops over all letters in the input &lt;code&gt;x&lt;/code&gt; and appends to the &lt;code&gt;morse&lt;/code&gt; array the translation in morse code. After the loop, the &lt;code&gt;morse&lt;/code&gt; array is joined together. For the decoding function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def decode(x):
    english = []
    morse_letters = x.split(&amp;quot; &amp;quot;)
    for letter in morse_letters:
        english.append(morse_to_letter[letter])
        
    english_message = &amp;quot;&amp;quot;.join(english)
    return english_message
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has a similar process as with &lt;code&gt;encode&lt;/code&gt;. First the input &lt;code&gt;x&lt;/code&gt; is split, and then each split morse symbol is looped over and converted to english using the &lt;code&gt;morse_to_letter&lt;/code&gt; dictionary. Before we run these functions, we can save them in a separate file called &lt;code&gt;morse.py&lt;/code&gt; and put it in our working directory. Then using another script (which will be called &lt;code&gt;run_morse.py&lt;/code&gt;), we can import the &lt;code&gt;morse.py&lt;/code&gt; functions as a module.&lt;/p&gt;
&lt;p&gt;So we can import &lt;code&gt;morse&lt;/code&gt; and run the &lt;code&gt;decode&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import morse
morse_message = &amp;quot;.... . .-.. .--. / .. -- / - .-. .- .--. .--. . -.. / .. -. / ... --- -- . / -- --- .-. ... . / -.-. --- -.. .&amp;quot;
morse.decode(morse_message)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Likewise, we can encode a secret message using &lt;code&gt;morse.encode&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;morse.encode(&amp;quot;please dont turn me into morse code&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing to note here is that the dictionaries &lt;code&gt;morse_to_letter&lt;/code&gt; and &lt;code&gt;letter_to_morse&lt;/code&gt; were defined outside of the functions. Since this module was imported, we can actually access this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;morse.letter_to_morse
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although oftentimes we would want variables to remain hidden, this is not possible if we are importing all the contents of a script. Importing Python files as modules is usually done so that functions can be imported across the working directory, as opposed to having one big file that contains all the code.&lt;/p&gt;
&lt;h3 id=&#34;classes&#34;&gt;Classes&lt;/h3&gt;
&lt;p&gt;Object oriented programming in Python is done through &lt;em&gt;classes&lt;/em&gt;, the general form of which is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class class_name:
      def __init__(self):
          self.variable_I_want_to_define = variable
          self.another_variable_I_want_to_define = variable_2
      def some_class_function(self, x):
          something_I_can_do_with_my_variables = x
          return some_output
      def some_other_function(self):
          self.an_internal_variable = 3
          return another_output
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The functions can be run as a member of the class, and the &lt;code&gt;__init__(self)&lt;/code&gt; section of the class is a function that is run when the class is &lt;em&gt;initialised&lt;/em&gt;. Any variables that are within the &lt;code&gt;__init__&lt;/code&gt; section of the class become defined, sometimes depending on the inputs to the &lt;code&gt;__init__&lt;/code&gt; function. These could be variables such as initial conditions, or initial estimates. We could add more inputs to the &lt;code&gt;__init__&lt;/code&gt; function after &lt;code&gt;self&lt;/code&gt;, if needed.&lt;/p&gt;
&lt;p&gt;The other functions can be called, and these could change some of the internal &lt;code&gt;self.&lt;/code&gt; variables, e.g. updating the initial conditions, running a certain loop or adding additional variables.&lt;/p&gt;
&lt;h4 id=&#34;example-morse-code-again&#34;&gt;Example: Morse Code Again&lt;/h4&gt;
&lt;p&gt;We can update the morse code example to write a class called &lt;code&gt;morse_code_translator&lt;/code&gt; that has an encode and decode function as part of the class. This is written as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class morse_code_translator:
      def __init__(self):
          self.letter_to_morse = {&#39;a&#39;:&#39;.-&#39;, &#39;b&#39;:&#39;-...&#39;, &#39;c&#39;:&#39;-.-.&#39;, &#39;d&#39;:&#39;-..&#39;, 
            &#39;e&#39;:&#39;.&#39;, &#39;f&#39;:&#39;..-.&#39;,  &#39;g&#39;:&#39;--.&#39;, &#39;h&#39;:&#39;....&#39;, &#39;i&#39;:&#39;..&#39;, &#39;j&#39;:&#39;.---&#39;, 
            &#39;k&#39;:&#39;-.-&#39;, &#39;l&#39;:&#39;.-..&#39;, &#39;m&#39;:&#39;--&#39;, &#39;n&#39;:&#39;-.&#39;, &#39;o&#39;:&#39;---&#39;, &#39;p&#39;:&#39;.--.&#39;, 
            &#39;q&#39;:&#39;--.-&#39;, &#39;r&#39;:&#39;.-.&#39;, &#39;s&#39;:&#39;...&#39;, &#39;t&#39;:&#39;-&#39;, &#39;u&#39;:&#39;..-&#39;, &#39;v&#39;:&#39;...-&#39;, 
            &#39;w&#39;:&#39;.--&#39;, &#39;x&#39;:&#39;-..-&#39;, &#39;y&#39;:&#39;-.--&#39;, &#39;z&#39;:&#39;--..&#39;, &#39;0&#39;:&#39;-----&#39;, 
            &#39;1&#39;:&#39;.----&#39;, &#39;2&#39;:&#39;..---&#39;, &#39;3&#39;:&#39;...--&#39;, &#39;4&#39;:&#39;....-&#39;, &#39;5&#39;:&#39;.....&#39;, 
            &#39;6&#39;:&#39;-....&#39;, &#39;7&#39;:&#39;--...&#39;, &#39;8&#39;:&#39;---..&#39;, &#39;9&#39;:&#39;----.&#39;, &#39; &#39;:&#39;/&#39;}
          self.morse_to_letter = {}
          for letter in self.letter_to_morse:
              morse = self.letter_to_morse[letter]
              self.morse_to_letter[morse] = letter
          self.english_message_history = []
          self.morse_message_history = []
      def encode(self, x):
          morse = []
          for letter in x:
              letter = letter.lower()
              morse.append(self.letter_to_morse[letter])
          morse_message = &amp;quot; &amp;quot;.join(morse)       
          self.english_message_history.append(x)
          self.morse_message_history.append(morse_message)
          return morse_message
      def decode(self, x):
          english = []
          morse_letters = x.split(&amp;quot; &amp;quot;)
          for letter in morse_letters:
              english.append(self.morse_to_letter[letter])
          english_message = &amp;quot;&amp;quot;.join(english)
          self.english_message_history.append(english_message)
          self.morse_message_history.append(x)
          return english_message
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this has simply moved the &lt;code&gt;decode&lt;/code&gt; and &lt;code&gt;encode&lt;/code&gt; functions over inside a class. The dictionaries are now written as &lt;code&gt;self.letter_to_morse&lt;/code&gt; and &lt;code&gt;self.morse_to_letter&lt;/code&gt;, with the &lt;code&gt;self.&lt;/code&gt; prefix. Other than that, I have also added two more variables: &lt;code&gt;english_message_history&lt;/code&gt; and &lt;code&gt;morse_message_history&lt;/code&gt;, to exemplify how variables can be updated by calling the functions. Every time &lt;code&gt;encode&lt;/code&gt; or &lt;code&gt;decode&lt;/code&gt; is run, these arrays keep track of all messages that have passed through either function. We can test to see if this works, by importing this class from &lt;code&gt;morse_class.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from morse_class import morse_code_translator
translator = morse_code_translator()
translator.encode(&amp;quot;please not again&amp;quot;)
translator.decode(&amp;quot;.--. .-.. . .- ... . / -.. --- -. - / - ..- .-. -. / -- . / .. -. - --- / . -. --. .-.. .. ... ....&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this has worked as expected, and the &lt;code&gt;translator&lt;/code&gt; object now can both decode and encode morse code messages. Now that these two messages have passed through the class, we can see if the history has worked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;translator.english_message_history
translator.morse_message_history
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the internal &lt;code&gt;self&lt;/code&gt; variables have been updated with the two messages that were passed through the class.&lt;/p&gt;
&lt;h2 id=&#34;statistical-models&#34;&gt;Statistical Models&lt;/h2&gt;
&lt;p&gt;Python has a lot of support for fitting statistical models and machine learning, this is mostly as part of the &lt;code&gt;sklearn&lt;/code&gt; package. Other modules must also be imported that provide different funtionality. Statistical models are handled by &lt;code&gt;sklearn&lt;/code&gt;, dataframes are handled by &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt;, randomness is handled by &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;random&lt;/code&gt;, and plots are handled by &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sklearn as sk
import sklearn.linear_model as lm
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;linear-regression&#34;&gt;Linear Regression&lt;/h3&gt;
&lt;p&gt;A basic linear regression model in python is handled as part of the &lt;code&gt;sklearn.linear_model&lt;/code&gt; module. The use of a linear model in Python can be explained through an example. Some simulated data can be set up as follows
$$
\boldsymbol{x} \sim \text{Unif}(0,1), \qquad y \sim N(\boldsymbol{\mu}, .5), \qquad \boldsymbol{\mu} = \beta_0 + \beta_1 \boldsymbol{x},
$$
where both $\boldsymbol{x}$ and $\boldsymbol{y}$ are of length $n$. Since this is simulated data, we pre-define $\beta_0 = 5$ and $\beta_1 = -2$ as the intercept and the gradient. We can use &lt;code&gt;numpy&lt;/code&gt; to sample this data, and the &lt;code&gt;linear_model&lt;/code&gt; module in &lt;code&gt;sklearn&lt;/code&gt; to fit a linear model to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n = 200
beta0 = 5
beta1 = -2
x = np.random.uniform(0, 1, n)
mu = beta0 + beta1*x
y = np.random.normal(mu, .5, n)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data can be converted into a dataframe with the &lt;code&gt;DataFrame&lt;/code&gt; function in &lt;code&gt;pandas&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = pd.DataFrame({&amp;quot;x&amp;quot;:x, &amp;quot;y&amp;quot;:y})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To set up the linear model, a model object must be set up in advance, and then the fitting routine can be called to it. This can be done in one line by nesting operations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = lm.LinearRegression(fit_intercept=True).fit(data[[&amp;quot;x&amp;quot;]], data[&amp;quot;y&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the model has been fit by first creating a linear model object with &lt;code&gt;lm.LinearRegression&lt;/code&gt; (and specifying that we &lt;em&gt;do&lt;/em&gt; want an intercept). Secondly, the &lt;code&gt;.fit()&lt;/code&gt; function has been called on that object to obtain parameter estimates $\beta_0$ and $\beta_1$ by passing the covariates &lt;code&gt;x&lt;/code&gt; and response variable &lt;code&gt;y&lt;/code&gt; to it. Now we can see whether these estimates bare resemblance to the true parameter values used to simulate the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Intercept:&amp;quot;, model.intercept_, &amp;quot;,&amp;quot;, &amp;quot;Gradient:&amp;quot;, model.coef_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept: 4.89676478340633 , Gradient: [-1.90845212]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is reasonably close to the true values. Note that here there is only one covariate vector, but this method is applicable to multiple covariates which would give multiple estimates of the gradient for each covariate. Now we can plot the data and the fit, a &lt;code&gt;pandas&lt;/code&gt; data frame has an inbuilt method used for plotting the data, which produces a scatter plot from &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data.plot.scatter(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;, figsize=(14, 11));
plt.plot(x, model.predict(data[[&amp;quot;x&amp;quot;]]), &#39;r&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/pythonstats_36_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;multiple-linear-regression&#34;&gt;Multiple Linear Regression&lt;/h3&gt;
&lt;p&gt;In the previous section, we fit a linear model with a single covariate vector. This is good for exemplary purposes as a two dimensional linear model is easy to visualise. Often in data analysis we can have multiple explanatory variables $\boldsymbol{x}_1, \dots, \boldsymbol{x}_p$ that give information about the respone variable $\boldsymbol{y}$.&lt;/p&gt;
&lt;p&gt;For an example of multiple linear regression, we will be using the Boston house prices dataset, which is part of the freely provided datasets from &lt;code&gt;sklearn&lt;/code&gt;. This dataset describes the median value of owner-occupied homes (per $1000), as well as 13 predictors that could provide information about the house value. We load this dataset and convert it to a &lt;code&gt;pandas&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.datasets import load_boston
data = load_boston()
boston = pd.DataFrame(data.data, columns = data.feature_names)
boston[&amp;quot;MEDV&amp;quot;] = data.target
boston.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;CRIM&lt;/th&gt;
      &lt;th&gt;ZN&lt;/th&gt;
      &lt;th&gt;INDUS&lt;/th&gt;
      &lt;th&gt;CHAS&lt;/th&gt;
      &lt;th&gt;NOX&lt;/th&gt;
      &lt;th&gt;RM&lt;/th&gt;
      &lt;th&gt;AGE&lt;/th&gt;
      &lt;th&gt;DIS&lt;/th&gt;
      &lt;th&gt;RAD&lt;/th&gt;
      &lt;th&gt;TAX&lt;/th&gt;
      &lt;th&gt;PTRATIO&lt;/th&gt;
      &lt;th&gt;B&lt;/th&gt;
      &lt;th&gt;LSTAT&lt;/th&gt;
      &lt;th&gt;MEDV&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.00632&lt;/td&gt;
      &lt;td&gt;18.0&lt;/td&gt;
      &lt;td&gt;2.31&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.538&lt;/td&gt;
      &lt;td&gt;6.575&lt;/td&gt;
      &lt;td&gt;65.2&lt;/td&gt;
      &lt;td&gt;4.0900&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;296.0&lt;/td&gt;
      &lt;td&gt;15.3&lt;/td&gt;
      &lt;td&gt;396.90&lt;/td&gt;
      &lt;td&gt;4.98&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.02731&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7.07&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.469&lt;/td&gt;
      &lt;td&gt;6.421&lt;/td&gt;
      &lt;td&gt;78.9&lt;/td&gt;
      &lt;td&gt;4.9671&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;242.0&lt;/td&gt;
      &lt;td&gt;17.8&lt;/td&gt;
      &lt;td&gt;396.90&lt;/td&gt;
      &lt;td&gt;9.14&lt;/td&gt;
      &lt;td&gt;21.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.02729&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7.07&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.469&lt;/td&gt;
      &lt;td&gt;7.185&lt;/td&gt;
      &lt;td&gt;61.1&lt;/td&gt;
      &lt;td&gt;4.9671&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;242.0&lt;/td&gt;
      &lt;td&gt;17.8&lt;/td&gt;
      &lt;td&gt;392.83&lt;/td&gt;
      &lt;td&gt;4.03&lt;/td&gt;
      &lt;td&gt;34.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.03237&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.458&lt;/td&gt;
      &lt;td&gt;6.998&lt;/td&gt;
      &lt;td&gt;45.8&lt;/td&gt;
      &lt;td&gt;6.0622&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;222.0&lt;/td&gt;
      &lt;td&gt;18.7&lt;/td&gt;
      &lt;td&gt;394.63&lt;/td&gt;
      &lt;td&gt;2.94&lt;/td&gt;
      &lt;td&gt;33.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.06905&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.18&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.458&lt;/td&gt;
      &lt;td&gt;7.147&lt;/td&gt;
      &lt;td&gt;54.2&lt;/td&gt;
      &lt;td&gt;6.0622&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;222.0&lt;/td&gt;
      &lt;td&gt;18.7&lt;/td&gt;
      &lt;td&gt;396.90&lt;/td&gt;
      &lt;td&gt;5.33&lt;/td&gt;
      &lt;td&gt;36.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The variable we are interested in predicting is &lt;code&gt;MEDV&lt;/code&gt;, and all other variables will be used to give information to predicting &lt;code&gt;MEDV&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before fitting any models, we can perform some exploratory data analysis to help the decision as to what predictor variables need to be included in the model fitting. Luckily, there are easy methods available that inspect how strongly correlated variables are with other variables in a &lt;code&gt;pandas&lt;/code&gt; data frame. Firstly, we will be using the inbuilt &lt;code&gt;corr&lt;/code&gt; function and plotting it as a heatmap. This uses the &lt;code&gt;seaborn&lt;/code&gt; module to create a heatmap.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import seaborn
corr = boston.corr()
cmap = seaborn.diverging_palette(-500, -720, as_cmap=True)
plt.figure(figsize=(14, 11))
seaborn.heatmap(corr, cmap=cmap);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/pythonstats_40_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are two things to infer from this plot: how correlated the predictor variables are from one another, and how correlated the predictor variables are from the response. We are mostly interested in how correlated the predictors are with &lt;code&gt;MEDV&lt;/code&gt;, as if they influence the value of &lt;code&gt;MEDV&lt;/code&gt; significantly, then they are likely to be important to include in a model. Another plot we can use to explore the data is a multi variable scatter plot. That is, a scatter plot for each pair of variables. &lt;code&gt;pandas&lt;/code&gt; has a function for this, as part of the &lt;code&gt;plotting&lt;/code&gt; submodule.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pandas.plotting import scatter_matrix
scatter_matrix(boston, figsize=(16, 16));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./pythonstats_42_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There is a lot to process in this plot, but you can restrict your attention to the final column on the right. This column will show scatter plots with all predictor variables and the response &lt;code&gt;MEDV&lt;/code&gt;. We are looking for obvious relationships between the two variables. &lt;code&gt;CRIM&lt;/code&gt; seems to have an effect at lower values, &lt;code&gt;ZN&lt;/code&gt; seems to have a small effect, &lt;code&gt;INDUS&lt;/code&gt; has a non-linear relationship,&lt;code&gt;RM&lt;/code&gt; has a clear linear relationship, &lt;code&gt;AGE&lt;/code&gt; seems to have a significant effect at lower values, &lt;code&gt;B&lt;/code&gt; has an effect at larger values and &lt;code&gt;LSTAT&lt;/code&gt; seems to have a quadratic relationship. These variables will be important to consider, but that does not mean all others are not. Since some of these predictors are categorical (even binary), it can be hard to infer their relationship with &lt;code&gt;MEDV&lt;/code&gt; from a scatter plot.&lt;/p&gt;
&lt;p&gt;We can now fit the model using the information above. The predictors we want to include are &lt;code&gt;CRIM&lt;/code&gt;, &lt;code&gt;ZN&lt;/code&gt;, &lt;code&gt;INDUS&lt;/code&gt;, &lt;code&gt;RM&lt;/code&gt;, &lt;code&gt;AGE&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;LSTAT&lt;/code&gt;. We use the linear model from &lt;code&gt;sklearn&lt;/code&gt; again, but first create the new data frame of the reduced dataset. Additionally we can include a quadratic effect for certain predictors by creating a new column in the data frame with the squared values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;boston_x = boston[[&amp;quot;CRIM&amp;quot;, &amp;quot;ZN&amp;quot;, &amp;quot;INDUS&amp;quot;, &amp;quot;RM&amp;quot;, &amp;quot;AGE&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;LSTAT&amp;quot;]]
boston_x.insert(len(boston_x.columns), &amp;quot;LSTAT2&amp;quot;, [i**2 for i in boston_x[&amp;quot;LSTAT&amp;quot;] ]) 
boston_x.insert(len(boston_x.columns), &amp;quot;AGE2&amp;quot;, [i**2 for i in boston_x[&amp;quot;AGE&amp;quot;] ]) 
boston_x.insert(len(boston_x.columns), &amp;quot;B2&amp;quot;, [i**2 for i in boston_x[&amp;quot;B&amp;quot;] ]) 

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now using the linear model with this modified data frame to fit the full model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mv_model = lm.LinearRegression(fit_intercept=True).fit(boston_x, data.target)
print(&amp;quot;Intercept: \n&amp;quot;, mv_model.intercept_, &amp;quot;\n&amp;quot;)
print(&amp;quot;Coefficients:&amp;quot;)
print(&amp;quot; \n&amp;quot;.join([str(boston_x.columns[i]) + &amp;quot; &amp;quot; + str(mv_model.coef_[i]) for i in np.arange(len(mv_model.coef_))]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept: 
 5.916804277809813 

Coefficients:
CRIM -0.11229848683846888 
ZN 0.0037476008537828498 
INDUS -0.0021805395766587208 
RM 4.0621316253156285 
AGE 0.07875634757295602 
B 0.042639346099976716 
LSTAT -2.063750521016358 
LSTAT2 0.042240400122337596 
AGE2 -0.0002509463690499019 
B2 -7.788232883181183e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;penalised-regression&#34;&gt;Penalised Regression&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;linear_model&lt;/code&gt; submodule of &lt;code&gt;sklearn&lt;/code&gt; doesn&amp;rsquo;t &lt;em&gt;just&lt;/em&gt; contain the &lt;code&gt;LinearRegression&lt;/code&gt; object, it contains many other variations of linear models. Some examples of these are penalised regression models, such as the Lasso model or ridge regression model. You can view the documentation for these online, and they work similarly to &lt;code&gt;LinearRegression&lt;/code&gt;. Below I will briefly go through an example of using a Lasso path algorithm on the dataset provided above. Firstly, the model object can be created immediately using the &lt;code&gt;linear_model.lars_path&lt;/code&gt; object.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;_,_,coefs = lm.lars_path(data.data, data.target)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is using the full dataset provided in &lt;code&gt;data.data&lt;/code&gt;, as the Lasso path plot can be used for feature selection, similar to the scatter matrix approach above.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;xx = np.sum(np.abs(coefs.T), axis=1)
xx /= xx[-1]
plt.figure(figsize=(16, 9))
plt.plot(xx, coefs.T)
ymin, ymax = plt.ylim()
plt.vlines(xx, ymin, ymax, linestyle=&#39;dashed&#39;)
plt.xlabel(&#39;|coef| / max|coef|&#39;)
plt.ylabel(&#39;Coefficients&#39;)
plt.title(&#39;LASSO Path&#39;)
plt.axis(&#39;tight&#39;)
plt.legend(data.feature_names)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/pythonstats_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The coefficients for covariates that provide a lot of information to the response variable are likely going to take a very large value of regularisation parameter to shrink them to zero. These are the lines that are more separated in the plot above, i.e. &lt;code&gt;CRIM&lt;/code&gt;, &lt;code&gt;RM&lt;/code&gt;, &lt;code&gt;NOX&lt;/code&gt;, and &lt;code&gt;CHAS&lt;/code&gt;. Interestingly, these were not the variables that were selected in the scatter matrix approach earlier in this report, highlighting the variation in parameter selection depending on the method.&lt;/p&gt;
&lt;h3 id=&#34;classification&#34;&gt;Classification&lt;/h3&gt;
&lt;p&gt;The final statistical model introduced will be one interested in classifying. Classification is interested in prediction of a usually non-numeric &lt;em&gt;class&lt;/em&gt;, i.e. assign the output to a pre-defined class based on some inputs. The classification model explained here will be &lt;strong&gt;Logistic Regression&lt;/strong&gt; (ignore the name, it&amp;rsquo;s not a regression method).&lt;/p&gt;
&lt;p&gt;Logistic regression is called so because it uses a &lt;em&gt;logistic function&lt;/em&gt; to map fitted values to probabilities. The basic form of a classification model is the same of that of a regression method, except the output from the model corresponds to latent, unobserved fitted values which decide the class of each set of inputs.&lt;/p&gt;
&lt;p&gt;We will start by simulating some data $\boldsymbol{y}$ that need to be classified, which will be generated by a mathematical combination of some inputs $\boldsymbol{x}$. Each element of $\boldsymbol{y}$ will be given one of the class labels $\mathcal{C}_1$, $\mathcal{C}_2$ or $\mathcal{C}_3$. The goal of the statistical model is parameter estimation, similar to the regression methods.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate x
n = 1000
x1 = np.random.uniform(0, 1, n)
x2 = np.random.uniform(0, 1, n)

# Create parameters and mean
beta0 = 3
beta1 = 1.5
beta2 = -4.8
beta3 = 0.5
mu = beta0 + beta1*x1 + beta2*x2 + beta3*x1**2

# Set class based on value of mean
y = np.repeat(&amp;quot;Class 1&amp;quot;, n)
y[mu &amp;gt; 0] = &amp;quot;Class 2&amp;quot;
y[mu &amp;gt; 2] = &amp;quot;Class 3&amp;quot;

# Combine into DataFrame
clsf_dat = pd.DataFrame({&amp;quot;y&amp;quot;:y,&amp;quot;x1&amp;quot;:x1,&amp;quot;x2&amp;quot;:x2 })
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;colors = y
colors[y == &amp;quot;Class 1&amp;quot;] = &amp;quot;red&amp;quot;
colors[y == &amp;quot;Class 2&amp;quot;] = &amp;quot;blue&amp;quot;
colors[y == &amp;quot;Class 3&amp;quot;] = &amp;quot;green&amp;quot;
clsf_dat.plot.scatter(&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;, c=colors, figsize=(14, 11));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/pythonstats_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that the data is set up, we can fit the logistic regression model to it. &lt;code&gt;LogisticRegression&lt;/code&gt; is a function as part of the &lt;code&gt;linear_model&lt;/code&gt; submodule of &lt;code&gt;sklearn&lt;/code&gt;, just as before, so this next step should be familiar to you.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lr_model = lm.LogisticRegression(fit_intercept=True, solver=&#39;newton-cg&#39;, multi_class = &#39;auto&#39;)
lr_model = lr_model.fit(clsf_dat[[&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;]], clsf_dat[&amp;quot;y&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Intercept: \n&amp;quot;, lr_model.intercept_, &amp;quot;\n&amp;quot;)
print(&amp;quot;Coefficients:&amp;quot;)
print(&amp;quot; \n&amp;quot;.join([str(clsf_dat.columns[i]) + &amp;quot; &amp;quot; + str(lr_model.coef_[i]) for i in np.arange(len(lr_model.coef_))]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Intercept: 
 [-4.45432353  0.92640783  3.52791569] 

Coefficients:
y [-4.06694321  9.66044839] 
x1 [0.18794764 0.76440687] 
x2 [  3.87899558 -10.42485527]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that there are multiple coefficients for each input! This is because even though the data were simulated using one parameter each, the logistic regression model has a different input combination for each class. The first class has both coefficients aliased to the intercept, and anything significantly different (in line with the combination of the remaining parameters) is given a different class.&lt;/p&gt;
&lt;p&gt;We can roughly evaluate the model&amp;rsquo;s performance by inspecting a plot of its predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = lr_model.predict(clsf_dat[[&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;colors = predictions
colors[predictions == &amp;quot;Class 1&amp;quot;] = &amp;quot;red&amp;quot;
colors[predictions == &amp;quot;Class 2&amp;quot;] = &amp;quot;blue&amp;quot;
colors[predictions == &amp;quot;Class 3&amp;quot;] = &amp;quot;green&amp;quot;
clsf_dat.plot.scatter(&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;, c=colors, figsize=(14, 11));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/pythonstats_60_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we can see an extremely similar plot to the one above, although it is not quite exactly the same! At the point of overlap between classes, some points are mislabelled as the wrong class, due to uncertainty around the decision boundaries.&lt;/p&gt;
&lt;p&gt;More complex models exist for classification and regression, but that&amp;rsquo;s for another time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Network for Identifying Cats and Dogs</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/neuralnet/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/neuralnet/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We fit a neural network to a dataset of cat and dog images, in an attempt to distinguish features that can correctly identify whether the image is of a cat or of a dog.&lt;/p&gt;
&lt;p&gt;This work uses the &lt;code&gt;keras&lt;/code&gt; library from &lt;code&gt;tensorflow&lt;/code&gt;. This portfolio will go over the following processes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Loading the data and formatting it be fit by a neural network in &lt;code&gt;tensorflow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fitting a neural network to the images in a training set&lt;/li&gt;
&lt;li&gt;Identifying and evaluating predictive performance on a testing set&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
import numpy as np
import IPython.display as display
import matplotlib.pyplot as plt

import os
import pathlib

from tensorflow import keras
from PIL import Image

tf.__version__
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&#39;2.1.0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;loading-and-formatting-the-dataset&#34;&gt;Loading and Formatting the Dataset&lt;/h2&gt;
&lt;p&gt;The data are from the Kaggle competition &amp;ldquo;Dogs vs Cats&amp;rdquo;. These images are loaded into the working directory and defined below. These are pre-separated into a training set, validation set and a testing set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir = &amp;quot;/home/fs19144/Documents/SC2/Section10/data/train/&amp;quot;
data_dir = pathlib.Path(data_dir)
class_names = np.array([item.name for item in data_dir.glob(&#39;*&#39;)])
image_count = len(list(data_dir.glob(&#39;*/*.jpg&#39;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data are pre-downloaded and saved separately. This code aboves load the data, retrieves the class names and the total number of images in the training set. The class names are retrieved from the folder names, being &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;dog&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class_names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([&#39;cat&#39;, &#39;dog&#39;], dtype=&#39;&amp;lt;U3&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The image count is calculated by reading the length of the list containing all elements in the data directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image_count
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6002
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To format the data for a neural network, a few things must be accomplished. Firstly, the images need to have a lower resolution that normal, to avoid large file sizes and to maintain consistency across the dataset. Secondly, the images need to be loaded in batches, to avoid storing all pixel values for all images in a large array. Consider doing this, we have 6002 images, each having a corresponding pixel value for red, green and blue. If the image height and width is 60 pixels, then in total the number of elements in the training set would be&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;60 * 60 * 3 * 6002
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;64821600
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is extremely large. This size increases greatly with a higher image resolution, or a larger dataset, highlighting the need to load images in batches. We define these variables to go into the model fitting below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batch_size = 32
image_height = 60
image_width = 60
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use functionality from tensorflow to list the data objects in a &lt;code&gt;tf.Dataset&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_ds = tf.data.Dataset.list_files(str(data_dir/&#39;*/*&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This training set contains the path to all files in the &lt;code&gt;train&lt;/code&gt; directory, which can be iterated over every time we want to create a batch. Below are some functions that we will be applying to this data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_label(file_path):
  parts = tf.strings.split(file_path, os.path.sep)
  return parts[-2] == class_names

def decode_img(img):
  img = tf.image.decode_jpeg(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)
  return tf.image.resize(img, [image_width, image_height])

def process_path(file_path):
  label = get_label(file_path)
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;process_path&lt;/code&gt; function takes the &lt;code&gt;file_path&lt;/code&gt; (given when we map across &lt;code&gt;train_ds&lt;/code&gt;), retrieves the label from &lt;code&gt;get_label&lt;/code&gt; and decodes the image into pixel values in &lt;code&gt;decode_img&lt;/code&gt;. &lt;code&gt;get_label&lt;/code&gt; simply reads the folder name which the file being iterated over is stored in, whilst &lt;code&gt;decode_img&lt;/code&gt; uses &lt;code&gt;tensorflow&lt;/code&gt; functionality to read the image in the required format. The &lt;code&gt;tf.data.Dataset&lt;/code&gt; structure which &lt;code&gt;train_ds&lt;/code&gt; is saved as has inbuilt mapping functionality, so we apply &lt;code&gt;process_path&lt;/code&gt; to get a labelled training set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;labelled_ds = train_ds.map(process_path, num_parallel_calls=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we create a function that prepares the dataset for training iterations. This shuffles the dataset, and takes a batch of the specified size we defined earlier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def prepare_for_training(ds, shuffle_buffer_size=1000):

  ds = ds.shuffle(buffer_size=shuffle_buffer_size)
  ds = ds.repeat()
  ds = ds.batch(batch_size)
  ds = ds.prefetch(buffer_size=2)

  return ds
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_ds_2 = prepare_for_training(labelled_ds)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;validation-set&#34;&gt;Validation Set&lt;/h4&gt;
&lt;p&gt;A neural network model can be improved by using a validation set to reduce overfitting. The neural network model will be fit using the validation set to check the loss on data that it is not training on. The process for formatting the data above can be repeated for different images in a &lt;code&gt;validation&lt;/code&gt; folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dir_val = &amp;quot;/home/fs19144/Documents/SC2/Section10/data/validation/&amp;quot;
data_dir_val = pathlib.Path(data_dir_val)
class_names_val = np.array([item.name for item in data_dir_val.glob(&#39;*&#39;)])
image_count_val = len(list(data_dir_val.glob(&#39;*/*.jpg&#39;)))
val_ds = tf.data.Dataset.list_files(str(data_dir_val/&#39;*/*&#39;))
labelled_ds_val = val_ds.map(process_path, num_parallel_calls=2)
val_ds_2 = prepare_for_training(labelled_ds_val)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;example-batch&#34;&gt;Example Batch&lt;/h4&gt;
&lt;p&gt;We can iterate once over &lt;code&gt;train_ds_2&lt;/code&gt; to view an example image batch, along with their labels:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image_batch, label_batch = next(iter(train_ds_2))
plt.figure(figsize=(10,12))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(image_batch[i], cmap=plt.cm.binary)
    ind = [bool(i) for i in (label_batch[i])]
    plt.xlabel(class_names[ind])
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/neuralnet_24_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;fitting-the-neural-network&#34;&gt;Fitting the Neural Network&lt;/h2&gt;
&lt;p&gt;We use the &lt;code&gt;keras&lt;/code&gt; library, a part of &lt;code&gt;tensorflow&lt;/code&gt;, to fit a neural network to classify these images. Most importantly, the &lt;code&gt;Sequential&lt;/code&gt; function to build the model and different layer functions to create the neural network layers within the &lt;code&gt;Sequential&lt;/code&gt; model build.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is the code used to specify the model using &lt;code&gt;Sequential&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = Sequential([
    Conv2D(16, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, 
           input_shape=(image_height, image_width ,3)),
    MaxPooling2D(),
    Dropout(0.2),
    Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;),
    MaxPooling2D(),
    Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;),
    MaxPooling2D(),
    Dropout(0.2),
    Flatten(),
    Dense(512, activation=&#39;relu&#39;),
    Dense(2)
])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This convolutional neural network contains 8 layers, with a final layer to output a classification.  The convolutional layers, given by &lt;code&gt;Conv2D&lt;/code&gt;, apply convolutional operations to the image and output a single value based on the operations. The pooling layers, given by &lt;code&gt;MaxPooling2D&lt;/code&gt;, extract key features of the image and reduce the dimensionality to reduce computational time. In this case, the max pooling approach extracts the maximum value over subregions of the image. &lt;code&gt;Dropout&lt;/code&gt; is another method that helps to reduce overfitting, dropping out a proportion of the dataset, and makes the distribution of weight values in the neural network more regular.&lt;/p&gt;
&lt;p&gt;Finally, the &lt;code&gt;Flatten&lt;/code&gt; function reformats the data, so that it is vector form and not in a high dimensional format. The &lt;code&gt;Dense&lt;/code&gt; layer provides most of the information to the model. These fully connected layers perform the classification on the output to all the other layers. The final &lt;code&gt;Dense(2)&lt;/code&gt; will output two nodes, giving probabilities for each one. This will be classifying either a cat or a dog.&lt;/p&gt;
&lt;p&gt;This model needs to be compiled. We use the &lt;code&gt;adam&lt;/code&gt; optimiser, which is a variation on gradient descent.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(optimizer=&#39;adam&#39;,
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=[&#39;accuracy&#39;])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we fit the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;history = model.fit(
    train_ds_2,
    steps_per_epoch = image_count // batch_size,
    epochs = 10,
    validation_data = val_ds_2,
    validation_steps = image_count_val // batch_size
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train for 187 steps, validate for 62 steps
Epoch 1/10
187/187 [==============================] - 22s 119ms/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6906 - val_accuracy: 0.5514
Epoch 2/10
187/187 [==============================] - 22s 119ms/step - loss: 0.6578 - accuracy: 0.5639 - val_loss: 0.6227 - val_accuracy: 0.6127
Epoch 3/10
187/187 [==============================] - 21s 112ms/step - loss: 0.6024 - accuracy: 0.6527 - val_loss: 0.6277 - val_accuracy: 0.6235
Epoch 4/10
187/187 [==============================] - 21s 113ms/step - loss: 0.5647 - accuracy: 0.6868 - val_loss: 0.5571 - val_accuracy: 0.6920
Epoch 5/10
187/187 [==============================] - 21s 114ms/step - loss: 0.5284 - accuracy: 0.7214 - val_loss: 0.5474 - val_accuracy: 0.7054
Epoch 6/10
187/187 [==============================] - 22s 116ms/step - loss: 0.4949 - accuracy: 0.7411 - val_loss: 0.5419 - val_accuracy: 0.7114
Epoch 7/10
187/187 [==============================] - 21s 114ms/step - loss: 0.4685 - accuracy: 0.7585 - val_loss: 0.5114 - val_accuracy: 0.7384
Epoch 8/10
187/187 [==============================] - 21s 114ms/step - loss: 0.4392 - accuracy: 0.7807 - val_loss: 0.4992 - val_accuracy: 0.7523
Epoch 9/10
187/187 [==============================] - 22s 117ms/step - loss: 0.4091 - accuracy: 0.7996 - val_loss: 0.5721 - val_accuracy: 0.7256
Epoch 10/10
187/187 [==============================] - 21s 113ms/step - loss: 0.3819 - accuracy: 0.8182 - val_loss: 0.4976 - val_accuracy: 0.7573
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;evaluating-the-model&#34;&gt;Evaluating the Model&lt;/h2&gt;
&lt;p&gt;Now that the model is fit, how do we know that it&amp;rsquo;s doing a good job? Firstly, by the final lines in the model fitting, we can see that the prediction accuracy ended at around 0.82, so 82% of training set predictions were correct, and around 75% of validation set predicctions were.&lt;/p&gt;
&lt;p&gt;We can also plot the loss on the training set and the validation set below. We want this to decrease in both cases, as a smaller loss means a better fit. We also want the predictiona accuracy to increase, so that predictions are more correct. If the validation set loss has also decreased to a low value, and the accuracy has increased, then the model has avoided overfitting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;acc  = history.history[&#39;accuracy&#39;]
loss = history.history[&#39;loss&#39;]
val_loss = history.history[&#39;val_loss&#39;]
val_acc  = history.history[&#39;val_accuracy&#39;]

epochs_range = range(10)

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;)
plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;)
plt.legend(loc=&#39;lower right&#39;)
plt.title(&#39;Training and Validation Accuracy&#39;)

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;)
plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;)
plt.legend(loc=&#39;upper right&#39;)
plt.title(&#39;Training and Validation Loss&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/neuralnet_34_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;These plots show that the loss steadily decreases, and the accuracy increases in both cases. Now we can evaluate some predictions on the test set. The test set do not have specific class labels, and we are treating these as unknown.&lt;/p&gt;
&lt;h4 id=&#34;test-set-predictions&#34;&gt;Test Set Predictions&lt;/h4&gt;
&lt;p&gt;To further test the performance of the model, and inspect it ourselves, we can see how well the model predicts on a test set (which the model has never seen). This is a separate dataset to the validation set which was part of the model fitting process.&lt;/p&gt;
&lt;p&gt;Firstly, we load the image using the &lt;code&gt;PIL.Image&lt;/code&gt; module, and load the images into a list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import PIL.Image as Image
import glob


pred_list = []
for filename in glob.glob(&#39;/home/fs19144/Documents/SC2/Section10/data/test/*.jpg&#39;): 
    x = Image.open(filename).resize((image_height, image_width))
    pred_list.append(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we loop over the 20 elements in this small test set to inspect the model predictions. Each image is loaded as a &lt;code&gt;numpy&lt;/code&gt; array, which automatically converts it into pixel format. These pixel values need to be divided by 255, to get RGB values in $[0,1]$. We obtain model predictions using the simple &lt;code&gt;predict&lt;/code&gt; function as part of the &lt;code&gt;model&lt;/code&gt; object. These predictions come in terms of fitted values (not strictly probabilities). The predicted classes are then given by taking the largest fitted values output by the model. We can plot the images and their predictions below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred_class = np.empty([len(pred_list)], dtype=&amp;quot;S3&amp;quot;)

plt.figure(figsize=(10,12))
for i in np.arange(len(pred_list)):
    x = np.array(pred_list[i])/255.0
    pred = model.predict(x[np.newaxis, ...])     
    pred_class[i] = class_names[np.argmax(pred[0])]
    
    # Plot
    plt.subplot(4,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x)
    plt.xlabel(pred_class[i])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://dannyjameswilliams.co.uk/img/portfolios/neuralnet_38_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are some funny looking dogs in this sample, and some funny looking cats. Clearly we can see that oftentimes the model calls a cat a dog, and a dog a cat (I imagine the dogs are more offended by that), but for the most part the predictions look accurate.&lt;/p&gt;
&lt;p&gt;There is a lot more that can go into improving this model, such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-processing the image data&lt;/li&gt;
&lt;li&gt;Increasing the dataset size&lt;/li&gt;
&lt;li&gt;Increasing the image resolution&lt;/li&gt;
&lt;li&gt;Adding more layers to the neural network&lt;/li&gt;
&lt;li&gt;Adding &amp;lsquo;null&amp;rsquo; images, that show neither a cat nor dog, so that the model does not always predict a cat or dog, it can be neither&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Section 5: High Performance Computing with Bluecrystal</title>
      <link>http://dannyjameswilliams.co.uk/portfolios/sc2/hpc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://dannyjameswilliams.co.uk/portfolios/sc2/hpc/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We’ve all been in the situation where we want to perform some extremely complicated computing process, such as MCMC or manipulating an extremely large data frame a lot of times, but our laptops just aren’t good enough. They don’t have enough cores, no dedicated GPU and only a small amount of memory. Luckily for academics and PhD students, universities in general sympathise with us. A high performance computing (HPC) cluster is a collection of highly powerful computers located somewhere that can be accessed remotely, and used to run terminal scripts for coding.&lt;/p&gt;
&lt;p&gt;This portfolio will detail the use of &lt;strong&gt;Bluecrystal&lt;/strong&gt;, the super computers available at the University of Bristol. To access these computers, one needs to do so through the terminal, and so a basic knowledge of manipulating and using file structures in &lt;code&gt;bash&lt;/code&gt; is required. Section 4 details the fundamentals of &lt;code&gt;bash&lt;/code&gt; if the reader is not already familiar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bluecrystal-phase-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bluecrystal Phase 3&lt;/h2&gt;
&lt;p&gt;This tutorial will focus on &lt;em&gt;Bluecrystal Phase 3&lt;/em&gt;, that is the third generation of Bluecrystal machines. The cluster is made up of 312 compute &lt;em&gt;nodes&lt;/em&gt;, which are where the processes are run. The basic nodes have specifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;64GB RAM&lt;/li&gt;
&lt;li&gt;300TB storage&lt;/li&gt;
&lt;li&gt;16 Cores&lt;/li&gt;
&lt;li&gt;Infiniband High Speed Network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As well as the large memory nodes, which have 256GB of RAM, and the GPU nodes which each have an NVIDIA Tesla K20 - an extremely powerful GPU.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;connecting-to-the-hpc-cluster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Connecting to the HPC Cluster&lt;/h2&gt;
&lt;p&gt;To access the HPC cluster, you would need to log in via SSH (secure shell) from a University of Bristol connection (or a VPN). The &lt;code&gt;ssh&lt;/code&gt; command in bash is your friend here. To log in (assuming you have an account), you perform the following command:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;ssh your_username@bluecrystalp3.bris.ac.uk&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It will then ask for your password, which you supply immediately after writing this command. From here you will be in the &lt;em&gt;log-in node&lt;/em&gt;, note that you should &lt;strong&gt;not&lt;/strong&gt; run any code on the log-in node, as this node is only purposed for connecting to the compute nodes. If you run any large code on the log-in node you will slow down the HPC cluster for everyone else.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;file-system&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;File System&lt;/h2&gt;
&lt;p&gt;Within the log-in node, you will have your own personal file directory where you can store your files and your code. By default, after logging in you will be in this directory, so if you use the &lt;code&gt;ls&lt;/code&gt; command you will see the contents of your personal directory immediately. You can write code here, through the terminal, or copy it into your directory from your own personal computer. You are free to make directories and files here, as the contents of your directory will be read by the compute nodes when you want to run a job.&lt;/p&gt;
&lt;p&gt;To copy a file from your own computer to your file system in the HPC cluster, you can use the &lt;code&gt;scp&lt;/code&gt; command in &lt;code&gt;bash&lt;/code&gt;, a command that is run from your &lt;em&gt;own&lt;/em&gt; computer only and looks like&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;scp path_to_my_file/file.txt your_username@bluecrystalp3.bris.ac.uk::path_inside_hpc/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Performing this operation would copy &lt;code&gt;file.txt&lt;/code&gt; in folder &lt;code&gt;path_to_my_file&lt;/code&gt; into the &lt;code&gt;path_inside_hpc&lt;/code&gt; folder on your directory in the HPC cluster. If you want to do this the other way around, and copy something from your HPC cluster file system to your personal computer, just switch the order of the arguments to &lt;code&gt;scp&lt;/code&gt;, but always do it from your own machine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-jobs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running Jobs&lt;/h2&gt;
&lt;p&gt;To run a job, you must write a &lt;code&gt;bash&lt;/code&gt; script that tells the compute node what to do. This &lt;code&gt;bash&lt;/code&gt; script will be interpreted at the HPC node and run accordingly. Below is a general template for what this &lt;code&gt;bash&lt;/code&gt; script would look like to be passed across:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#!/bin/bash
#
#PBS -l nodes=2:ppn=1,walltime=24:00:00

# working directory
export WORK_DIR=$HOME
cd $WORK_DIR

# print to output
echo JOB ID: $PBS_JOBID
echo Working Directory `pwd`

# run something
/bin/hostname&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line &lt;code&gt;#!/bin/bash&lt;/code&gt; tells the compiler to read this as &lt;code&gt;bash&lt;/code&gt;, and the third line &lt;code&gt;#PBS -l nodes=2:ppn=1,walltime=24:00:00&lt;/code&gt; gives information to the HPC cluster as to what you want for the job. You can change these arguments to your suiting, e.g. increase the &lt;code&gt;walltime&lt;/code&gt; if you think your code will run for more than 24 hours.&lt;/p&gt;
&lt;p&gt;You must save this &lt;code&gt;bash&lt;/code&gt; script as something like &lt;code&gt;run_R.sh&lt;/code&gt;, and then when logged into Bluecrystal, use the command &lt;code&gt;qsub&lt;/code&gt; - meaning to submit this job to the queue, i.e.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;qsub run_R.sh&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which would add this job to the queue. Since there are many people that use the HPC cluster, your job may not start immediately, and you might have to wait. You may have to wait longer if your &lt;code&gt;walltime&lt;/code&gt; is particularly high, as you will be waiting for enough nodes to become available.&lt;/p&gt;
&lt;div id=&#34;other-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Functions&lt;/h3&gt;
&lt;p&gt;As well as &lt;code&gt;qsub&lt;/code&gt;, there are other commands that you can use to play with the HPC cluster. Some notable ones are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;qstat&lt;/code&gt; gives a list of current jobs being run and those in the queue&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qstat -u user_name&lt;/code&gt; gives a list of current jobs queued and running by &lt;code&gt;user_name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qstat job_id&lt;/code&gt; gives information about the job &lt;code&gt;job_id&lt;/code&gt; being run&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qdel job_id&lt;/code&gt; deletes a job with a given &lt;code&gt;job_id&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-different-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running Different Code&lt;/h2&gt;
&lt;p&gt;To run other programming languages on the HPC cluster, it can be a bit of faff. To run code such as Python or R on the cluster, you must first load the &lt;em&gt;module&lt;/em&gt; associated with the particular language. On the log-in node, you can run&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;module avail&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to get a list of all available modules. There will be a lot. Choose one that you like, for example, I am a personal fan of &lt;code&gt;languages/R-3.6.2-gcc9.1.0&lt;/code&gt;, and you can load this with &lt;code&gt;module load module_name&lt;/code&gt;, for example&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;module load languages/R-3.6.2-gcc9.1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which will allow you to run R and R scripts. To submit a job that runs an R script, you must add this line to the job script before you run the code. To run an R script from &lt;code&gt;bash&lt;/code&gt;, you use&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Rscript script_name.R&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;using-r-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using R Packages&lt;/h3&gt;
&lt;p&gt;Since packages cannot be installed globally on the log-in node, you can install them locally instead. You first type the command &lt;code&gt;R&lt;/code&gt; into bash, and then &lt;code&gt;install.packages(&amp;quot;package_name&amp;quot;)&lt;/code&gt;. It will ask you if you want the package to be installed locally, which you say yes to.&lt;/p&gt;
&lt;p&gt;After this, all packages installed on your local file system on the log-in node will be accessible as normal when running job scripts.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
