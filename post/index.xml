<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog | Danny James Williams</title>
    <link>https://dannyjameswilliams.co.uk/post/</link>
      <atom:link href="https://dannyjameswilliams.co.uk/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-gb</language><lastBuildDate>Mon, 05 Jun 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dannyjameswilliams.co.uk/images/icon_hu6de9a8f7dd4e8a8bd7c2613cf2ad59bf_37670_512x512_fill_lanczos_center_3.png</url>
      <title>Blog</title>
      <link>https://dannyjameswilliams.co.uk/post/</link>
    </image>
    
    <item>
      <title>2 Years of PhD Research: Stein Discrepancies with a Twist</title>
      <link>https://dannyjameswilliams.co.uk/post/tksd/</link>
      <pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/tksd/</guid>
      <description>


&lt;p&gt;This is a blog post detailing &lt;a href=&#34;https://arxiv.org/abs/2306.00602&#34;&gt;Approximate Stein Classes for Truncated Density Estimation&lt;/a&gt;, by myself and my supervisor, Song Liu, which recently got accepted into &lt;strong&gt;ICML 2023&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Pretend, for a moment, that you are the kind of person who likes to see where animals live, and you go out for the day to find where all the animal habitats are. You are interested in the broader picture; the general spread of habitat locations across a certain region. What you would be doing is looking to &lt;strong&gt;model a density&lt;/strong&gt; based on each observation of a habitat. However, you might find that these habitats arbitrarily stop after some point, and you don’t have an exact reason why. In a similar way, you might not be allowed to cross into a neighbouring country to continue measurements. In both of these scenarios, you are prohibited from viewing a &lt;em&gt;full picture&lt;/em&gt; of your dataset due to some unknown circumstances - but you &lt;em&gt;do&lt;/em&gt; have access to something, which is a collection of points that roughly make up the ‘edge’ of your domain, where &lt;strong&gt;your data are truncated&lt;/strong&gt;. &lt;em&gt;How do you estimate your density now?&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Up until the introduction of this work, to estimate the density of your wildlife habitat locations, you would probably try to use &lt;em&gt;TruncSM&lt;/em&gt; &lt;a href=&#34;#truncsm&#34;&gt;[1]&lt;/a&gt;, a very fine work which uses Score Matching &lt;a href=&#34;#scorematching1&#34;&gt;[2]&lt;/a&gt; to do truncated density estimation. This work is quite interesting if you are a fan of this kind of thing. If you want to read more about it I also wrote a &lt;a href=&#34;https://dannyjameswilliams.co.uk/post/nodata/&#34;&gt;blog post last year&lt;/a&gt; which goes into a few more details, or read &lt;a href=&#34;https://www.jmlr.org/papers/volume23/21-0218/21-0218.pdf&#34;&gt;the full paper here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The jist of the method is that our true density, &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, (which is made up of only samples, only the wildlife habitats we observed) needs to be modelled by something which we denote as &lt;span class=&#34;math inline&#34;&gt;\(p_{\boldsymbol{\theta}}\)&lt;/span&gt;, which looks like
&lt;span class=&#34;math display&#34;&gt;\[
p_{\boldsymbol{\theta}}= \frac{\bar{p}_{\boldsymbol{\theta}}}{Z(\boldsymbol{\theta})}, \; \; \; Z(\boldsymbol{\theta}) = \int_{V}\bar{p}_{\boldsymbol{\theta}} d\boldsymbol{x}.
\]&lt;/span&gt;
We can modify &lt;span class=&#34;math inline&#34;&gt;\(p_{\boldsymbol{\theta}}\)&lt;/span&gt; only through &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}\)&lt;/span&gt;, and so we want to find a &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
p_{\boldsymbol{\theta}}\approx q.
\]&lt;/span&gt;
This is relatively straightforward most of the time, when we integrate &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\theta})\)&lt;/span&gt; in a ‘normal’ way (&lt;span class=&#34;math inline&#34;&gt;\(V = \mathbb{R}^d\)&lt;/span&gt;). But this blog post is not about ‘most of the time’, we are looking at something harder (&lt;span class=&#34;math inline&#34;&gt;\(V \neq \mathbb{R}^d\)&lt;/span&gt;). The integration for &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\theta})\)&lt;/span&gt; is really hard. So hard in fact, that no one wants to integrate it at all (sorry &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\theta})\)&lt;/span&gt; but you’re too difficult). This is known as &lt;em&gt;unnormalised density estimation&lt;/em&gt;. Well, turns out we can ignore our issues altogether if we just use Score matching. &lt;strong&gt;By using Score Matching we can ignore the difficult parts of what makes estimating a truncated density hard&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Our method, Truncated Kernelised Stein Discrepancies (what a mouthful, we’ll call it TKSD from now on), uses the same broad strokes as Score Matching, which, roughly speaking, means we also use the &lt;em&gt;score function&lt;/em&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{\psi}_{p_{\boldsymbol{\theta}}} = \nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta}),
\]&lt;/span&gt;
a Stein operator,
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{T}_{p_{\boldsymbol{\theta}}} \boldsymbol{f}(\boldsymbol{x}) := \sum^d_{l=1}\psi_{p_{\boldsymbol{\theta}}, l}(\boldsymbol{x}) f_l(\boldsymbol{x}) + \partial_{x_l}f_l(\boldsymbol{x}),
\]&lt;/span&gt;
and the knowledge that
&lt;span class=&#34;math display&#34; id=&#34;eq:stein&#34;&gt;\[\begin{equation}
\mathbb{E}_{q} [\mathcal{T}_{q} \boldsymbol{f}(\boldsymbol{x})] = 0 \tag{1}
\end{equation}\]&lt;/span&gt;
if, for &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{f} \in \mathcal{F}^d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}^d\)&lt;/span&gt; is a &lt;em&gt;Stein class&lt;/em&gt; of functions (note that both the expectation and Stein operator are with respect to the density &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;). These three equations form the basis for a lot of unnormalised density estimation, thus it makes sense that we want to use them when developing a new method.&lt;/p&gt;
&lt;p&gt;Instead of minimising the score matching divergence like &lt;em&gt;TruncSM&lt;/em&gt;, we want to construct a discrepancy based on Minimum Stein discrepancies &lt;a href=&#34;#barp2019&#34;&gt;[3]&lt;/a&gt;. If we want to make the two densities, &lt;span class=&#34;math inline&#34;&gt;\(p_{\boldsymbol{\theta}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, as close to each other as possible, we would want to minimise &lt;a href=&#34;#eq:stein&#34;&gt;(1)&lt;/a&gt;, since if it equals zero, then the two densities are equal. We also go one step further than that, by making this problem &lt;em&gt;as challenging as possible&lt;/em&gt; by including a maximisation (supremum) over the function class also:
&lt;span class=&#34;math display&#34; id=&#34;eq:minsup&#34;&gt;\[\begin{equation}
\min_{\boldsymbol{\theta}} \sup_{\boldsymbol{f} \in \mathcal{F}^d}\mathbb{E}_{q} [\mathcal{T}_{p_{\boldsymbol{\theta}}} \boldsymbol{f}(\boldsymbol{x})]. \tag{2}
\end{equation}\]&lt;/span&gt;
If we let &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}^d\)&lt;/span&gt; be an reproducing kernel Hilbert space (RKHS), then &lt;a href=&#34;#eq:minsup&#34;&gt;(2)&lt;/a&gt; can be evaluated exactly. This is called Kernelised Stein Discrepancy (KSD) &lt;a href=&#34;#ksd&#34;&gt;[4]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tksd-how-does-he-do-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;TKSD: How &lt;em&gt;does&lt;/em&gt; he do it?&lt;/h1&gt;
&lt;p&gt;Well, we described above what we &lt;em&gt;want&lt;/em&gt; to use, but we can’t &lt;em&gt;actually&lt;/em&gt; use it. All because of that pesky truncation. The issue is due to &lt;a href=&#34;#eq:stein&#34;&gt;(1)&lt;/a&gt; &lt;em&gt;not actually holding&lt;/em&gt; when the density is truncated in a way which we do not know (recall the aim of this project is to be able to estimate the density when we do not have an exact form of the truncation boundary, and instead access it through a set of points). The actual cause is complicated, but involves the derivation of &lt;a href=&#34;#eq:stein&#34;&gt;(1)&lt;/a&gt;, and a boundary condition on an integration by parts not holding when the density is truncated. So, we have to do something slightly different.&lt;/p&gt;
&lt;p&gt;Two lemmas, one proposition, one remark and one final theorem later, we get the following:
&lt;span class=&#34;math display&#34; id=&#34;eq:approxstein&#34;&gt;\[\begin{equation}
\mathbb{E}_{q} [ \mathcal{T}_{q} \tilde{\boldsymbol{g}}(\boldsymbol{x}) ] = O_P(\varepsilon_m), \tag{3}
\end{equation}\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\boldsymbol{g}} \in \mathcal{G}^d_{0, m}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}^d_{0, m}\)&lt;/span&gt; is basically a set of functions which we optimise over (similar to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{F}^d\)&lt;/span&gt; above), but also include a constraint on a &lt;em&gt;finite set of boundary points&lt;/em&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;, such that &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\boldsymbol{g}}(\boldsymbol{x}&amp;#39;) = 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}&amp;#39;\)&lt;/span&gt; in this finite set. This constraint enables the above equation to hold!&lt;/p&gt;
&lt;p&gt;Note that &lt;a href=&#34;#eq:approxstein&#34;&gt;(3)&lt;/a&gt; is not an exact analogue of &lt;a href=&#34;#eq:stein&#34;&gt;(1)&lt;/a&gt; from before, but instead, &lt;span class=&#34;math inline&#34;&gt;\(O_P(\varepsilon_m)\)&lt;/span&gt; means that it &lt;em&gt;decreases towards zero&lt;/em&gt; as &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; increases. I like to think of this as similar to sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; in most of statistics. Our accuracy increases as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; does, and in this case the same can be said of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;🚨🚨 &lt;strong&gt;Caution: Long Equation Ahead&lt;/strong&gt; 🚨🚨&lt;/p&gt;
&lt;p&gt;We can minimise in the same way as &lt;a href=&#34;#eq:minsup&#34;&gt;(2)&lt;/a&gt;. Two theorems and a long analytic solution later we obtain our objective function,
&lt;span class=&#34;math display&#34;&gt;\[
\sum^d_{l=1} \mathbb{E}_{\boldsymbol{x} \sim q} \mathbb{E}_{\boldsymbol{y} \sim q} \left[ u_l(\boldsymbol{x}, \boldsymbol{y}) - \mathbf{v}_l(\boldsymbol{x})^\top(\mathbf{K}&amp;#39;)^{-1}\mathbf{v}_l(\boldsymbol{y}) \right]
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
u_l(\boldsymbol{x}, \boldsymbol{y}) &amp;amp;=\psi_{p, l}(\boldsymbol{x})\psi_{p, l}(\boldsymbol{y})k(\boldsymbol{x},\boldsymbol{y}) +\psi_{p, l}(\boldsymbol{x}) \partial_{y_l}k(\boldsymbol{x}, \boldsymbol{y}) \nonumber \\
&amp;amp;\qquad \qquad+\psi_{p, l}(\boldsymbol{y}) \partial_{x_l}k(\boldsymbol{x}, \boldsymbol{y}) + \partial_{x_l}\partial_{y_l}k(\boldsymbol{x}, \boldsymbol{y}), \label{eq:ul}
\end{align}\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{v}_l(\boldsymbol{z}) =\psi_{p, l}(\boldsymbol{z}) \boldsymbol{\varphi}_{\boldsymbol{z}, \mathbf{x}&amp;#39;}^\top+ (\partial_{z_l}\boldsymbol{\varphi}_{\boldsymbol{z}, \mathbf{x}&amp;#39;})^\top\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\varphi}_{\boldsymbol{z}, \mathbf{x}&amp;#39;} = [ k(\boldsymbol{z}, \boldsymbol{x}_1&amp;#39;), \dots, k(\boldsymbol{z}, \boldsymbol{x}_m&amp;#39;) ]\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the kernel function associated with the RKHS &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}^d_{0, m}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\phi}_{\mathbf{x}&amp;#39;} = [k(\boldsymbol{x}_1&amp;#39;, \cdot), \dots, k(\boldsymbol{x}_m&amp;#39;, \cdot)]^\top\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{K}&amp;#39; = \boldsymbol{\phi}_{\mathbf{x}&amp;#39;}\boldsymbol{\phi}_{\mathbf{x}&amp;#39;}^\top\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yes this is quite a lot. No it is not important to understand every detail.&lt;/strong&gt; The key takeaway is that we have a loss function, consisting only of linear algebra operations, which we can minimise to obtain a truncated density estimate when the boundary is not known fully! 🎉🎉🎉&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(There are also two assumptions for one final theorem which proves this is a consistent estimator. You think this sounds like a lot of theorems? This is only mild, as far as statistics papers go.)&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;finally-something-interesting-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Finally something interesting, results!&lt;/h1&gt;
&lt;p&gt;I know, I know, you must be thinking “Is the estimation error across a range of experiments comparable to previous implementations of truncated density estimators considering the use of an approximate set of boundary points instead of an exact functional form?”.&lt;/p&gt;
&lt;p&gt;Or maybe you are just thinking “Is it better than the state-of-the-art?”. Same question, really. The answer is yes, it does pretty well.&lt;/p&gt;
&lt;div id=&#34;simulation-study&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation Study&lt;/h2&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;l2_d_approximate.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 1&lt;/strong&gt;: Simple simulation experiment with &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt; ball truncation.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;This plot shows mean estimation error over 64 trials in a simple task of estimating the mean of Gaussian distribution truncated within a &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt; ball, whilst varying the dimension &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. We compare TKSD to &lt;em&gt;TruncSM&lt;/em&gt; under two scenarios, the exact scenario is where &lt;em&gt;TruncSM&lt;/em&gt; has access to the explicit boundary formulation, and the approximate scenario is where it only has access to a finite number of samples on the boundary - the same samples we give to TKSD. Overall, TKSD trades blows with &lt;em&gt;TruncSM (exact)&lt;/em&gt;, and does significantly better than &lt;em&gt;TruncSM (approximate)&lt;/em&gt;, even though it is given the exact same information. &lt;strong&gt;So with less information, TKSD still matches the state-of-the-art method.&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;l1_d_approximate.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 2&lt;/strong&gt;: Simple simulation experiment with &lt;span class=&#34;math inline&#34;&gt;\(\ell_1\)&lt;/span&gt; ball truncation.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;This second plot shows the same experiment setup but for truncation of the &lt;span class=&#34;math inline&#34;&gt;\(\ell_1\)&lt;/span&gt; ball instead of the &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt; ball. We also plot runtime for all methods. Similar to the last experiment, TKSD and &lt;em&gt;TruncSM (exact)&lt;/em&gt; have comparable errors across all dimensions. The main message in this example is how &lt;em&gt;long&lt;/em&gt; it takes &lt;em&gt;TruncSM (exact)&lt;/em&gt; to run, because analytically calculating the functional boundary for high dimension &lt;span class=&#34;math inline&#34;&gt;\(\ell_1\)&lt;/span&gt; balls is costly, combinatorically costly with &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, in fact. &lt;em&gt;TruncSM (approximate)&lt;/em&gt; is, like before, not very good, even though it is cheaper to run. Both instances of &lt;em&gt;TruncSM&lt;/em&gt; have issues, whereas TKSD seems to be the superior option.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;mixture_ex.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 3&lt;/strong&gt;: More complicated simulation experiment with &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt; ball truncation on a mixed Gaussian distribution.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;mixture.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 4&lt;/strong&gt;: Mixed Gaussian experiment results, varying across sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, and number of mixture modes (amount of means estimated at once).
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;The next set of experiments contains a more complex setup, which is estimating multiple modes of a mixed Gaussian distribution. This is a similar experiment setup to before, except we are estimating 2, 3 and 4 means of a Gaussian at the same time. Figure 3 shows the experiment visually; as we vary the number of mixture modes, the distribution becomes more complex and thus harder to estimate accurately.&lt;/p&gt;
&lt;p&gt;Figure 4 shows the mean estimation error across 64 trials for TKSD and &lt;em&gt;TruncSM (exact)&lt;/em&gt;. We vary the number of mixture modes (left) from 2-4, and measure how that changes the error across both methods. We also fix the number of mixture modes as 2, and vary sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; (right), and also compare the error. Across both experiments, TKSD is Ivan Drago, and &lt;em&gt;TruncSM&lt;/em&gt; is Apollo Creed (sorry). &lt;strong&gt;TKSD has a significantly smaller error than &lt;em&gt;TruncSM&lt;/em&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Example&lt;/h2&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;regression.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 5&lt;/strong&gt;: Example of using TKSD for regression on simulated data (left) and a real world example (right).
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Let’s look at one specific example before we go, a simple linear regression. Since TKSD is a density estimation method, we can use it to estimate parameters of the (conditional) mean of a Normal distribution, given some feature variables. Truncation happens in the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; domain, so that all of our data is truncated according to conditions on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. In the first plot, we simulate a regression setting where we know the true parameters and can see the untruncated data. We pretend to only observe &lt;span class=&#34;math inline&#34;&gt;\(y_i \geq 5\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\forall i = 1,\dots, n\)&lt;/span&gt;, and fit the model to the remaining data. When comparing the TKSD fit to a naive least squares implementation (MLE) which does not account for truncation, you can clearly see the difference that accounting for this simple truncation makes in our regression line.&lt;/p&gt;
&lt;p&gt;The second plot is an experiment on a real-world dataset, given by UCLA: Statistical Consulting Group &lt;a href=&#34;#data&#34;&gt;[5]&lt;/a&gt;. This dataset contains student test scores in a school for which the acceptance threshold is 40/100, and therefore the response variable (the
test scores) are truncated below by 40 and above by 100. Since no scores get close to 100, we only consider one-sided
truncation at &lt;span class=&#34;math inline&#34;&gt;\(y = 40\)&lt;/span&gt;. The aim of the regression is to model the response variable, the test scores, based on a single covariate of each students’ corresponding score on a different test. We see very clearly in this example that accounting for this truncation seems to give a better fit than a regular least squares solution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;This work has taken up the majority of my PhD, around 2 years. It is more complicated than I have given it credit for in this post, and please do read the full paper if you want more detail. Even with all the detail, it is not a work that would normally take 2 years. It started as a way of extending the previous implementation we developed in &lt;em&gt;TruncSM&lt;/em&gt;, to try and adaptively solve for what we were calling a ‘boundary function’. It became clear that score matching was holding us back, and then we kept having to add extra constraints and details to an implementation around Stein discrepancies. Amongst loads of different ideas, things also kept going wrong, so it is a great relief to see this research finished, working, and even performing extremely well, not to mention being accepted to ICML!&lt;/p&gt;
&lt;p&gt;Anyway, why would you be interested in TKSD in general? If you care about truncated densities, and want something that is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;adaptive to the dataset at hand&lt;/li&gt;
&lt;li&gt;requires no prior knowledge about the boundary, except being able to obtain samples from it&lt;/li&gt;
&lt;li&gt;performs better in more complicated scenarios&lt;/li&gt;
&lt;li&gt;has a nice theoretical and empirical results&lt;/li&gt;
&lt;li&gt;is an acronym&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;then look no further than TKSD!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jmlr.org/papers/volume23/21-0218/21-0218.pdf&#34;&gt;[1]&lt;/a&gt; &lt;a id=&#34;truncsm&#34;&gt;&lt;/a&gt;Liu, S., Kanamori, T., and Williams, D. J. Estimating density models with truncation boundaries using score matching. Journal of Machine Learning Research, 23(186):1–38, 2022.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf&#34;&gt;[2]&lt;/a&gt; &lt;a id=&#34;scorematching1&#34;&gt;&lt;/a&gt;Hyvärinen, A. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24):695–709, 2005.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2019/file/ba7609ee5789cc4dff171045a693a65f-Paper.pdf&#34;&gt;[3]&lt;/a&gt; &lt;a id=&#34;barp2019&#34;&gt;&lt;/a&gt; Barp, A., Briol, F.-X., Duncan, A., Girolami, M., and Mackey, L. Minimum stein discrepancy estimators. In Advances in Neural Information Processing Systems, volume 32, 2019.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v48/chwialkowski16.pdf&#34;&gt;[4]&lt;/a&gt;&lt;a id=&#34;ksd&#34;&gt;&lt;/a&gt; Chwialkowski, K., Strathmann, H., and Gretton, A. A kernel test of goodness of fit. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 2606–2615. PMLR, 2016&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.oarc.ucla.edu/stata/dae/truncated-regression/&#34;&gt;[5]&lt;/a&gt; &lt;a id=&#34;data&#34;&gt;&lt;/a&gt; UCLA: Statistical Consulting Group. Truncated regression — stata data analysis examples. URL &lt;a href=&#34;https://stats.oarc.ucla.edu/stata/dae/truncated-regression/&#34; class=&#34;uri&#34;&gt;https://stats.oarc.ucla.edu/stata/dae/truncated-regression/&lt;/a&gt;. Accessed March 17, 2023.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Natural Language Embeddings of A Game of Thrones</title>
      <link>https://dannyjameswilliams.co.uk/post/asoiaf/</link>
      <pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/asoiaf/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/post/asoiaf/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;For those of you familiar with either HBO’s Game of Thrones, or George RR Martin’s A Song of Ice and Fire (ASOIAF), you will probably be aware of the vast differences between each character and the depth of each storyline. For those of you who aren’t familiar; each book chapter is written from the perspective of a different character, and George RR Martin writes each character in a distinct and unique way. A chapter written from the perspective of Eddard Stark, an aged and grizzled war veteran, will be portrayed vastly different to a chapter from the perspective of his son, Bran Stark, a 9 year old innocent ‘sweet summer child’.&lt;/p&gt;
&lt;p&gt;The stories surrounding all of the characters span continents, explore diverse themes and face different problems, and so we would expect there to be intrinsic differences between the language used for all of these different perspectives. &lt;em&gt;Can we use Natural Language Processing (NLP) to show how George RR Martin writes each of these characters?&lt;/em&gt; More specifically, is there actually a &lt;em&gt;mathematical difference&lt;/em&gt; between the chapters, grouped by character, based on the language alone?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A note on spoilers&lt;/strong&gt;: I am not including any excerpts from either the books or the show, and do not discuss the story. However, you will be able to see how many chapters are included for each character, as well as if any characters &lt;em&gt;stop&lt;/em&gt; having point-of-view chapters after a specific book.&lt;/p&gt;
&lt;div id=&#34;how-are-we-going-to-use-nlp-for-game-of-thrones&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How are we going to use NLP for Game of Thrones?&lt;/h2&gt;
&lt;p&gt;To show how characters are different amongst the ASOIAF books, I will use &lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;BERT&lt;/a&gt;, which is a NLP model built by Google, and an extremely powerful tool which can output &lt;em&gt;sentence embeddings&lt;/em&gt;. Put simply, a sentence embedding is a sequence of words, e.g. a sentence, which has been converted to a very high-dimensional mathematical vector. The vector itself won’t have any meaning to you or I on its own, but it has context relative to other sentences. For example, if two different sentence embeddings are very far away from each other, we can probably infer that the two original sentences are quite different.&lt;/p&gt;
&lt;p&gt;BERT can achieve this as it has been trained on a large corpus of language data: &lt;a href=&#34;https://arxiv.org/pdf/1506.06724.pdf&#34;&gt;a dataset of books&lt;/a&gt; and the full English Wikipedia. One way in which BERT trains is by &lt;em&gt;masked language modelling&lt;/em&gt;, which means it tries to predict random words in the dataset using the ‘transformer’ architecture, which is an extremely clever way of incorporating left-to-right and right-to-left directionality in a sentence so that the full context of the word is integrated. By learning where words come in the context of a sentence, BERT learns a lot about the English language. Whilst we aren’t trying to predict any words for our task, we will use the information that BERT learned whilst training as our emebeddings.&lt;/p&gt;
&lt;p&gt;Fortunately for us, BERT is made completely open-source and free to use via &lt;a href=&#34;https://huggingface.co/bert-base-uncased&#34;&gt;Huggingface&lt;/a&gt;, a collection of NLP models, datasets, and more. We can extract the embeddings from BERT and visualise them for each chapter to see if there is a strong degree of separation, which would indicate key differences in the language for different characters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asoiaf-embeddings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ASOIAF Embeddings&lt;/h1&gt;
&lt;p&gt;We can give BERT a sentence from ASOIAF, and it will output a relevant vector which gives information about this sentence. Repeating this for every sentence in the ASOIAF book series, we can obtain the high-dimensional sentence embeddings from each chapter of ASOIAF, by grouping by chapter and taking the average embedding across sentences.&lt;/p&gt;
&lt;p&gt;The embedding dimension output given by the base BERT model is 768, which we definitely cannot visualise. For this reason I have used &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/&#34;&gt;UMAP&lt;/a&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, a low-dimension projection method, to reduce the dimension to 3 so that we can visualise it.&lt;/p&gt;
&lt;div id=&#34;separation-by-character&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Separation by Character&lt;/h2&gt;
&lt;p&gt;Below I have plotted the (now 3D) embeddings in an interactive scatter plot, which you can rotate and move around. You can hover over each point to see the point-of-view character for each chapter and the corresponding book, as well as click on character names on the legend to remove them.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;750&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;100%&#34; src=&#34;//plotly.com/~dannyjameswilliams/169.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Mean BERT sentence embeddings of chapters in ASOIAF, split by character (with each book labelled), restricted to the 10 most frequently occuring point-of-view characters. The dimension was reduced from 768 to 3 using UMAP. &lt;a href=&#34;https://chart-studio.plotly.com/~dannyjameswilliams/169/#/&#34;&gt;See in fullscreen here&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;It is important to note that &lt;em&gt;no information about the classes (chapters/characters) was used at any point&lt;/em&gt;, so any structure we can see that separates the different characters or chapters is purely based on the language alone. The factors that could influence this are, for example: word choice, writing style, sentence length, and more.&lt;/p&gt;
&lt;p&gt;So what can we infer from this? The key aspects we are looking for are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How clustered are chapters from the same character?&lt;/li&gt;
&lt;li&gt;How separated are chapters from different characters?&lt;/li&gt;
&lt;li&gt;Are similar characters close to one another?&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;my-interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;My Interpretation&lt;/h3&gt;
&lt;p&gt;From what I can see, the honourable and consistent Eddard Stark occupies a distinct region of the plot, and doesn’t stray far from it, but he is joined by other chapters from the first book. Daenerys’s storyline is mostly separated from the rest of the characters, so it makes sense that her chapters are grouped together and distinct in the plot. Other characters, such as Arya, seem to be uniquely identifiable due to being separated from the other clusters. The questionably lovable dwarf, Tyrion, has a plotline which spans battlefields, court intrigue, romance, death, and more. This seems to be shown here by his embeddings being spread across the entire space, representing the large variability in his changing viewpoints and scenarios.&lt;/p&gt;
&lt;p&gt;Surprisingly, I expected the child characters, most notably Sansa and Bran, to occupy a distinct region because of their naive and childlike viewpoints, but their distributions aren’t too different from most other characters. In fact, Jon’s and Bran’s chapters seem to exist in the same regions of the plot, which could indicate a similarity in the characters, or at least how the characters are written.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;separation-by-book&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Separation by Book&lt;/h2&gt;
&lt;p&gt;There seems to be a divide based on the book which each chapter is written in. Just by switching the labels (and removing the top 10 character restriction), we can inspect the same plot with a different angle.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;750&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;100%&#34; src=&#34;//plotly.com/~dannyjameswilliams/171.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Mean BERT sentence embeddings of chapters in ASOIAF, split by book. The dimension was reduced from 768 to 3 using UMAP. &lt;a href=&#34;https://chart-studio.plotly.com/~dannyjameswilliams/171/#/&#34;&gt;See in fullscreen here&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;The divide between classes is far more clear in this case&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; - books 2 and 3 (ACOK and ASOS respectively), are far, far, different from the other books in the series. The first, fourth and fifth books (AGOT, AFFC and ADWD respectively), are more closely related, with the first book being in a more distinctive class. Books 4 and 5 were written at the same time, which might explain their embeddings being so intertwined.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Thoughts&lt;/h1&gt;
&lt;p&gt;It is interesting how we can &lt;em&gt;mathematically see the difference in language&lt;/em&gt; based on the book or the character. There is a clear mathematical distinction between writing styles used for different characters, and we have even shown the evolution of George RR Martin’s prose over the course of writing the ASOIAF series.&lt;/p&gt;
&lt;p&gt;We can see the potential usefulness of these BERT embeddings, and they have far more use outside of plotting them to see their groups. We could’ve calculated the distance between embeddings to see exactly which characters are the most different or the most similar. We could use the embeddings themselves in a data science application; for example, how well can we classify which character is being written about based on only language?&lt;/p&gt;
&lt;p&gt;Existing research in NLP has given us the avenue to do all of this. It is an extremely interesting field, and is constantly developing. The methods we used here are free, and open to anyone for experimenting with. What other applications do you think would be cool to see? If you would like to ask any questions or have any discussion, &lt;a href=&#34;https://dannyjameswilliams.co.uk/#contact&#34;&gt;see my contact page&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The UMAP transform was fit with hyperparameters &lt;code&gt;min_dist = 0&lt;/code&gt; and &lt;code&gt;n_neighbors = 30&lt;/code&gt;, chosen by trial and error, to try to separate the classes as much as possible. All other values were kept as default in &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/api.html#umap&#34;&gt;this function&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For completeness, here are the initialisms used for each book. AGOT: A Game of Thrones, ACOK: A Clash of Kings, ASOS: A Storm of Swords, AFFC: A Feast for Crows, ADWD: A Dance with Dragons&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How can we do data science without all of our data?</title>
      <link>https://dannyjameswilliams.co.uk/post/nodata/</link>
      <pubDate>Thu, 24 Jun 2021 12:19:03 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/nodata/</guid>
      <description>  

&lt;script src=&#34;https://dannyjameswilliams.co.uk/post/nodata/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;This research serves as a pre-cursor to the work that I am carrying out as part of my PhD, and involves unnormalised and truncated probability density estimation. This is a field which I find extremely interesting, and I believe that it has far reaching applications not just across data science and machine learning, but across many other disciplines as well. This blog post is an attempt at a more accessible explanation behind some of the core concepts in &lt;a href=&#34;https://arxiv.org/pdf/1910.03834.pdf&#34;&gt;this paper (in pre-print)&lt;/a&gt;, by Song Liu, Takafumi Kanamori and myself.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Firstly, consider the locations of reported crimes in Chicago below, and think about where the ‘centres’ of these reported crimes are.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/chicago.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 1:&lt;/strong&gt; Homicide locations inside the city of Chicago recorded in 2008.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Now think about where typical statistical estimation methods would put these crime ‘centres’. How do you think these would differ to what the ‘true’ centres are? Something like &lt;em&gt;maximum likelihood&lt;/em&gt; estimation may be slightly incorrect in some way, because the reported crimes get cut off, or &lt;em&gt;truncated&lt;/em&gt; beyond the city’s borders.&lt;/p&gt;
&lt;p&gt;The dataset is incomplete; crimes do not magically stop where we decide the city ends, but data collection does.&lt;/p&gt;
&lt;p&gt;This is an example of a more general problem in statistics named &lt;strong&gt;truncated probability density estimation&lt;/strong&gt;: How do we estimate the parameters of a statistical model when data are not fully observed, and are cut off by some artificial boundary?&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;A statistical model (a probability density function involving some parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt;) is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\boldsymbol{x}; \boldsymbol{\boldsymbol{\theta}}) = \frac{1}{Z(\boldsymbol{\boldsymbol{\theta}})} \bar{p}(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}}), \qquad Z(\boldsymbol{\boldsymbol{\theta}}) = \int \bar{p}(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}})d\boldsymbol{x}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is made up of two components: &lt;span class=&#34;math inline&#34;&gt;\(\bar{p}(\boldsymbol{x}; \boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt;), the unnormalised part of the model, which is known analytically; and &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt;, the normalising constant. Over complicated boundaries such as city borders, &lt;em&gt;this normalising constant cannot be calculated directly&lt;/em&gt;, which is why something like maximum likelihood would fail in this case.&lt;/p&gt;
&lt;p&gt;When we perform estimation, we aim to find &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt; which makes our model density, &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x}; \boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt;, as closely resemble a theoretical data density, &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;, as possible. Usually, we can estimate &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\boldsymbol{\theta}}\)&lt;/span&gt; by trying to minimise some measure of difference between the &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x};\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt;. But we cannot do this while &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt; is unknown!&lt;/p&gt;
&lt;p&gt;Most methods in this regard opt for a numerical approximation to integration, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&#34;&gt;MCMC&lt;/a&gt;. But these methods are usually very slow and computationally heavy. Surely there must be a better way?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-recipe-for-a-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Recipe for a Solution&lt;/h2&gt;
&lt;div id=&#34;ingredient-1-score-matching&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ingredient #1: Score Matching&lt;/h4&gt;
&lt;p&gt;A novel estimation method called score matching enables estimation even when the normalising constant, &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\boldsymbol{\theta}})\)&lt;/span&gt;, is unknown. Score matching begins by taking the derivative of the logarithm of the probability density function, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta}) = \nabla_\boldsymbol{x} \log \bar{p}(\boldsymbol{x};\boldsymbol{\theta}),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which has become known as the score function. When taking the derivative, the dependence on &lt;span class=&#34;math inline&#34;&gt;\(Z(\boldsymbol{\theta})\)&lt;/span&gt; is removed. To estimate the parameter vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}\)&lt;/span&gt;, we can minimise the difference between &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x};\boldsymbol{\theta})\)&lt;/span&gt; by minimising the difference between the two score functions, &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nabla_\boldsymbol{x} \log q(\boldsymbol{x})\)&lt;/span&gt;. One such distance measure is the expected squared distance, so that score matching aims to minimise&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} [\| \nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta}) - \nabla_\boldsymbol{x} \log q(\boldsymbol{x})\|_2^2].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With this first ingredient, we have eliminated the requirement that we must know the normalising constant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ingredient-2-a-weighting-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ingredient #2: A Weighting Function&lt;/h4&gt;
&lt;p&gt;Heuristically, we can imagine our weighting function should vary with how close a point is to the boundary. To satisfy score matching assumptions, we require that this weighting function &lt;span class=&#34;math inline&#34;&gt;\(g(\boldsymbol{x})\)&lt;/span&gt; must have the property that &lt;span class=&#34;math inline&#34;&gt;\(g(\boldsymbol{x}&amp;#39;)\)&lt;/span&gt; = 0 for any point &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}&amp;#39;\)&lt;/span&gt; on the boundary. A natural candidate would be the Euclidean distance from a point &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{x}\)&lt;/span&gt; to the boundary, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
g(\boldsymbol{x}) = \|\boldsymbol{x} - \tilde{\boldsymbol{x}}\|_2, \qquad \tilde{\boldsymbol{x}} = \text{argmin}_{\boldsymbol{x}&amp;#39;\text{ in boundary}} \|\boldsymbol{x} - \boldsymbol{x}&amp;#39;\|_2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This easily satisfies our criteria. The distance is going to be exactly zero on the boundary itself, and will approach zero the closer the points are to the edges.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ingredient-3-any-statistical-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ingredient #3: Any Statistical Model&lt;/h4&gt;
&lt;p&gt;Since we do not require knowledge of the normalising constant through the use of score matching, we can choose any probability density function &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x}; \boldsymbol{\theta})\)&lt;/span&gt; that is appropriate for our data. For example, if we are modelling count data, we may choose a Poisson distribution. If we have some sort of centralised location data, such as in the Chicago crime example in the introduction, we may choose a multivariate Normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;combine-ingredients-and-mix&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Combine Ingredients and Mix&lt;/h3&gt;
&lt;p&gt;We aim to minimise the expected distance between the score functions, and we weight this by our function &lt;span class=&#34;math inline&#34;&gt;\(g(\boldsymbol{x})\)&lt;/span&gt;, to give&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\boldsymbol{\theta}} \mathbb{E} [g(\boldsymbol{x}) \| \nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta}) - \nabla_\boldsymbol{x} \log q(\boldsymbol{x})\|_2^2].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The only unknown in this equation now is the data density &lt;span class=&#34;math inline&#34;&gt;\(q(\boldsymbol{x})\)&lt;/span&gt; (if we knew the true data density, there would be no point in estimating it with &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x};\boldsymbol{\theta})\)&lt;/span&gt;). However, we can rewrite this equation using integration by parts as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\boldsymbol{\theta}} \mathbb{E} \big[ g(\boldsymbol{x}) [\|\nabla_\boldsymbol{x} \log p(\boldsymbol{x}; \boldsymbol{\theta})\|_2^2 + 2\text{trace}(\nabla_\boldsymbol{x}^2 \log p(\boldsymbol{x}; \boldsymbol{\theta}))] + 2 \nabla_\boldsymbol{x} g(\boldsymbol{x})^\top \nabla_\boldsymbol{x} \log p(\boldsymbol{x};\boldsymbol{\theta})\big].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This can be numerically minimised, or if &lt;span class=&#34;math inline&#34;&gt;\(p(\boldsymbol{x}; \boldsymbol{\theta})\)&lt;/span&gt; is in the exponential family, analytically minimised to obtain estimates for &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\theta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;artificial-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Artificial Data&lt;/h3&gt;
&lt;p&gt;As a sanity check for testing estimation methods, it is often reassuring to perform some experiments on simulated data before moving to real world applications. Since we know the true parameter values, it is possible to calculate how far our estimated values are from their true counterparts, thus giving a way to measure estimation accuracy.&lt;/p&gt;
&lt;p&gt;Pictured below in Figure 2 are two experiments where data are simulated from a multivariate normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\mu}^* = [1, 1]\)&lt;/span&gt; and known variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1\)&lt;/span&gt;. Our aim is to estimate the parameter &lt;span class=&#34;math inline&#34;&gt;\(\hat{\boldsymbol{\mu}}\)&lt;/span&gt; to be close to &lt;span class=&#34;math inline&#34;&gt;\([1,1]\)&lt;/span&gt;. The red crosses in the image are the true means at &lt;span class=&#34;math inline&#34;&gt;\([1,1]\)&lt;/span&gt;, and the red dots are the estimates given by truncated score matching.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/mvn_l2.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 2 (a)&lt;/strong&gt;: Points are only visible around a unit ball from [0,0].
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/mvn_box.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 2 (b)&lt;/strong&gt;: Points are only visible inside a box to the left of the mean.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;These figures clearly indicate that in this basic case, this method is giving sensible results. Even by ignoring most of our data, as long as we formulate our problem correctly, we can still get accurate results for estimation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chicago-crime&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Chicago Crime&lt;/h3&gt;
&lt;p&gt;Back to our motivating example, where the task is to estimate the ‘centres’ of reported crime in the city of Chicago, given the longitudes and latitudes of homicides in 2008. Our specification changes from the synthetic data somewhat:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;from the original plots, it seems like there are two centres, so the statistical model we choose is a 2-dimensional mixed multivariate Normal distribution;&lt;/li&gt;
&lt;li&gt;the variance is no longer known, but to keep estimation straightforward, the standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is fixed so that &lt;span class=&#34;math inline&#34;&gt;\(2\sigma\)&lt;/span&gt; roughly covers the ‘width’ of the city (&lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.06^\circ\)&lt;/span&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below I implement estimation for the mean for both truncated score matching and maximum likelihood.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;../../img/post/nodata/chicago_estimate.png&#34; alt=&#34;&#34; /&gt;
&lt;figcaption&gt;
&lt;strong&gt;Figure 3:&lt;/strong&gt; Homicides in Chicago recorded in 2008, with the means estimated by truncated score matching (truncSM) and maximum likelihood estimation (MLE).
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Truncated score matching has placed its estimates for the means in similar, but slightly different places than standard MLE. Whereas the MLE means are more central to the truncated dataset, the truncated score matching estimates are placed closer to the outer edges of the city. For the region of crime in the top half of the city, the data are more ‘tightly packed’ around the border – which is a property we expect of Normally distributed data closer to its mean.&lt;/p&gt;
&lt;p&gt;Whilst we don’t have any definitive ‘true’ mean to compare it to, we could say that the truncSM mean is closer to what we would expect than the one estimated by MLE.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that this result does not reflect the true likelihood of crime in a certain neighbourhood, and has only been presented to provide a visual explanation of the method. The actual likelihood depends on various characteristics in the city that are not included in our analysis, &lt;a href=&#34;https://link.springer.com/article/10.1007/s41109-019-0239-8&#34;&gt;see here for more details&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;Score matching is a powerful tool, and by not requiring any form of normalising constant, enables the use of some more complicated models. Truncated probability density estimation is just one such example of an intricate problem which can be solved with this methodology, but it is one with far reaching and interesting applications.&lt;/p&gt;
&lt;p&gt;Whilst this blog post has focused on location-based data and estimation, truncated probability density estimation could have a range of applications. For example, consider disease/virus modelling such as the COVID-19 pandemic: The true number of cases is obscured by the number of tests that can be performed, so the density evolution of the pandemic through time could be fully modelled with incorporation of this constraint using this method. Other applications as well as extensions to this method will be fully investigated in my future PhD projects.&lt;/p&gt;
&lt;div id=&#34;further-reading-and-bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further Reading and Bibliography&lt;/h3&gt;
&lt;p&gt;Firstly, &lt;a href=&#34;https://github.com/dannyjameswilliams/truncsm-blog&#34;&gt;here is the github page&lt;/a&gt; where you can reproduce all the plots made available in this blog post, as well as some generic code for the implementation of truncated score matching.&lt;/p&gt;
&lt;p&gt;The original score matching reference:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf&#34;&gt;Estimation of Non-Normalized Statistical Models by Score Matching, Aapo Hyvärinen (2005)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper from which this blog post originates:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.03834.pdf&#34;&gt;Estimating Density Models with Truncation Boundaries using Score Matching. Song Liu, Takafumi Kanamori, &amp;amp; Daniel J. Williams (2021)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How random are you?</title>
      <link>https://dannyjameswilliams.co.uk/post/randomchoices/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/randomchoices/</guid>
      <description>


&lt;p&gt;The notion of &lt;em&gt;true random&lt;/em&gt; has perplexed scientists and statisticians for decades, if not longer. But there’s one process that we still don’t know much about - the human brain. What is it that causes our decisions to be made? When we are asked to ‘pick a random number between 1 and 10’, how ‘random’ is the number we give? Is it some complicated, deterministic signal of neurons in our brain? Or is it actually random?&lt;/p&gt;
&lt;p&gt;The following analysis and results come from a survey collected online, which obtained &lt;span class=&#34;math inline&#34;&gt;\(n = 2190\)&lt;/span&gt; participants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you’re in a hurry, or don’t fancy reading this article, &lt;/strong&gt;&lt;a href=&#34;infographic.png&#34;&gt;&lt;strong&gt;check out the handy infographic&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;pick-a-random-number-between-1-and-10-twice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pick a random number between 1 and 10, twice&lt;/h2&gt;
&lt;p&gt;This is perhaps the most basic question to ask, and one that is very visually interpretable. This question was asked in two distinct ways, pictured below:&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;img/questionA.png&#34; width=&#34;642&#34; style=&#34;background-color: #9ecff7; padding:2px; display: inline-block;&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Question A.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;img/questionB.png&#34; width=&#34;429&#34; style=&#34;background-color: #9ecff7; padding:2px; display: inline-block;&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Question B.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;These questions are asked with the following hypothesis: &lt;em&gt;does the answer input being on a scale change how people answer the question?&lt;/em&gt; In another way, if you are picking a random number, and you can see the numbers laid out in order in front of you, are you more likely to pick a number which is more central? Let’s look at the results to find out.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;200&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;49%&#34; src=&#34;//plotly.com/~dannyjameswilliams/15.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;iframe height=&#34;200&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;49%&#34; src=&#34;//plotly.com/~dannyjameswilliams/17.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Frequencies of choices between 1 and 10 for question style A (left) and question style B (right). Included is a line of the expected value &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}(\text{no. of choices}) = np\)&lt;/span&gt;, plotted within a &lt;span class=&#34;math inline&#34;&gt;\(\pm 1.96 \cdot \sqrt{\sigma}\)&lt;/span&gt; confidence interval, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the binomial variance.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Surprisingly, both results are reasonably similar. Some statistics of these are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;4 is the most frequent number in both cases&lt;/strong&gt;, different from the usual value of 7 that has been in similar surveys before. In the comments for this survey, people seemed to expect 7 to be the most picked. It is possible that since the participants were aware of previous research in the area, they deliberately would not have picked 7.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The average ‘difference from uniformity’ was around 2.1% for both types of questions&lt;/strong&gt;. This is the percentage difference from the expected value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;10.1% of people picked the same number for both questions&lt;/strong&gt;, and for true random number generation, the true value would be 10%!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The values at the edges, 1 and 10, were picked far less often&lt;/strong&gt; than values in the centre.&lt;/li&gt;
&lt;li&gt;The largest difference between questions was that &lt;strong&gt;10 was picked less in question type B than question type A&lt;/strong&gt;. A possible reason is that 10 is two digits long, which would require extra effort for question type B than for question type A.&lt;/li&gt;
&lt;li&gt;A Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; test for uniformity showed &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values where &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt;&amp;lt; 0.0001\)&lt;/span&gt;, indicating with a reasonable level of certainty that &lt;strong&gt;neither sets of answers were uniformly distributed&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So people don’t seem to be entirely random in this case, but this is only one aspect of randomness. People picked the same number for both questions the correct amount of times! How about pairs of answers, i.e. for both &lt;em&gt;answer A&lt;/em&gt; and &lt;em&gt;answer B&lt;/em&gt;?&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;350&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;100%&#34; src=&#34;//plotly.com/~dannyjameswilliams/25.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Distribution of (&lt;em&gt;answer A&lt;/em&gt; - &lt;em&gt;answer B&lt;/em&gt;), plotted with the expected Irwin–Hall distribution.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;If we subtract the answers from one another, we are essentially summing uniform random variables, which are expected to have a &lt;a href=&#34;https://en.wikipedia.org/wiki/Irwin%E2%80%93Hall_distribution&#34;&gt;Irwin-Hall distribution&lt;/a&gt;, or a triangular distribution when we are only summing two variables.&lt;/p&gt;
&lt;p&gt;Put simply, imagine you are rolling two six-sided dice. The most common sum of their values is 7, since there are more combinations that can sum to 7 than anything else. The same is true in this case, we expect the most common result of &lt;em&gt;answer A&lt;/em&gt; - &lt;em&gt;answer B&lt;/em&gt; to be 0, and then equally 1 and -1, and so on.&lt;/p&gt;
&lt;p&gt;Our distribution above actually does look similar to the expected triangular distribution (although we do have some oddities towards the centre of the triangle, likely caused by the lack of 1’s and 10’s). Does this mean human randomness isn’t too bad after all? In this aspect, we are quite good with the &lt;em&gt;pairs&lt;/em&gt; of our answers, even if our individual answers aren’t quite as good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pick-a-random-letter-from-the-alphabet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pick a random letter from the alphabet&lt;/h2&gt;
&lt;p&gt;Okay, so we have trouble with edge effects - we don’t like to pick 1s and 10s when we are trying to randomly think of a number. What if we were to randomly select a letter? There’s no real intrinsic numeric value associated to a letter (unless you count its position in the alphabet), so maybe we are better at selecting a random letter. Let’s take a look.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;300&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;90%&#34; src=&#34;//plotly.com/~dannyjameswilliams/62.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Frequency of chosen letters arranged alphabetically.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Ah, so we can’t select letters randomly either, but it doesn’t seem to depend on any sort of edge effects alphabetically. That is, people seem to pick A and Z a healthy amount. The frequencies for each letter are very far from what we’d expect if we were to have sampled these letters uniformly. What could have been affecting our judgement here? Maybe the popularity of a certain letter in the English language?&lt;/p&gt;
&lt;p&gt;Plotted below is the relationship between the &lt;em&gt;frequency at which a letter occurs in the English language&lt;/em&gt; (as a percentage) versus the &lt;em&gt;frequency it was picked in the survey&lt;/em&gt;.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;300&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;90%&#34; src=&#34;//plotly.com/~dannyjameswilliams/23.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Letter percentage in the English Language plotted against the number of times the letter was picked in the survey. The red line is a line of best fit captured from a linear regression with a cubic polynomial transform on the English language percentage.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;There is no significant relationship here, the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; value for a linear regression is very low, at &lt;span class=&#34;math inline&#34;&gt;\(R^2 \approx 0.152\)&lt;/span&gt;, meaning the model is not capturing the variability in the data. What else could be affecting how we are choosing these letters?&lt;/p&gt;
&lt;p&gt;Earlier I stated the assumption that there was no intrinsic numeric value associated to a letter, so that we would not have any trouble with edge effects skewing the results. That assumption was wrong! If you’re reading this on a computer or laptop, take a look what is just underneath your screen.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;180&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;90%&#34; src=&#34;//plotly.com/~dannyjameswilliams/21.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Frequency of chosen letters as a heatmap overlaid on a keyboard.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;It turns out that the most frequently picked letters are those that are most central on a QWERTY keyboard. This notion of picking the most central option is corroborated by the choice of number in the first question, being that people do not like to pick numbers or letters that appear on what they consider as the ‘edge’.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pick-a-random-number-between-1-and-50&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pick a random number between 1 and 50&lt;/h2&gt;
&lt;p&gt;What if our selection range is so large that these edge effects can be nullified? If we ask people to select numbers between 1 and 50, will we see an extension of what we saw in the first question? I.e. are people going to pick the most central values again (say, between 10 and 40), or will it be something else?&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;300&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;90%&#34; src=&#34;//plotly.com/~dannyjameswilliams/19.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Frequency of chosen numbers between 1 and 50.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;An interesting selection of numbers here. This range does not seem to be uniformly distributed in the slightest. Here are some facts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Against the expected value of 10%, only &lt;strong&gt;4.3% of people selected a multiple of 10&lt;/strong&gt;, whereas &lt;strong&gt;18.7% of people chose a number with a 7&lt;/strong&gt;, e.g. 17, 27 etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The lowest picked number was 30, by 0.5% of people&lt;/strong&gt;, and &lt;strong&gt;the highest picked number was 37, by 5.8% of people&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Perhaps you remember the ‘trick’ that your friend would play on you as a kid in the playground. They would ask you a series of maths questions, and then ask you to &lt;em&gt;name a vegetable&lt;/em&gt;. You would inevitably say ‘carrot’, then they’d reveal a piece of paper with the word ‘carrot’ on it. Were you truly tricked? Turns out, if you’re asked to name a vegetable, &lt;a href=&#34;http://sciencechatforum.com/viewtopic.php?p=264070&#34;&gt;most people say carrot regardless&lt;/a&gt;. It’s not a trick, when asked to name something at random, we pick an extremely common vegetable.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Analysis of the Lyrics of Kanye West</title>
      <link>https://dannyjameswilliams.co.uk/post/kanye/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/kanye/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://dannyjameswilliams.co.uk/rmarkdown-libs/forceNetwork-binding/forceNetwork.js&#34;&gt;&lt;/script&gt;


&lt;em&gt;“2020 I’mma run the whole election”&lt;/em&gt; may not sound like the words of a lyrical genius, but I ask you to &lt;em&gt;“name one genius that ain’t crazy”&lt;/em&gt;. For the duration of your read of this post, take a minute to separate an artist from their art. It is undeniable that Kanye West has had serious influence over the musical industry during his career. He has single-handedly influenced hip hop since his first album, &lt;em&gt;The College Dropout&lt;/em&gt;, and that’s not to mention &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_awards_and_nominations_received_by_Kanye_West&#34;&gt;all of his awards&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;
&lt;center style=&#34;color:#717d7e;&#34;&gt;
&lt;em&gt;&amp;quot;I woke up early this mornin’ with a new state of mind&lt;/em&gt; &lt;br&gt;
&lt;em&gt;A creative way to rhyme without usin’ nines and guns&amp;quot;&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;
Controversy aside, what makes Kanye West so influential to the hip-hop industry? His lyrics must play a part to his success. The &lt;a href=&#34;https://cloud.google.com/natural-language/&#34;&gt;Google Natural Language API&lt;/a&gt; can perform a full language analysis of Kanye West’s lyrics for free. This uses a pre-trained natural language model by Google, and the API is available for use in Python.&lt;/p&gt;
&lt;p&gt;The data were mostly obtained by using the &lt;a href=&#34;https://cran.r-project.org/web/packages/genius/index.html&#34;&gt;genius&lt;/a&gt; package in R, but missing entries were copied and pasted manually from the Genius website. The analysis is restricted to his studio albums, but not solo albums, so that his collaborations with Kid Cudi and Jay Z are included.
All albums considered are &lt;em&gt;The College Dropout&lt;/em&gt;, &lt;em&gt;Late Registration&lt;/em&gt;, &lt;em&gt;Graduation&lt;/em&gt;, &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt;, &lt;em&gt;My Beautiful Dark Twisted Fantasy&lt;/em&gt;, &lt;em&gt;Watch the Throne&lt;/em&gt;, &lt;em&gt;Yeezus&lt;/em&gt;, &lt;em&gt;The Life of Pablo&lt;/em&gt;, &lt;em&gt;ye&lt;/em&gt;, &lt;em&gt;Kids See Ghosts&lt;/em&gt; and &lt;em&gt;Jesus is King&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This article is split into four sections:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entity Analysis&lt;/li&gt;
&lt;li&gt;Sentiment Analysis&lt;/li&gt;
&lt;li&gt;Sentiment and Magnitude against Album Reception&lt;/li&gt;
&lt;li&gt;Generating a New Kanye Song&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All code used for the analysis and graphics in this post can be found in the github repository &lt;a href=&#34;https://github.com/dannyjameswilliams/kanyenet&#34;&gt;kanyenet&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;entity-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Entity Analysis&lt;/h2&gt;
&lt;p&gt;The natural language API provided by Google includes entity analysis - the identification of &lt;em&gt;entities&lt;/em&gt;, which can be interpreted as the ‘important parts’ of the text. Each song is passed individually to the API, collating the number of entities for each song. Below is a network of word connections, where the links between nodes represent words that appear in the same song. The graph is interactive, so you can scroll around and zoom in and out to see all the connections. You can click on a node to view the total number of times it appears across all songs.&lt;/p&gt;
It might take a while to load, and is best viewed on desktop. &lt;strong&gt;Content warning:&lt;/strong&gt; An effort has been made to censor offensive words, but some may have slipped through the cracks.
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;forceNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;links&#34;:{&#34;source&#34;:[0,0,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,4,5,5,5,5,5,5,5,5,5,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,9,9,9,10,10,10,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,14,14,14,14,14,14,15,15,15,15,15,15,16,16,16,16,16,16,17,17,17,18,18,18,19,19,19,19,19,19,19,19,19,19,20,20,21,21,22,22,23,23,23,23,23,23,23,24,24,25,25,25,25,26,26,27,27,27,27,27,28,28,28,28,28,29,29,29,29,29,29,29,29,30,30,30,31,31,32,32,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,35,35,36,36,36,36,36,36,36,36,37,37,37,37,37,37,38,39,39,39,40,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,43,44,45,46,46,47,48,48,48,49,49,49,49,49,50,50,50,51,51,51,51,52,52,52,53,53,53,53,53,54,54,54,54,55,55,55,56,56,56,56,57,57,57,57,58,58,58,58,59,59,60,60,60,61,61,61,61,61,61,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,64,64,65,65,65,65,65,65,65,66,66,67,67,67,67,67,68,68,68,68,68,68,69,69,69,69,70,71,71,71,71,71,71,71,71,71,71,72,72,72,72,72,73,73,74,74,74,75,75,75,75,76,77,77,78,79,80,80,80,80,80,80,81,81,82,83,83,83,83,83,83,83,83,83,83,83,84,84,85,85,86,86,86,86,87,87,87,87,88,89,89,89,89,89,89,89,89,89,89,90,90,90,90,91,91,92,92,92,92,92,93,93,94,95,95,95,96,96,97,98,98,98,98,98,98,98,98,98,98,98,98,98,98,99,100,100,100,100,100,101,102,103,103,104,104,105,106,107,108,108,108,109,109,109,109,109,109,109,110,110,110,111,111,111,111,112,112,113,113,113,113,113,114,114,115,116,116,116,116,116,116,116,116,116,116,117,117,117,117,117,117,118,119,119,119,120,120,121,121,122,123,124,125,125,125,125,125,125,125,125,125,125,126,127,127,128,128,129,129,129,129,129,130,130,130,131,132,133,134,135,135,136,137,138,139,139,139,139,139,139,140,140,140,141,141,141,142,143,144,145,146,147,148,148,149,149,150,150,151,152,153,154,155,156,157,157,158,159,160,161,162,163,164,164,165,166,167,168],&#34;target&#34;:[0,124,1,95,121,167,2,15,71,116,125,139,155,3,7,90,116,161,4,5,36,61,83,98,117,140,164,165,6,17,59,80,7,77,79,86,90,109,125,127,132,154,156,161,8,30,96,101,9,112,168,10,73,89,11,43,48,66,90,12,33,34,62,63,83,98,116,125,129,139,144,13,20,76,14,53,75,89,90,97,15,71,116,125,139,155,16,113,116,117,139,154,17,59,80,18,26,142,19,58,89,92,98,125,130,131,149,153,20,76,21,61,22,126,23,37,49,57,108,150,151,24,45,25,50,98,162,26,142,27,65,119,120,125,28,54,89,104,166,29,42,68,71,80,110,116,134,30,96,101,31,115,32,89,33,34,62,63,83,98,116,125,129,139,144,34,62,63,83,98,116,125,129,139,144,35,70,36,61,83,98,117,140,164,165,37,49,57,108,150,151,38,39,42,125,40,41,61,67,72,109,116,125,42,68,71,80,110,116,125,134,157,163,43,48,66,90,98,125,129,133,139,44,45,46,82,47,48,66,90,49,57,108,150,151,50,98,162,51,52,141,147,52,141,147,53,75,89,90,97,54,89,104,166,55,99,102,56,74,103,139,57,108,150,151,58,130,149,153,59,80,60,84,146,61,67,72,83,98,106,109,116,117,125,137,140,164,165,62,63,83,98,116,125,129,139,144,63,83,98,116,125,129,139,144,64,159,65,81,85,107,119,120,125,66,90,67,72,109,116,125,68,71,80,110,116,134,69,139,141,145,70,71,80,89,110,116,125,134,136,139,155,72,109,116,125,158,73,89,74,103,139,75,89,90,97,76,77,79,78,79,80,89,105,110,116,134,81,160,82,83,98,116,117,125,129,139,140,144,164,165,84,146,85,107,86,109,127,132,87,98,109,123,88,89,90,92,97,98,104,105,125,131,166,90,91,97,161,91,152,92,98,122,125,131,93,94,94,95,121,167,96,101,97,98,109,116,117,123,125,129,131,139,140,144,162,164,165,99,100,111,129,148,154,101,102,103,139,104,166,105,106,107,108,150,151,109,116,123,125,127,129,132,110,116,134,111,129,148,154,112,168,113,116,117,139,154,114,126,115,116,117,125,129,134,139,143,144,154,155,117,139,140,154,164,165,118,119,120,125,120,125,121,167,122,123,124,125,129,131,136,137,139,144,154,155,156,126,127,132,128,166,129,139,144,148,154,130,149,153,131,132,133,134,135,154,136,137,138,139,141,144,145,154,155,140,164,165,141,145,147,142,143,144,145,146,147,148,154,149,153,150,151,151,152,153,154,155,156,157,163,158,159,160,161,162,163,164,165,165,166,167,168],&#34;value&#34;:[1,2,1,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,1,2,2,2,6,2,2,2,2,2,2,2,2,2,2,2,1,2,2,2,1,2,2,1,2,2,1,2,2,2,2,4,4,4,4,4,4,4,8,4,4,4,4,1,2,2,2,2,2,2,2,2,1,2,2,2,2,2,1,2,2,2,2,2,1,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,1,2,1,4,1,2,1,2,2,2,2,2,2,1,2,1,2,2,2,1,2,1,2,2,2,2,1,2,2,2,2,1,2,2,2,2,2,2,2,1,2,2,1,2,1,2,1,2,2,2,2,2,4,2,2,2,2,1,2,2,2,2,4,2,2,2,2,1,2,1,2,2,2,2,2,2,2,1,2,2,2,2,2,1,1,2,2,1,1,2,2,2,2,2,2,3,2,2,2,2,2,2,2,2,2,5,2,2,2,2,2,2,2,2,1,1,1,2,1,2,2,2,1,2,2,2,2,1,2,2,1,2,2,2,1,2,2,1,2,2,2,2,1,2,2,2,2,2,2,1,2,2,2,1,2,2,2,1,2,2,2,1,2,1,2,2,8,2,2,2,2,2,2,2,2,4,2,2,2,2,1,2,2,2,4,2,2,2,2,1,2,2,4,2,2,2,2,1,2,5,2,2,2,2,2,2,1,2,1,2,2,2,2,1,2,2,2,2,2,1,2,2,2,1,5,2,2,2,4,4,2,2,2,2,2,2,2,2,2,1,2,1,2,2,1,2,2,2,1,1,2,2,1,3,2,2,2,2,2,3,2,1,3,4,4,2,2,2,2,2,2,2,2,1,2,1,2,1,2,2,2,1,2,2,2,1,8,2,2,2,2,2,2,4,2,2,4,2,2,2,2,2,3,2,2,2,2,1,2,1,1,2,2,1,2,1,8,4,4,2,2,6,4,2,2,2,2,2,2,2,1,1,2,2,2,2,1,1,1,2,1,2,1,1,1,1,2,2,4,2,2,2,2,2,2,1,2,2,1,2,2,2,1,2,1,2,2,2,2,1,2,1,17,2,8,4,2,8,2,4,2,2,2,2,2,2,2,2,1,1,2,2,1,2,1,2,1,3,1,14,2,2,2,2,4,2,2,2,2,2,1,2,1,2,4,4,2,2,2,1,2,2,1,1,2,1,1,4,1,1,1,6,2,2,2,2,2,1,2,2,2,2,2,1,1,1,1,1,1,1,2,1,2,1,2,1,1,1,8,1,1,1,2,1,1,1,3,1,1,1,2,1,2,1,1],&#34;colour&#34;:[&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;,&#34;#c2c2c2&#34;]},&#34;nodes&#34;:{&#34;name&#34;:[&#34;300&#34;,&#34;a million&#34;,&#34;air&#34;,&#34;all&#34;,&#34;america&#34;,&#34;ass&#34;,&#34;ayy&#34;,&#34;baby&#34;,&#34;baby jesus&#34;,&#34;ball&#34;,&#34;bang&#34;,&#34;beam&#34;,&#34;beat&#34;,&#34;big brother&#34;,&#34;bitch&#34;,&#34;bitches&#34;,&#34;blame game&#34;,&#34;bottle&#34;,&#34;bout&#34;,&#34;boy&#34;,&#34;brother&#34;,&#34;bulls***&#34;,&#34;c&#39;mon homie&#34;,&#34;cab&#34;,&#34;champion&#34;,&#34;chance&#34;,&#34;chill&#34;,&#34;church&#34;,&#34;city&#34;,&#34;concert&#34;,&#34;coretta&#34;,&#34;crack music n****&#34;,&#34;dad&#34;,&#34;dame&#34;,&#34;deal&#34;,&#34;dem&#34;,&#34;dessert&#34;,&#34;destination&#34;,&#34;diamond&#34;,&#34;door&#34;,&#34;dream&#34;,&#34;eighteen&#34;,&#34;everybody&#34;,&#34;everything&#34;,&#34;ey &#39;ey &#39;ey&#34;,&#34;eye&#34;,&#34;f***&#34;,&#34;fadin&#34;,&#34;faith&#34;,&#34;fare&#34;,&#34;fire&#34;,&#34;fly&#34;,&#34;fore&#34;,&#34;freak&#34;,&#34;freedom&#34;,&#34;friend&#34;,&#34;frightenin&#34;,&#34;front&#34;,&#34;gangsta&#34;,&#34;genie&#34;,&#34;ghost&#34;,&#34;girl&#34;,&#34;glass&#34;,&#34;glasses&#34;,&#34;glory&#34;,&#34;god&#34;,&#34;god dream&#34;,&#34;gold digger&#34;,&#34;gossip&#34;,&#34;graveshift&#34;,&#34;gwaan&#34;,&#34;hand&#34;,&#34;head&#34;,&#34;hell&#34;,&#34;help&#34;,&#34;highlight&#34;,&#34;hip hop brother&#34;,&#34;hoe&#34;,&#34;home&#34;,&#34;homie&#34;,&#34;i&#39;ma&#34;,&#34;jesus&#34;,&#34;jungle&#34;,&#34;kanye&#34;,&#34;kid&#34;,&#34;king&#34;,&#34;l.a.&#34;,&#34;la la&#34;,&#34;lie&#34;,&#34;life&#34;,&#34;light&#34;,&#34;lord&#34;,&#34;love&#34;,&#34;love lock-down&#34;,&#34;love lockdown&#34;,&#34;luxury&#34;,&#34;malcolm&#34;,&#34;mama&#34;,&#34;man&#34;,&#34;many&#34;,&#34;mars&#34;,&#34;martin&#34;,&#34;memorie&#34;,&#34;menacin&#34;,&#34;mind&#34;,&#34;mine&#34;,&#34;mistake&#34;,&#34;mob&#34;,&#34;moment&#34;,&#34;money&#34;,&#34;monster&#34;,&#34;moon&#34;,&#34;mothaf***a&#34;,&#34;motherf***er&#34;,&#34;murder&#34;,&#34;music&#34;,&#34;n****&#34;,&#34;name&#34;,&#34;new&#34;,&#34;night sky&#34;,&#34;nightlife&#34;,&#34;nike&#34;,&#34;nobody&#34;,&#34;nothing&#34;,&#34;omen&#34;,&#34;one&#34;,&#34;paper&#34;,&#34;parties&#34;,&#34;party&#34;,&#34;people&#34;,&#34;pimp&#34;,&#34;pinocchio&#34;,&#34;place&#34;,&#34;power&#34;,&#34;profit&#34;,&#34;rain&#34;,&#34;reason&#34;,&#34;robocop&#34;,&#34;rosie&#34;,&#34;s***&#34;,&#34;salad&#34;,&#34;sky&#34;,&#34;slave&#34;,&#34;somebody&#34;,&#34;song&#34;,&#34;spaceship&#34;,&#34;spirit&#34;,&#34;spot&#34;,&#34;star&#34;,&#34;step&#34;,&#34;street&#34;,&#34;street light&#34;,&#34;stress&#34;,&#34;talk&#34;,&#34;thing&#34;,&#34;thirty&#34;,&#34;toast&#34;,&#34;two&#34;,&#34;vision&#34;,&#34;war&#34;,&#34;water&#34;,&#34;way&#34;,&#34;wire&#34;,&#34;word&#34;,&#34;work&#34;,&#34;workout plan&#34;,&#34;world&#34;,&#34;yeezy&#34;,&#34;zone&#34;],&#34;group&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169],&#34;nodesize&#34;:[6,5,12,14,7,7,8,49,6,6,5,6,14,14,20,5,11,6,5,15,6,9,11,5,9,6,8,11,6,9,5,19,5,7,7,12,5,5,8,5,5,5,20,42,12,7,5,8,10,5,9,8,8,6,8,16,6,5,6,6,22,64,8,9,8,52,9,8,6,8,6,45,21,5,6,6,6,8,19,22,23,29,7,41,21,6,13,9,8,50,42,27,25,12,9,11,6,6,62,6,5,6,5,6,5,6,10,6,5,27,12,9,7,6,17,12,84,21,9,13,5,5,15,27,5,97,10,12,10,29,6,5,5,16,7,6,6,8,7,54,5,19,7,5,5,10,6,8,9,24,6,5,9,6,72,5,15,15,7,8,12,18,5,12,6,8,11,16,9],&#34;size&#34;:[6,5,12,14,7,7,8,49,6,6,5,6,14,14,20,5,11,6,5,15,6,9,11,5,9,6,8,11,6,9,5,19,5,7,7,12,5,5,8,5,5,5,20,42,12,7,5,8,10,5,9,8,8,6,8,16,6,5,6,6,22,64,8,9,8,52,9,8,6,8,6,45,21,5,6,6,6,8,19,22,23,29,7,41,21,6,13,9,8,50,42,27,25,12,9,11,6,6,62,6,5,6,5,6,5,6,10,6,5,27,12,9,7,6,17,12,84,21,9,13,5,5,15,27,5,97,10,12,10,29,6,5,5,16,7,6,6,8,7,54,5,19,7,5,5,10,6,8,9,24,6,5,9,6,72,5,15,15,7,8,12,18,5,12,6,8,11,16,9]},&#34;options&#34;:{&#34;NodeID&#34;:&#34;name&#34;,&#34;Group&#34;:1,&#34;colourScale&#34;:&#34;d3.scaleOrdinal(d3.schemeCategory20);&#34;,&#34;fontSize&#34;:18,&#34;fontFamily&#34;:&#34;Calibri&#34;,&#34;clickTextSize&#34;:45,&#34;linkDistance&#34;:&#34;function(d){return d.value * 10}&#34;,&#34;linkWidth&#34;:&#34;function(d) { return Math.sqrt(d.value); }&#34;,&#34;charge&#34;:-150,&#34;opacity&#34;:0.9,&#34;zoom&#34;:true,&#34;legend&#34;:false,&#34;arrows&#34;:false,&#34;nodesize&#34;:true,&#34;radiusCalculation&#34;:&#34; Math.sqrt(d.nodesize)+6&#34;,&#34;bounded&#34;:false,&#34;opacityNoHover&#34;:0.15,&#34;clickAction&#34;:&#34;alert(\&#34;Total count of &#39;\&#34; + (d.name) + \&#34;&#39;: \&#34; + (d.size) + \&#34;\n\&#34;)&#34;}},&#34;evals&#34;:[&#34;options.linkDistance&#34;],&#34;jsHooks&#34;:{&#34;render&#34;:[{&#34;code&#34;:&#34;function(el, x) { \n    d3.selectAll(\&#34;.node text\&#34;).style(\&#34;fill\&#34;, \&#34;black\&#34;);\n  }&#34;,&#34;data&#34;:null}]}}&lt;/script&gt;
&lt;figcaption&gt;
Network of entities. Connections between nodes represent entities being in the same song.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br&gt;
To see a larger version of this network, with the ability to filter by album and minimum number of occurences of a word, &lt;a href=&#34;https://dannyjameswilliams.shinyapps.io/kanyenetwork/&#34;&gt;click here to view it as an R shiny app.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This graph only contains entities that appear more than 4 times across all songs, as the full network would contain far too much information, and nothing would be visible. The larger size of the node indicates a word being more frequent across all songs.&lt;/p&gt;
&lt;p&gt;Since this undirected graph is &lt;em&gt;not fully connected&lt;/em&gt;, then it is impossible to connect all entities to each other via other entities that appear in the same song. Here we can also see the most common words at the center of the graph, and as expected, the most frequent words also seem to have the most connections.&lt;/p&gt;
&lt;p&gt;We can also note a few interesting qualities; firstly that Kanye never talks about Jesus in a song without also talking about God. Secondly, Kanye only talks about mistakes when he also talks about girls, I wonder what could that mean?&lt;/p&gt;
&lt;p&gt;Themes are also apparent in different clusters of the graph, for example, a small section containing &lt;em&gt;star&lt;/em&gt;, &lt;em&gt;moon&lt;/em&gt; and &lt;em&gt;mars&lt;/em&gt;, or entities such as &lt;em&gt;workout plan&lt;/em&gt;, &lt;em&gt;dessert&lt;/em&gt; and &lt;em&gt;salad&lt;/em&gt; in another section, seemingly referring to exercise and health. Particular songs can also be isolated out at the outer sections of the graph, which explains some of the more uncommon words that appear with few connections.&lt;/p&gt;
&lt;p&gt;But how do the frequency of these entities change over time? We can inspect the top 10 entities separately for each album, which gives a good indication of their individual themes, as well as Kanye’s use of different lyricism throughout his career.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/kanye/top10words_notitles.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Top 10 entities by album in order of their release.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Kanye’s albums have a broad range of different themes, which correspond to different entities being more prevalent within them. Kanye’s most heartfelt album, &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt; (the fourth album) contained no curse words, but instead had more references to ‘love’ and ‘life’. It is easy to judge the theme of each album by their most common entities, such as &lt;em&gt;Jesus is King&lt;/em&gt; (the final one). Again there are no profanities, instead the album is heavily focused on religion, being a gospel album. Here, ‘Jesus’ is the most common entity, followed by god.&lt;/p&gt;
Some of the entities which might seem ‘irrelevant’ that can be found within these charts often correspond to a single song with purposeful repetition. ‘Toast’ appears in the top 10 entities for the album &lt;em&gt;My Beautiful Dark Twisted Fantasy&lt;/em&gt;, not because it is an amazing breakfast food, but because it is repeated in the chorus of &lt;em&gt;Runaway&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;
&lt;center style=&#34;color:#717d7e;&#34;&gt;
&lt;em&gt;“Let’s have a toast for the douche bags, let’s have a toast for the assholes,&lt;em&gt;&lt;br&gt;
&lt;/em&gt;Let’s have a toast for the scumbags, every one of them that I know”&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;As a side note, you may wonder why the word ‘amazing’ does not appear in the top 10 entities for &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt;, due to it appearing a whopping 55 times in the song &lt;em&gt;Amazing&lt;/em&gt;. Thankfully, Google’s API does not classify it as an entity, since it is actually an adjective. However, ‘love lockdown’ and ‘robocop’ still made it through, as both words are repeated many times in their own songs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;Natural language analysis can also classify the &lt;em&gt;sentiment&lt;/em&gt; of a piece of text, in this case, the sentiment of song lyrics, one song at a time. The sentiment ranges from -1 to 1, where a negative/positive value means the song has a lower/higher sentiment, generally referring to the mood of the song - whether it is more uplifting or sad.&lt;/p&gt;
&lt;p&gt;Firstly, the sentiment API extracts the sentiment from each sentence separately. We can take a look at the density of all sentence sentiments below.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/kanye/sentiment_density.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Density of sentence sentiment.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;So most of the sentence sentiments are negative, in general Kanye’s lyrics convey a more sombre tone than they do a positive one. Wording can play a key part in how the natural language analysis measures sentiment. The lowest sentiment sentences are at -0.9, so let’s take a look at some examples of these.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;“Oh, how could you be so heartless?”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“The devil is alive I feel him breathing.”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“And when I’m older, you ain’t gotta work no more.”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can see the first two generally are quite negative, but the last one has been misrepresented. In this line, Kanye talks about when he was a kid, he wanted to take care of his mother when she got older so that she wouldn’t have to go to work again, but the syntax of the sentence tricked the API into believing it was a generally negative sentence. This isn’t very common, but does highlight one of the limitations of this natural language API.&lt;/p&gt;
&lt;p&gt;A song’s &lt;em&gt;magnitude&lt;/em&gt; is defined as the sum of the absolute values of the sentiments for each sentence in the song. Quite a mouthful - consider it as the ‘emotionality’ of the song; the higher the sentiment is (in one way or another), the more emotional the song becomes. We can break down the sentiment and magnitude over all songs by album.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/kanye/sentiment_magnitude.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
&lt;figcaption&gt;
Sentiment (top) and magnitude (bottom), averaged for each song and split by album.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Generally, Kanye’s albums are all quite negative, with the exception of &lt;em&gt;Kids See Ghosts&lt;/em&gt; and &lt;em&gt;Jesus is King&lt;/em&gt;, his two latest albums. After the release of &lt;em&gt;Graduation&lt;/em&gt;, Kanye went through various personal traumas. We can see this reflected here, as the sentiment up to &lt;em&gt;Graduation&lt;/em&gt; was increasing, after which it began decreasing again. Only recently has the sentiment began to increase again.&lt;/p&gt;
&lt;p&gt;The album with the lowest sentiment is &lt;em&gt;Yeezus&lt;/em&gt;, which surprisingly does not have the largest magnitude, although the magnitude does not vary as much with album. The sentiment in &lt;em&gt;808’s &amp;amp; Heartbreak&lt;/em&gt; has the largest variance, with certain songs reaching very low and (comparatively) high values. These two albums generally are considered quite emotional, so it is reassuring to see this reflected in the sentiment analysis.&lt;/p&gt;
&lt;p&gt;We might say that it’s nice that Kanye’s latest releases are more positive, but there is a general attitude of &lt;em&gt;“I miss the old Kanye”&lt;/em&gt;. Is that the lower mood Kanye? The always rude Kanye?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-and-magnitude-against-album-reception&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment and Magnitude against Album Reception&lt;/h2&gt;
&lt;p&gt;The next question in our heads should be, what can we infer from this? Some albums have a higher or lower sentiment than others, but does this have any relationship with the album itself?&lt;/p&gt;
&lt;p&gt;It turns out: well, maybe. Plotted below are the relationships between the mean sentiment and the mean magnitude, by album, against the aggregated critic reviews for each album collected from &lt;a href=&#34;https://www.metacritic.com/person/kanye-west&#34;&gt;Metacritic&lt;/a&gt;.&lt;/p&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe height=&#34;450&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; width=&#34;100%&#34; src=&#34;//plotly.com/~dannyjameswilliams/3.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Linear regression for Album sentiment against metacritic score.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;figure class=&#34;image&#34;&gt;
&lt;center&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;450&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;//plotly.com/~dannyjameswilliams/6.embed?autosize=true&amp;amp;link=false&amp;amp;modebar=false&#34;&gt;
&lt;/iframe&gt;
&lt;figcaption&gt;
Linear regression for Album magnitude against metacritic score.
&lt;/figcaption&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Linear regression models are chosen because of the sparsity of data. Any more complex model is likely unneeded, but would also lack sufficient degrees of freedom to perform any meaningful inference. Modelling these variables separately allowed for more succinct visualisation and interpretation of their effects in isolation. The goodness of fit for each model can be evaluated somewhat with &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; values:
&lt;span class=&#34;math display&#34;&gt;\[
R^2_{\text{sentiment}} \approx  0.270, \qquad R^2_{\text{magnitude}} \approx  0.675.
\]&lt;/span&gt;
So the model involving each album’s mean magnitude explains more of the variance than the sentiment, and there is some definite correlation there. Does this mean an album with a higher magnitude (i.e. contains more emotional lyrics) will be more critically acclaimed? I can’t say for sure, but it’s an interesting point to note.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-a-new-kanye-song&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating a New Kanye Song&lt;/h2&gt;
&lt;p&gt;Finally, I would like to end this post by creating the lyrics for a new Kanye song. &lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;OpenAI’s unsupervised GPT-2 language model&lt;/a&gt; was trained on 40GB of internet text, and tasked with predicting the next word given in the text. The model contains an amazing 1.5 billion parameters. The &lt;a href=&#34;https://github.com/minimaxir/gpt-2-simple&#34;&gt;gpt-2-simple&lt;/a&gt; package in Python provides straightforward access to finetuning the GPT-2 model to an additional dataset, so that text generations are based on the new data, but the language has already been learned from the 40GB of internet text (if you think GPT-2 is impressive, take a look at &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;GPT-3&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I input the formatted lyric text file to finetune the GPT-2 model to Kanye’s lyrics, and generated many large sized texts to be considered as songs. This was done in Google colab notebooks, and the code used to do so is &lt;a href=&#34;https://colab.research.google.com/drive/13tC0RIr_IpNNWr6zInYkyormDgKsyF18?usp=sharing&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of the songs generated, I picked a coherent and ‘song-like’ generation and changed the formatting (added spacing). I hope you enjoy the latest release!&lt;/p&gt;
&lt;center style=&#34;color:#717d7e;&#34;&gt;
My name is Bess, I’m 21 years old and I just want to be a real star&lt;br&gt;
I just want to be a superstar&lt;br&gt;
The city skyline, the planes flying overhead&lt;br&gt;
I’m groovy as f***, like Good Charlotte&lt;br&gt;
Uh, and I just want my daddy to be proud of me&lt;br&gt;
&lt;br&gt;
Cause I ain’t talkin’ about Kris when it comes to being in the club&lt;br&gt;
It’s Jay that I’m talkin’ about, man&lt;br&gt;
Even though he got the baby’s clothes on&lt;br&gt;
I done wore nothing but red until he tucked me in&lt;br&gt;
And when he woke up, I was still wearing everything buterin’&lt;br&gt;
S*** was very “The Big Lebowski”&lt;br&gt;
&lt;br&gt;
I was standin’ by myself writing this song&lt;br&gt;
And I just started to cry&lt;br&gt;
Because this s*** can’t get any worse&lt;br&gt;
This s*** can’t get any worse&lt;br&gt;
Oh, Lord, oh, Lord&lt;br&gt;
&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
I’m comin’, I’m comin’ in, load ya weapons&lt;br&gt;
&lt;br&gt;
And I came back, I came back, I came back&lt;br&gt;
And I looked in the mirror and I seen the biggest&lt;br&gt;
The guns are in the table, the weapons is in the air&lt;br&gt;
&lt;br&gt;
Yeah, make America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
Keep America great again&lt;br&gt;
&lt;br&gt;
&lt;/center&gt;
&lt;p&gt;Seems to match Kanye pretty well, we have mention of his mother-in-law, Kris Jenner, as well as his sort-of friend and collaborator Jay-Z. Then we finish with a classic controversial Kanye segment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactive-visualisations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactive Visualisations&lt;/h2&gt;
&lt;p&gt;Most of the visualisations that I have shown can be found in an R shiny app below.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dannyjameswilliams.shinyapps.io/kanyenet/&#34;&gt;The kanyenet interactive R shiny app.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also included are additional interactive plots, enabling the filtering by album for sentiment densities and viewing the sentiment and magnitude of each song. You can also view more generations from the GPT-2 model!&lt;/p&gt;
&lt;p&gt;Again, you can view all code used for this report in the github repository &lt;a href=&#34;https://github.com/dannyjameswilliams/kanyenet&#34;&gt;kanyenet&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hot Takes for R</title>
      <link>https://dannyjameswilliams.co.uk/post/hottakes/</link>
      <pubDate>Sun, 28 Jun 2020 15:39:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/hottakes/</guid>
      <description>
&lt;script src=&#34;https://dannyjameswilliams.co.uk/post/hottakes/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The arrow assigment operator &lt;code&gt;&amp;lt;-&lt;/code&gt; is useless. Before I’m crucified by the R community, hear me out and read this post.&lt;/p&gt;
&lt;p&gt;Every time I read code written by an academic, lecturer or someone who uses R frequently, I come across the arrow symbol &lt;code&gt;&amp;lt;-&lt;/code&gt; used for assignment of variables. Never in my career have I seen someone systematically use the equals symbol &lt;code&gt;=&lt;/code&gt; across their code.&lt;/p&gt;
&lt;div id=&#34;benefits-of-the-arrow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Benefits of the arrow&lt;/h3&gt;
&lt;p&gt;A frequent association with &lt;code&gt;&amp;lt;-&lt;/code&gt; is in how assignment works in R. The variable on the right hand side of the operator is assigned to the one on the left. Hence the arrow makes a lot of sense. We can also do it the other way around, for instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;3 -&amp;gt; x
y &amp;lt;- 5
cat(&amp;quot;x is&amp;quot;, x, &amp;quot;and y is&amp;quot;, y, &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## x is 3 and y is 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the arrow has a benefit when teaching programming, so if you’re a beginner it is obvious which way around variables are assigned. If you’re not a beginner, it might reinforce this knowledge so that you don’t make mistakes.&lt;/p&gt;
&lt;p&gt;You can also use the arrow inside of functions to assign variables, for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(x &amp;lt;- solve(matrix(rnorm(100^2), 100, 100)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.002   0.000   0.004&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can view &lt;code&gt;x&lt;/code&gt; separately, even though it was assigned inside the &lt;code&gt;system.time&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x[1:5, 1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]        [,2]        [,3]       [,4]        [,5]
## [1,]  0.50730275 -0.35703351 -0.39847262  0.7788050  0.14551130
## [2,] -0.18092188  0.23194703  0.11982541 -0.4136690 -0.04548487
## [3,] -0.09788994  0.08614508  0.10585201 -0.1223789 -0.01548020
## [4,]  0.06537141 -0.16506482  0.09142846  0.1438222  0.02654319
## [5,]  0.03935626 -0.05008914 -0.09419370  0.2286113 -0.03929192&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is perhaps its most useful application, which you cannot do with &lt;code&gt;=&lt;/code&gt;. The &lt;code&gt;=&lt;/code&gt; sign inside of a function argument is strictly used for matching the function argument with the variable you’re passing through.&lt;/p&gt;
&lt;p&gt;The arrow also has historical significance, since R’s predecessor, S, used &lt;code&gt;&amp;lt;-&lt;/code&gt; exclusively. This &lt;a href=&#34;https://www.r-bloggers.com/why-do-we-use-arrow-as-an-assignment-operator/&#34;&gt;R-bloggers post&lt;/a&gt; explains that S was based on an older language called APL, which was designed on a keyboard that had an arrow key exactly like &lt;code&gt;&amp;lt;-&lt;/code&gt;. But our keyboards now only have a key for &lt;code&gt;=&lt;/code&gt;, right?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-you-should-accept-the-equals-sign&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why you should accept the equals sign&lt;/h3&gt;
&lt;p&gt;But I’m here today to tell you to not use &lt;code&gt;&amp;lt;-&lt;/code&gt; and to use &lt;code&gt;=&lt;/code&gt; instead. Start by asking yourself why you use the arrow? Maybe you have historical reasons and used R before 2001, or more likely, you’re following convention for coding in R that even &lt;a href=&#34;https://google.github.io/styleguide/Rguide.html&#34;&gt;styling guides&lt;/a&gt; &lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34;&gt;recommend&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Firstly, no other programming language uses the arrows, at least, none of the most frequently used ones such as Python, MATLAB, C++, Julia, Javascript, etc. So if you’re like me and use R alongside other programming languages, why would you bother using &lt;code&gt;&amp;lt;-&lt;/code&gt; instead of &lt;code&gt;=&lt;/code&gt;? Wouldn’t you like consistency across the languages you write in, at least so that your muscle memory doesn’t have to change depending on whether you’re fitting a Neural network in Python, or a GAM in R?&lt;/p&gt;
&lt;p&gt;Okay fair enough, maybe you don’t mind switching coding styles depending on what language you’re writing in, after all, you are going to be changing a lot more than just the assignment operator. So what other benefits does &lt;code&gt;=&lt;/code&gt; have?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a button for it on the keyboard.&lt;/li&gt;
&lt;li&gt;Consistency between function arguments and assignment.&lt;/li&gt;
&lt;li&gt;Increased readability and neatness since it has fewer character (admittedly, this is subjective).&lt;/li&gt;
&lt;li&gt;Similarity with equality operator (&lt;code&gt;==&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;No confusion between for example &lt;code&gt;x&amp;lt;-2&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(x=2\)&lt;/span&gt;) and &lt;code&gt;x &amp;lt; -2&lt;/code&gt; (&lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; -2\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Consistency with &lt;em&gt;mathematics itself&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, I prefer to use the equals assigment operator over the arrow, because I like to code in more than just one language.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-neat-full-stop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The neat full stop&lt;/h3&gt;
&lt;p&gt;While I’m on the subject of the arrow, using a full stop in a variable name brings a lot of confusion. This one is a lot less controversial than disregarding the arrow in my opinion. We can name a variable in R as&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;some.variable = 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks neat! But in other languages, this would throw an error. Why is that? Languages like Python use &lt;code&gt;.&lt;/code&gt; as a class operator, and you use it to access elements of a class exclusively, so you cannot use it in variable names. But R doesn’t have this problem, right?&lt;/p&gt;
&lt;p&gt;When defining an S3 class in R, you can overwrite some default functions (such as &lt;code&gt;print&lt;/code&gt; or &lt;code&gt;plot&lt;/code&gt;) with a new function that handles these default operations in a different way for your specific S3 class. To do this for an S3 class called &lt;code&gt;mys3class&lt;/code&gt;, you would write a new functions as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.mys3class = function(x, ...){
  ...
}
plot.mys3class = function(x, ...){
  ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look familiar? So full stops &lt;em&gt;do&lt;/em&gt; have a purpose in R apart from assigning neat variable names. For me, I don’t like using full stops for the main reason I don’t like using the &lt;code&gt;&amp;lt;-&lt;/code&gt; operator: &lt;strong&gt;consistency&lt;/strong&gt;. If I’m using &lt;code&gt;&amp;lt;-&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt;, it will be for a specific purpose where I cant use &lt;code&gt;=&lt;/code&gt; or &lt;code&gt;_&lt;/code&gt; (however, these are rules that I’ve broken myself, and you can probably find instances of it in my portfolios).&lt;/p&gt;
&lt;p&gt;So whilst neither the arrow (&lt;code&gt;&amp;lt;-&lt;/code&gt;) for assignment nor the full stop (&lt;code&gt;.&lt;/code&gt;) for variable naming are completely useless, better alternatives &lt;em&gt;do&lt;/em&gt; exist. However, if you value your code looking neat above all else, and aren’t bothered by cross platform consistency; then you can use R’s exclusive &lt;code&gt;&amp;lt;-&lt;/code&gt;, or its inconsistent &lt;code&gt;.&lt;/code&gt; without issue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(1:5, 1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://dannyjameswilliams.co.uk/post/hottakes/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Electricity Demand Forecasting Hackathon</title>
      <link>https://dannyjameswilliams.co.uk/post/hackathon/</link>
      <pubDate>Mon, 17 Feb 2020 15:39:00 +0000</pubDate>
      <guid>https://dannyjameswilliams.co.uk/post/hackathon/</guid>
      <description>


&lt;p&gt;In February I participated in a COMPASS hackathon, where me and my fellow students fit statistical models to try to improve predictions in forecasting electricity demand based on weather related variables.&lt;/p&gt;
&lt;p&gt;We were fortunate to be visited by Dr Jethro Browell, a Research Fellow at the University of Strathclyde, who gave a brief lecture on how electricity demand was calculated, and how much it has changed over the last decade. After the lecture, Dr Mateo Fasiolo, a lecturer who works with us, explained a basic Generalised Additive Model (GAM) which can be used to forecast electricity demand for a particular dataset.&lt;/p&gt;
&lt;p&gt;Our task was to output a set of predictions for a testing dataset and submit them to the group git repository. We only had access to the predictor variables of this dataset, so we wouldn’t know how well our model was doing until it was submit and checked by Dr Fasiolo, who then put all submitted scores on the projector at the front of the room. The team with the lowest root mean squared error at the end would be crowned the winner.&lt;/p&gt;
&lt;p&gt;Me and my team “Jim” (named so because we went to the gym) performed well at the start, extending the basic GAM to include additional covariates and interactions, as well as including some feature engineering. The second team “AGang” (because all of their names began with “A”) took the edge over us by removing a single variable that we didn’t realise was actually making our model worse, and their GAM produced better predictions overall by a small margin. The third team “D &amp;amp; D” (because both their names began with a D) was having no luck at all, trying to implement a random forest model as opposed to a GAM, but their predictions were significantly off, and their code took much longer to run than ours, leaving them with little time to troubleshoot.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://dannyjameswilliams.co.uk/img/post/hackathon.jpg&#34; alt=&#34;The COMPASS cohort after participating in the hackathon, with Dr Jethro Browell and Dr Mateo Fasiolo in the front.&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The COMPASS cohort after participating in the hackathon, with Dr Jethro Browell and Dr Mateo Fasiolo in the front.
&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;Try as we did, we were unable to do any better than our original model; but we limited our scope to a GAM, and did not try anything out-of-the-box compared to the other two teams.&lt;/p&gt;
&lt;p&gt;The “AGang” were set to win it, until a surprise twist of fate sent “D&amp;amp;D” soaring into the lead, with predictions that had a far smaller error than anyone elses. The random forest model they were fitting before had an error, and they managed to fix the error, finish running the model and submit their predictions with only moments to spare. Thus, we came last.&lt;/p&gt;
&lt;p&gt;This was a fun competition, even though we lost. I realise that our mistake now was that we did not include anything special in our model that accounted for different weather patterns in different regions. Our model would have done very well if it was more variable; so that certain predictors were included in some areas that had more solar power, for instance. The way which we fit the model was the same for all regions, even though they were all quite different.&lt;/p&gt;
&lt;p&gt;You can read the article from the Bristol school of mathematics &lt;a href=&#34;https://www.bristol.ac.uk/maths/news/2020/compass-hackathon.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
