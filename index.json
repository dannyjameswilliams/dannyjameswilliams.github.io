[{"authors":["admin"],"categories":null,"content":"I am a PhD student studying at the University of Bristol under the COMPASS CDT (Computational Statistics and Data Science Centre for Doctoral Training). I graduated from the University of Exeter with a Masters in Mathematics in 2019.\n","date":1624537143,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1624894740,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dannyjameswilliams.co.uk/author/daniel-williams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daniel-williams/","section":"authors","summary":"I am a PhD student studying at the University of Bristol under the COMPASS CDT (Computational Statistics and Data Science Centre for Doctoral Training). I graduated from the University of Exeter with a Masters in Mathematics in 2019.","tags":null,"title":"Daniel Williams","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://dannyjameswilliams.co.uk/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"This set of portfolios contains the notes that were submitted for the first statistical computing module in the COMPASS CDT.\nTo view these portfolios, please see the menu bar on the left.\nThese notes focus primarily on using R. R has a lot of advantages for data science, but personally my favourite is its simplicity. In other programming languages such as Python, there are many different types of data structures (such as pandas data frames, numpy arrays etc.), where R has a consistent infrastructure which will not get confusing.\nFor these reasons, R is a good beginners language, and does not require a lot of learning. It is not perfect though, as it can be quite slow in some cases, and does not have as wide usage as Python.\nThe basics of R are covered first, as well as programming paradigms such as reproducibility, using Github.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"594a9d213331a3fb896e41d0852ef5b4","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/portfolios/sc1/","section":"portfolios","summary":"Statistical computation in R, with a focus on modelling and efficient programming.","tags":null,"title":"Statistical Computing 1","type":"docs"},{"authors":null,"categories":null,"content":"This set of portfolios contains the notes that were submitted for the second statistical computing module in the COMPASS CDT.\nTo view these portfolios, please see the menu bar on the left. You may have to scroll down.\nIn general, these notes form more advanced statistical computation methodologies, using a lower level language than R: C++. C++ is beneficial for its speed advantages over interpreted languages such as R or Python. This makes it a good candidate for computationally intensive methods such as Bayesian MCMC sampling, or when working with big data.\nMost of my statistical work has been using R, so this module was a good opportunity for me to learn C++, and develop my Python skills. In general, I still prefer R to Python for basic data analysis for its simplicity, but there are a lot of cases (such as neural networks in TensorFlow) where Python is superior. It is important to be able to know both.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"d00df7db275d73edf825fd87c9d3a38f","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/portfolios/sc2/","section":"portfolios","summary":"More advanced statistical computation in C++, R, Rcpp and Python.","tags":null,"title":"Statistical Computing 2","type":"docs"},{"authors":null,"categories":null,"content":" Basic C++ C++ programs are written in ‘chunks’, and each chunk can be a function which contains some code, or a main chunk, which is the program that is run when the code is compiled. Here is an example of a main chunk that runs some code.\n// include \u0026#39;iostream\u0026#39; package #include \u0026lt;iostream\u0026gt; // int means this will output an integer, main() signifies the primary code to run int main() { // pipe operator, using std library, cout is to output something, endl is to end line std::cout \u0026lt;\u0026lt; \u0026quot;Hello\u0026quot; \u0026lt;\u0026lt; std::endl; // return integer 0 to show all is okay return 0; } This chunk of code will output the string Hello when run, with comments (starting with //) describing what each line does. However, this cannot be run by itself, since C++ is a compiled programming language; so we need a compiler. Firstly, we save this file as hello.cpp, and using the g++ compiler in the terminal will compile this code into an executable program.\ng++ hello.cpp -o hello This starts with g++, telling the terminal to use the g++ program, then chooses the file hello.cpp, -o hello specifies that the name of the output is hello. This has compiled a program into the current working directory which can also be run in the terminal.\n./hello ## Hello So the code has run succesfully! The only output was Hello as that was all that was specified to be output. Note that in C++ every integer, string, float, etc. needs to be defined in advance. Instead of in programming languages like R and Python, where nothing really needs to be specified in advance, variables need to be defined each time. For example,\nint a = 3 float b = 14.512231 double c = 4e200 std::string d = \u0026quot;What up\u0026quot; To further exemplify the usage of C++, consider the following example.\n#include \u0026lt;iostream\u0026gt; int main() { for (int i=1; i\u0026lt;=100; i++) { if (i\u0026gt;=95) { std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; } else if (i\u0026lt;5) { for(float j=-1.5; j\u0026gt;=-3.5; j--) { std::cout \u0026lt;\u0026lt; i*j \u0026lt;\u0026lt; \u0026quot; \u0026quot;; } } } } This has used a range of different code, including for loops and conditional statements if and else if. These are implemented in C++ similarly to the way they are implemented in R. In fact, the if statements almost have the exact same formatting as R. Loops are a bit different. The syntax that C++ uses for basic loops are for(initialise; condition; increment), where initialise refers to the first element that is being looped through, condition is the stopping condition of the loop and increment is how much the initaliser increases on each iteration.\nIn this case, for the increment we have used i++ and j--, which is shorthand for i = i + 1 and j = j - 1. The stopping conditions are when i is less than or equal to 100, and when j is greater than or equal to -3.5. Each iteration of the loop checks two conditions, the value of i, and different operations happen when i is less than 5, or greater than or equal to 95. We can run this code to see the output.\ng++ loopif.cpp -o loopif ./loopif ## -1.5 -2.5 -3.5 -3 -5 -7 -4.5 -7.5 -10.5 -6 -10 -14 95 96 97 98 99 100 Times Table Example Consider for a further example writing a function to print the times tables for a given number, a given number of times. For this, we can exemplify the use of multi-file programs. By creating a header file, with extension .h, and including this in our main program, we can specify functions in a different file. This helps navigate large code files, and separating different functions in different files can be extremely useful.\nLet’s start by creating a times table function in a timestable.cpp file. This takes two integer inputs, the first being the times table we want to print, and the second being the maximum number we want to multiply to the first input.\n#include \u0026lt;iostream\u0026gt; void timestable(int a, int b) { for(int i = 1 ; i \u0026lt;= b; i++) { std::cout \u0026lt;\u0026lt; a*i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; } std::cout \u0026lt;\u0026lt; std::endl; } So this loops from i=1 to b, and multiplies a by i on each iteration. Now we create a header file, called timestable.h, containing the following\n#ifndef _TIMESTABLE_H #define _TIMESTABLE_H void timestable(int, int); #endif  This header file checks and defines the token _TIMESTABLE_H, and then simply declares the function timestable. When reading this function into another C++ file, it will declare the timestable function that is defined in timestable.cpp. A main file, called main.cpp to read the header file, which will define the timestable function, and run it for for two examples.\n#include \u0026lt;iostream\u0026gt; #include \u0026quot;timestable.h\u0026quot; int main() { std::cout \u0026lt;\u0026lt; \u0026quot;Five times table\u0026quot; \u0026lt;\u0026lt; std::endl; timestable(5, 10); std::cout \u0026lt;\u0026lt; \u0026quot;Twelve times table\u0026quot; \u0026lt;\u0026lt; std::endl; timestable(12, 12); } So we are expected the output of the five times table, up to 5 \\(\\times\\) 10, and the twelve times table, up to 12 \\(\\times\\) 12. Let’s compile and run this program.\ng++ main.cpp timestable.cpp -o timestable ./timestable ## Five times table ## 5 10 15 20 25 30 35 40 45 50 ## Twelve times table ## 12 24 36 48 60 72 84 96 108 120 So it has worked as expected.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"af1c3259adde8240a067cde5045238fd","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc2/intro/","section":"portfolios","summary":"Basic C++ C++ programs are written in ‘chunks’, and each chunk can be a function which contains some code, or a main chunk, which is the program that is run when the code is compiled.","tags":null,"title":"Intro to C++","type":"docs"},{"authors":null,"categories":null,"content":" Command Console R provides a command console, which is where all code is processed. You can enter commands directly into the command console, or run them from a script. Both will result in whatever command being executed. For example, we can perform an operation such as\n10*10 ## [1] 100 and it will output the result. If we wanted to save the output, we assign this code to a variable, which is saved into the environment.\nmultiplication_variable \u0026lt;- 10*10 multiplication_variable2 = 10*10 So now we have two variables in our environment, multiplication_variable and multiplication_variable2. Both should be the same value, the only difference in how they were assigned. multiplication_variable was assigned with the \u0026lt;- operator, whereas multiplication_variable2 was assigned with the = operator.\nWe can use the == command to check whether two variables are equal. This is an equality sign, and will output either TRUE or FALSE (or a vector of TRUE and FALSE if working with vectors).\nmultiplication_variable == multiplication_variable2 ## [1] TRUE Confirming that the two variables are equal to each other!\n Operators and Functions R has many operators, too many to list here, but you can intuitively understand the basic operators such as divide (/), multiply (*), add (+) and subtract (-). Some other less common operators include the matrix multiply (%*%), integer division (%/%), integer modulus (%%) and exponentiate (^).\nBy default, R loads in a certain number of basic packages, including base, stats and utils. Through these packages, a large amount of functions are available, all useful. Other packages can be loaded by using the library function. For example, suppose I wanted to simulate from a multivariate Normal distribution. There is no package in base R to do this, but there is a function to do this in the MASS library. First, if this package is not installed then it needs to be done so by using install.packages(\u0026quot;MASS\u0026quot;) (which only needs to be done once). To load the library, we run\nlibrary(MASS) Now the function should be available. To find out what arguments the function takes, and what to input to the function, we can look at its help file by running ?mvrnorm, this has a ‘Usage’ section detailing the following\nmvrnorm(n = 1, mu, Sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE) n - the number of samples required. mu - a vector giving the means of the variables. Sigma - a positive-definite symmetric matrix specifying the covariance matrix of the variables. tol - tolerance (relative to largest variance) for numerical lack of positive-definiteness in Sigma. empirical - logical. If true, mu and Sigma specify the empirical not population mean and covariance matrix. EISPACK - logical: values other than FALSE are an error. So the variables in mvrnorm we need to specify are n, mu and Sigma. Let’s run the function now\nx = mvrnorm(5, mu = c(1,2), Sigma = matrix(c(1,1,1,1),2,2)) and we can look at this output by simply typing x:\nx ## [,1] [,2] ## [1,] 0.97765673 1.9776567 ## [2,] 0.49305126 1.4930513 ## [3,] -0.06323688 0.9367631 ## [4,] 0.77825704 1.7782570 ## [5,] 2.23512186 3.2351219 Notice that in the specification to mvrnorm, two other functions were used; c and matrix. If you are curious about these, look at the help files for them. Packages and functions are key to using R effectively and efficiently.\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"c8c4bfef1927b29a00b0020d41700bbd","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/intro/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/intro/","section":"portfolios","summary":"Command Console R provides a command console, which is where all code is processed. You can enter commands directly into the command console, or run them from a script. Both will result in whatever command being executed.","tags":null,"title":"Introduction to R","type":"docs"},{"authors":null,"categories":null,"content":" Performing operations on vectors In general, there are three methods that can be used to perform the same operation (such as a mathematical operation) on every element in a vector \\(\\boldsymbol{x}\\). A simple way of doing this is with a loop, which iterates once for each element in \\(\\boldsymbol{x}\\), and performs the operation one at a time. Vectorisation refers to the process of applying the same operation to every element in a vector at once, whereas the apply function applies any function across a vector in a single line of code.\nComparison between vectorisation, loops and apply We can test the efficiency of using vectorisation as opposed to using a loop or an apply function. We will construct three pieces of code to do the same thing, that is, apply the function \\(f(x) = \\sin(x)\\) to all elements in a vector, which is constructed of the natural numbers up to 100,000. We start by creating the vector:\nx = seq(1,100000) Now we can create three functions. One that uses a for loop, one that uses apply and one that works by vectorising.\nloop_function = function(x){ y = numeric(length(x)) for(i in 1:length(x)){ y[i] = sin(x[i]) } return(y) } apply_function = function(x){ y = apply(as.matrix(x), 1, sin) return(y) } vector_function = function(x){ y = sin(x) return(y) } Now that the functions are constructed, we can use the inbuilt R function system.time to calculate how long it takes each function to run.\nsystem.time(loop_function(x)) ## user system elapsed ## 0.058 0.000 0.080 system.time(apply_function(x)) ## user system elapsed ## 0.312 0.000 0.312 system.time(vector_function(x)) ## user system elapsed ## 0.006 0.000 0.005 Naturally, none of these computations take very long to perform, as the process of taking the sine isn’t very complex. However, you can still see the order of which functions are the fastest. In general, vectorisation will always be more efficient than loops or apply functions, and loops are faster than using apply.\nThere are cases where it will not be possible to use vectorisation to carry out a task on an array. In this case, it is necessary to construct a function to pass through apply, or performing operations within a loop. In general, loops are faster and more flexible - as they allow you to do more in each iteration than a function could. Some situations where you might want to use apply is to make a simple process neater in the code. If you were doing something relatively straightforward, you will save space and make the code more readable by using apply, as opposed to a loop.\nIt is common practice to always vectorise your code when you can, as it comes with a significant speed increase, as loops and apply functions are slower than vectorised code.\n Other functions There are different variants of the apply function depending on how your data are constructed, and how you would want your output.\n apply(X, MARGIN, FUN, ...) is the basic apply function. MARGIN refers to which dimension remains constant when performing the function. For example, apply(sum,1,x) will sum across the columns, and the number of rows will remain constant. lapply(X, FUN, ...) is an apply function that returns a list as its output, each element in the list corresponding to applying the given function to each value in X. sapply(X, FUN, ...) is a wrapper of lapply that will simplify the output so that it is not in list form. mapply(FUN, ...) is a multi-dimensional version of sapply, with mapply it is possible to add more than one input to the function, and it will return a vector of values for each set of inputs.  Map, Reduce and Filter Map maps a function to a vector. This is similar to lapply. For example:\nx = seq(1, 3, by=1) f = function(a) a+5 M = Map(f,x) M ## [[1]] ## [1] 6 ## ## [[2]] ## [1] 7 ## ## [[3]] ## [1] 8 This has added 5 to every element in x, and returned a list of outputs for each element. In fact, Map performs the same operation as mapply does, which we can see in the function itself:\nMap ## function (f, ...) ## { ## f \u0026lt;- match.fun(f) ## mapply(FUN = f, ..., SIMPLIFY = FALSE) ## } ## \u0026lt;bytecode: 0x55bfca7455a8\u0026gt; ## \u0026lt;environment: namespace:base\u0026gt; Reduce performs a given function on pairs of elements in a vector. The procedure is iterated from left to right, and a single value is returned. This can be done from right to left by adding the argument right=TRUE. As an example, consider division:\nf = function(x, y) x/y x = seq(1, 3, by=1) Reduce(f, x) ## [1] 0.1666667 Reduce(f, x, right=TRUE) ## [1] 1.5 In the first case, Reduce worked by dividing 1 by 2, then this result by 3. In the second case, this was in reverse, first dividing 3 by 2, then this result by 1.\nFilter will ‘filter’ an array into values that satisfy the condition. For example\nx = seq(1,5) condition = function(x) x \u0026gt; 3 Filter(condition,x) ## [1] 4 5 Filter is similar to just indexing an array using TRUE/FALSE values, but instead of indexing using an array, it indexes using a function. However, we can inspect the interior of the function\nFilter ## function (f, x) ## { ## ind \u0026lt;- as.logical(unlist(lapply(x, f))) ## x[which(ind)] ## } ## \u0026lt;bytecode: 0x55bfc77914d8\u0026gt; ## \u0026lt;environment: namespace:base\u0026gt; So infact, the function for Filter simply uses lapply to get the indices of the TRUE/FALSE values, and indexes the array for input x with a simple subsetting.\n   Parallel Computing By using the parallel package, you can make use of all processing cores on your computer. Naturally, if you only have a single core processor, this is irrelevant, but most computers in the modern day have 2, 4, 8 or more cores. Parallel computing will allow R to run up to this many proccesses at the same time. A lot of important tasks in R can be sped up with parallel computing, for example MCMC. In MCMC, using \\(n\\) cores can allow you to also run \\(n\\) chains at once, with (in theory) no slowdown.\nSupercomputers generally have an extremely large number of cores, so being able to run code in parallel is important in computationally expensive programming jobs.\nThere are some disadvantages to this: namely that splitting a process to four different cores will also require four times as much memory. If your memory isn’t sufficient for the amount of cores that you are using, this will cause a significant slowdown.\nUsing mclapply or foreach There are two main methods to parallelise a set of commands (or a function) in R. The first method is a parallel version of apply, and the second method is a parallel version of mclapply. To illustrate how these work, consider the example of an \\(ARMA(1,1)\\) model, which has an equation of the form \\[ x_t = \\epsilon_t + \\alpha\\epsilon_{t-1} + \\beta x_{t-1} \\] \\[ \\epsilon_t \\sim N(0, 1) \\] A function that generates an \\(ARMA(1,1)\\) process can be written as:\narma11 = function(alpha=0.5, beta=1, initx, N=1000){ x = eps = numeric(N) x[1] = initx eps[1] = rnorm(1,0,1) eps[2] = rnorm(1,0,1) x[2] = eps[1] + alpha*eps[2] + beta*x[1] for(i in 3:N){ eps[i] = rnorm(1,0,1) x[i] = eps[i] + alpha*eps[i-1] + beta*x[i-1] } return(x) } This will generate a vector of \\(x\\) values for each timestep from \\(t=1,\\dots,N\\). We can see a plot of this generated time series by running a simulated \\(ARMA\\) timeseries of length \\(N=1000\\).\nplot(1:1000, arma11(initx=0.5,N=1000),type=\u0026quot;l\u0026quot;, xlab=\u0026quot;Time (t)\u0026quot;, ylab=expression(x), main=\u0026quot;Time Series Example\u0026quot;) Now that the functions are set up for testing, we can now set up the computer to work in parallel. This involves loading the required packages and detecting the number of cores we have available.\nlibrary(parallel) library(MASS) no.cores = detectCores() no.cores ## [1] 8 So we have this many cores to work with (on my laptop, there are 8 cores). The no.cores variable will be passed into the parallel computing functions. We can now use mclapply to simulate this \\(ARMA\\) model a large amount of times, and calculate the difference from the first value and the last value as a statistic. By putting the arma11 function in a wrapper, we can pass it through to mclapply.\narma_wrapper = function(x) { y = arma11(alpha=1, beta=1, initx=x, N=1000) return(head(y,1) - tail(y,1)) } xvals = rep(0.5,1000) MCLoutput = unlist(mclapply(xvals, arma_wrapper, mc.cores = no.cores)) So now, MCLoutput is a vector, of length 1000, that contains the differences between the first and last value in a generated time series from an \\(ARMA(1,1)\\) model with \\(\\alpha=1\\) and \\(\\beta=1\\).\nhead(MCLoutput) ## [1] -5.373153 -3.740134 -88.112201 53.205845 -4.410669 21.850910 mean(MCLoutput) ## [1] -0.2121396 Since this process is iterated at every time step, and ran 1000 times on top of that, it will be efficient for testing the efficiency of parallel computing. We can also construct a foreach loop that will carry out the same task. The foreach function is supplied by the foreach library. It is similar to a for loop but does not depend on each previous iteration of the loop. Instead, foreach runs the contents of the loop in parallel a specified number of times.\nlibrary(foreach) library(doParallel) ## Loading required package: iterators registerDoParallel(no.cores) FEoutput = foreach(i=1:1000) %dopar% { y = arma11(initx=0.5,N=1000) head(y,1) - tail(y,1) } The foreach loop that has been set up performs the same process as the arma_wrapper function earlier at each iteration, it simulates an \\(ARMA(1,1)\\) process with \\(N=1000\\) time steps 1000 times.\nhead(unlist(FEoutput)) ## [1] -22.303263 -7.588862 20.798046 41.559121 14.506028 29.758766 mean(unlist(FEoutput)) ## [1] -0.8435657 Now that all of the parallel methods are set up, we can time them and compare them to not using parallel at all.\nsystem.time(mclapply(xvals, arma_wrapper, mc.cores = no.cores)) ## user system elapsed ## 5.749 0.835 2.056 system.time(foreach(i=1:1000) %dopar% { y = arma11(initx=0.5,N=1000) head(y,1) - tail(y,1) }) ## user system elapsed ## 6.023 0.882 1.847 system.time(for(i in 1:1000){ arma11(initx=0.5,N=1000) }) ## user system elapsed ## 4.795 0.007 4.802 This shows that the fastest method is mclapply, which is different from the normal case of apply being slower than a simple loop. Both methods significantly sped up computation time against the non-parallel version.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"4faa3c572a702d2b2eae46459e23b050","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/common/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/common/","section":"portfolios","summary":"Performing operations on vectors In general, there are three methods that can be used to perform the same operation (such as a mathematical operation) on every element in a vector \\(\\boldsymbol{x}\\).","tags":null,"title":"Common R","type":"docs"},{"authors":null,"categories":null,"content":" R can be interfaced with both C and C++, coming with a significant speed up in computation time and efficiency. R has an inbuilt system for calling C with the .Call function, and RCpp can be used to integrate more easily with C++, enabling the use of pre-existing functions similar to those in base R. This portfolio will detail how this integration between R and C is possible effectively through the use of examples in a statistical sense.\nAdaptive Kernel Regression Smoothing In the situation where we have some data which can not fit into a conventional linear model, we can employ the use of kernel smoothing to fit a more flexible model. More specifically, suppose the data has been generated from the following model \\[ y_i = \\sin(\\alpha \\pi x^3) + \\epsilon_i, \\qquad \\text{ with } \\qquad \\epsilon_i \\sim N(0, \\sigma^2), \\] where \\(x\\) is a uniformly random sample, and \\(\\alpha\\) and \\(\\sigma\\) are fixed parameters. This simulated data can be generated in R with parameter values \\(\\alpha = 4\\) and \\(\\sigma=0.2\\), for \\(i=1,\\dots, n\\):\nset.seed(998) n = 200 x = runif(n) y = sin(4*pi*x^3) + rnorm(n, 0, 0.2) plot(x, y, pch = 20) From the plot it is clear that the data are non-linear, so a simple linear model would not be suitable. A kernel regression smoother can be used to estimate the conditional expectation \\(\\mu(x) = \\mathbb{E}(y|x)\\) more flexibly using \\[ \\hat{\\mu}(x) = \\frac{\\sum^n_{i=1}\\kappa_\\lambda (x, x_i)y_i}{\\sum^n_{i=1}\\kappa_\\lambda(x, x_i)}, \\] where \\(\\kappa\\) is a kernel with bandwidth \\(\\lambda \u0026gt; 0\\). This kernel regression can be written in R as well as C, so it provides opportunity for comparison between the two languages.\nWriting a function in C We can write a C function to implement a Gaussian kernel with variance \\(\\lambda^2\\), shown below.\n#include \u0026lt;R.h\u0026gt; #include \u0026lt;Rinternals.h\u0026gt; #include \u0026lt;Rmath.h\u0026gt; SEXP meanKRS(SEXP y, SEXP x, SEXP x0, SEXP lambda) { int n, n0; double lambda0; double *outy, *x00, *xy, *y0; n0 = length(x0); x = PROTECT(coerceVector(x, REALSXP)); y = PROTECT(coerceVector(y, REALSXP)); SEXP out = PROTECT(allocVector(REALSXP, n0)); n = length(x); outy = REAL(out); lambda0 = REAL(lambda)[0]; x00 = REAL(x0); xy = REAL(x); y0 = REAL(y); for(int i=0; i\u0026lt;n0; i++) { double num = 0, den = 0; for(int j=0; j\u0026lt;n; j++) { num += dnorm(xy[j], x00[i], lambda0, 0)*y0[j]; den += dnorm(xy[j], x00[i], lambda0, 0); } outy[i] = num/den; } UNPROTECT(3); return out; }  This is a large, and quite complicated function to what would normally be implemented in R. The main reason for the length of the function is due to the amount of declarations that have to be made in C. At the start, all integers and doubles are declared, with a * signifying that these are pointers, which instead point to a location in memory rather than being a variable themselves. x, y and out are all defined as vectors; either defining the inputs or allocating a new vector, with PROTECT signifying that these locations in memory are protected. Then the pointers are set up with the REAL function deciding where to point, and the [0] index meaning to take the value of where is being pointed to instead of setting up a pointer.\nAfter this, the actual kernel smoothing is calculated within two for loops, as there is no global definition for sum in C. The numerator num and denominator den are defined on each iteration of the outer loop, and they are added to themselves in the inner loop. Once the inner loop iterations are complete, the value of out is assigned as the division of these two (which is pointed to by outy). Finally, the protected vectors are unprotected, to free up memory.\nThe headers at the beginning of the file, written as #include \u0026lt;header.h\u0026gt; allow the inclusion of certain pre-written functions that make writing C code simpler. Using the R, Rinternals and Rmath were not all necessary, however Rmath provided the dnorm function that was necessary for the Gaussian kernel.\nAlso note that the use of a double for loop is not inefficient in C as it would be in R, so there is no significant slowdown coming from nested loops. Now the function is written, it can be saved into the current working directory and loaded into R.\nsystem(\u0026quot;R CMD SHLIB meanKRS.c\u0026quot;) dyn.load(\u0026quot;meanKRS.so\u0026quot;) A wrapper function can be set up to clean up code, i.e. an R function can be made which explicitly calls the C function with the same arguments. After this, we can plot the results to see how the smoothing looks with an arbitrary value of \\(\\lambda^2 = 0.02\\).\nmeanKRS = function(y, x, x0, lambda) .Call(\u0026quot;meanKRS\u0026quot;, y, x, x0, lambda) plot(x, y, pch = 20) lines(seq(0, 1, len=1000), meanKRS(y, x, seq(0,1,len=1000), lambda = 0.02), col=\u0026quot;blue\u0026quot;, lwd=2) So this kernel smoothing approach has provided a nice fit to the simulated data, and appears to be going through the centre of the points across the plot.\nTo compare the computational efficiency, we can compare this C function with a simpler function written in R, called meanKRS_R (function not shown here for brevity, the R function performs the same operation whilst making use of R’s sum function instead of having a loop).\nall.equal(meanKRS_R(y,x,seq(0,1,len=1000),0.06), meanKRS(y,x,seq(0,1,len=1000),0.06)) ## [1] TRUE So the values are exactly the same, and the function is accurate. How much quicker is it? We can use microbenchmark to display summary statistics of timing both functions 100 times.\nlibrary(microbenchmark) microbenchmark(C_KRS = meanKRS(y,x,seq(0,1,len=1000),0.06), R_KRS = meanKRS_R(y,x,seq(0,1,len=1000),0.06), times = 100) ## Unit: milliseconds ## expr min lq mean median uq max neval ## C_KRS 14.32545 16.17882 16.36242 16.27690 16.42271 20.37999 100 ## R_KRS 30.25504 32.08685 33.45924 32.42864 34.38512 43.47946 100 So the C code is significantly faster, as expected, by about a factor of two.\n Implementing cross-validation The value of \\(\\lambda\\) used here was chosen arbitrarily, but in practice it is common to use \\(k\\)-fold cross-validation to choose a more optimal value of \\(\\lambda\\). We can implement a cross-validation routine in C. Implementing a function within a function is difficult in C, but it is relatively straightforward in Rcpp. This example goes through the use of Rcpp to implement cross-validation.\nRcpp is a package in R which provides tools to implement C++ code in R. It works in a similar way to how the .Call interface works for C code, but with a more accessible interface. Firstly, we create a file in the working directory called KRS_cv.cpp, where the Rcpp functions will be stored. This file contains a few functions, the first of which being the meanKRS function re-written for Rcpp.\n// [[Rcpp::export(name = \u0026quot;meanKRS\u0026quot;)]] NumericVector meanKRS_I(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda) { int n, n0; n0 = x0.size(); n = x.size(); NumericVector out(n0); for(int i=0; i\u0026lt;n0; i++) { double num = 0, den = 0; NumericVector dval = dnorm(x, x0[i], lambda, 1); double max_dval = max(dval); for(int j=0; j\u0026lt;n; j++) { num = num + exp(dval[j]-max_dval)*y[j]; den = den + exp(dval[j]-max_dval); } out[i] = num/den; } return out; } The cross-validation function cvKRS is written as:\n// [[Rcpp::export(name = \u0026quot;cvKRS\u0026quot;)]] NumericVector cvKRS_I(const NumericVector y, const NumericVector x, const int k, const NumericVector lambdas) { // declare variables and vectors int n = y.size(); NumericVector mse_lambda(lambdas.size()); NumericVector out(1); NumericVector sorted_x(n); NumericVector sorted_y(n); // sort x and y according to the order of x IntegerVector order_x = stl_order(x); for(int kk=0; kk \u0026lt; n; kk++) { int ind = order_x[kk]-1; sorted_y[kk] = y[ind]; sorted_x[kk] = x[ind]; } // set up indices to cross-validate for IntegerVector idxs = seq_len(k); IntegerVector all_idxs = rep_each(idxs, n/k); // different lambdas for(int jj = 0; jj \u0026lt; lambdas.size(); jj++) { double lambda = lambdas[jj]; NumericVector mse(k); // cross-validation loop for(int ii=1; ii \u0026lt;= k; ii++) { const LogicalVector kvals = all_idxs != ii; NumericVector y_t = clone(sorted_y); NumericVector x_t = clone(sorted_x); NumericVector y_cross = y_t[kvals]; NumericVector x_cross = x_t[kvals]; NumericVector fit = meanKRS_I(y_cross, x_cross, sorted_x, lambda); // calculate mean squared error NumericVector error = pow((fit[!kvals] - sorted_y[!kvals]), 2); mse[ii-1] = mean(error); } // average mean squared error for each value of lambda mse_lambda[jj] = mean(mse); } // output lambda which gave the smallest mean squared error int best_pos = which_min(mse_lambda); out[0] = lambdas[best_pos]; return out; } Comments within the function (succeeding a //) give explanation of each section of the function. This function also calls the other function meanKRS by using meanKRS_I. The function was defined as meanKRS_I in the .cpp file, but when exported into R it is defined as meanKRS, based on the comment preceeding the function definition. Both functions were defined within the same file, and so can call one another, provided the function being called is defined first. There is a function used within cvKRS that is not part of base Rcpp functionality: stl_order. This does the same thing as order in base R, but needed to be defined separately.\nNow that the function is defined, we can call it within R by first loading the Rcpp library and then using the sourceCpp function to source the C++ file, located in the same working directory.\nlibrary(Rcpp) sourceCpp(\u0026quot;KRS_cv.cpp\u0026quot;) This is the stage where we would get compilation errors, if there were any. Now we can run the function over a series of \\(\\lambda\\) values and plot the kernel regression over the simulated data for the optimal \\(\\lambda\\) selected by cross-validation.\nlambda_seq = exp(seq(log(1e-6),log(100),len=50)) best_lambda = cvKRS(y, x, k = 20, lambdas = lambda_seq) plot(x, y, pch = 20) lines(seq(0, 1, len=1000), meanKRS(y, x, seq(0,1,len=1000), best_lambda), col=\u0026quot;deeppink\u0026quot;, lwd=2) To compare computational efficiency, we can benchmark speeds against a function written in R (not shown here).\nmicrobenchmark(cvKRS(y, x, k = 20, lambdas = lambda_seq), cvKRS_R(y, x, k = 20, lambdas = lambda_seq), times = 5) ## Unit: seconds ## expr min lq mean ## cvKRS(y, x, k = 20, lambdas = lambda_seq) 2.289333 2.293236 2.660317 ## cvKRS_R(y, x, k = 20, lambdas = lambda_seq) 6.390262 6.629278 7.100683 ## median uq max neval ## 2.380029 3.141275 3.197711 5 ## 7.481269 7.486666 7.515939 5 So this further exemplifies the significant speed increase that comes with writing in C, or Rcpp. In fact, this function had an average of three times the speed of that of the R function, against the two times speed up the previous function had. This is likely due to using two functions written in C++ (as cross validation calls the original kernel regression function).\nThis has improved the fit, but no single value of \\(\\lambda\\) leads to a satisfactory fit, due to the first half of the function (for \\(x \u0026lt; 0.5\\)) wanting a smooth \\(\\mu(x)\\) as it is quite linear, and the second half wanting a less smooth one, as it is more ‘wiggly’. We can let the smoothness depend on \\(x\\) by constructing \\(\\lambda(x)\\), which will improve the fit. This method is based around modelling the residuals and varying \\(\\lambda\\) more when the residuals are larger. Thus in practice \\(\\lambda(x)\\) is a sequence of values instead of a single value. Below are the contents of the file that contains the functions to implement this written in Rcpp.\n#include \u0026lt;Rcpp.h\u0026gt; using namespace Rcpp; NumericVector fitKRS(const NumericVector x, const NumericVector x0, const double lambda, const NumericVector y, const NumericVector lambda_vec) { NumericVector copy_y = clone(y); int n = x.size(); int n0 = x0.size(); NumericVector out(n0); for(int ii=0; ii\u0026lt;n0; ii++) { NumericVector dval = dnorm(x, x0[ii], lambda*lambda_vec[ii], 1); double max_dval = max(dval); double num=0, den=0; for(int jj=0; jj\u0026lt;n; jj++) { num = num + exp(dval[jj] - max_dval)*copy_y[jj]; den = den + exp(dval[jj] - max_dval); } out[ii] = num/den; } return out; } // [[Rcpp::export(name = \u0026quot;mean_var_KRS\u0026quot;)]] NumericVector mean_var_KRS_I(const NumericVector y, const NumericVector x, const NumericVector x0, const double lambda) { int n = x.size(); int n0 = x0.size(); NumericVector res(n); NumericVector lambda_1sn(n, 1.0); NumericVector lambda_1sn0(n0, 1.0); NumericVector mu = fitKRS(x, x, lambda, y, lambda_1sn); NumericVector resAbs = abs(y - mu); NumericVector madHat = fitKRS(x, x0, lambda, resAbs, lambda_1sn0); NumericVector w = 1 / madHat; w = w / mean(w); NumericVector out = fitKRS(x, x0, lambda, y, w); return out; } The first function, fitKRS was used to save space, since the same operation is performed multiple times with different parameters. Different weightings w get added to the vector of \\(\\lambda\\) values in the final stage, resulting in the varied \\(\\lambda(x)\\) parameter. We can plot this to show this change, using the initial value of \\(\\lambda\\) selected by cross-validation earlier.\nsourceCpp(\u0026quot;meanvarKRS.cpp\u0026quot;) varied_mu = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = best_lambda) plot(x, y, pch=20) lines(seq(0, 1, len=1000), varied_mu, col = \u0026quot;darkgoldenrod\u0026quot;, lwd=2) This looks like a good fit, and the function is working as intended. How well does the speed of the function written in Rcpp compare to one written in R?\nmicrobenchmark(C = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = best_lambda), R = mean_var_KRS_R(y = y, x = x, x0 = seq(0,1,len=1000), lam = best_lambda), times = 500) ## Unit: milliseconds ## expr min lq mean median uq max neval ## C 25.56793 35.80579 41.67632 39.84551 45.09670 84.63745 500 ## R 35.03011 54.59121 64.87276 61.94020 70.77707 122.68250 500 This has an expected speed up again. And to make sure both functions are outputting the same thing, we can use all.equal again:\nall.equal(mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = best_lambda), mean_var_KRS_R(y = y, x = x, x0 = seq(0,1,len=1000), lam = best_lambda)) ## [1] TRUE  Cross Validation Again The value of \\(\\lambda\\) used for fitting this local regression is that picked from cross-validation where \\(\\lambda\\) does not vary with \\(x\\), in the previous section. To improve this fit further, we can use cross-validation again, but fitting the model with the new mean_var_KRS function on each iteration instead of the basic kernel regression. The function is written in Rcpp below.\n// [[Rcpp::export(name = \u0026quot;mean_var_cv_KRS\u0026quot;)]] NumericVector mean_var_cv_KRS_I(const NumericVector y, const NumericVector x, const int k, const NumericVector lambdas) { int n = y.size(); NumericVector mse_lambda(lambdas.size()); NumericVector out(1); NumericVector sorted_x(n); IntegerVector order_x = stl_order(x); NumericVector test; NumericVector sorted_y(n); for(int kk=0; kk \u0026lt; n; kk++) { int ind = order_x[kk]-1; sorted_y[kk] = y[ind]; sorted_x[kk] = x[ind]; } IntegerVector idxs = seq_len(k); IntegerVector all_idxs = rep_each(idxs, n/k); for(int jj = 0; jj \u0026lt; lambdas.size(); jj++) { double lambda = lambdas[jj]; NumericVector mse(k); for(int ii=1; ii \u0026lt;= k; ii++) { const LogicalVector kvals = all_idxs != ii; NumericVector y_t = clone(sorted_y); NumericVector x_t = clone(sorted_x); NumericVector y_cross = y_t[kvals]; NumericVector x_cross = x_t[kvals]; NumericVector fit = mean_var_KRS_I(y_cross, x_cross, sorted_x, lambda); NumericVector error = pow((fit[!kvals] - sorted_y[!kvals]), 2); mse[ii-1] = mean(error); } mse_lambda[jj] = mean(mse); } int best_pos = which_min(mse_lambda); out[0] = lambdas[best_pos]; return out; } This functions is very similar to the cross-validation function implemented earlier. It loops over different values of \\(\\lambda\\), but instead uses these as a starting point to fit a series of points using \\(\\lambda(x)\\). The error is calculated against the cross-validated points and the starting \\(\\lambda\\) that results in the smallest error is returned. Now we can call this in R, and re-run the mean_var_KRS function with the newly selected \\(\\lambda\\) and see how it compares.\nsourceCpp(\u0026quot;meanvarcvKRS.cpp\u0026quot;) cv_best_lambda = mean_var_cv_KRS(y, x, k=20, exp(seq(log(0.01), log(1), len=50))) cv_varied_mu = mean_var_KRS(y = y, x = x, x0 = seq(0,1,len=1000), lambda = cv_best_lambda) plot(x, y, pch=20) lines(seq(0, 1, len=1000), cv_varied_mu, col = \u0026quot;darkorchid\u0026quot;, lwd=2) This looks a lot smoother for lower values of \\(x\\) than before, which looks like a better fit overall!\nA way to improve this model might involve a local regression approach, which would be fitting parameter values at different (possibly uniform) intervals across the data set. Local regression will be covered in the next section.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"426222779ec1355ec801d59cce440d98","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/rnc/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc2/rnc/","section":"portfolios","summary":"R can be interfaced with both C and C++, coming with a significant speed up in computation time and efficiency. R has an inbuilt system for calling C with the .","tags":null,"title":"Integrating R and C","type":"docs"},{"authors":null,"categories":null,"content":" Introduction Rcpp sugar and Armadillo are libraries included in Rcpp that allow different processes:\n Sugar provides an array of basic functions in Rcpp that are similar to inbuilt base R functions. These include functions such as cbind, sum, sin, sample and many more. These are essential for writing simple code in Rcpp. Armadillo is a package used for linear algebra operations in Rcpp. It allows specification of matrices, 3D matrices, vectors and others.  This portfolio will explain the use of key functions and features of Armadillo and sugar by implementing local polynomial regression on a data set on solar electricity production in Sydney. We can load this data into the R environment first.\nload(\u0026quot;solarAU.RData\u0026quot;) head(solarAU) ## prod toy tod logprod ## 8832 0.019 0.000000e+00 0 -3.540459 ## 8833 0.032 5.708088e-05 1 -3.170086 ## 8834 0.020 1.141618e-04 2 -3.506558 ## 8835 0.038 1.712427e-04 3 -3.036554 ## 8836 0.036 2.283235e-04 4 -3.079114 ## 8837 0.012 2.854044e-04 5 -3.816713 The column prod is a production measure, and logprod is the log of the production measure, which will be the response variable. The covariates are toy - the time of year, within 0 and 1 as a percentage, and tod - the time of day, from 0 to 47, measured in half an hour intervals. A simple polynomial regression model is of the form \\[ \\mathbb{E}(y|\\boldsymbol{x}) = \\beta_0 + \\beta_1\\text{tod} + \\beta_2\\text{tod}^2 + \\beta_3\\text{toy} + \\beta_4 \\text{toy}^2 = \\tilde{\\boldsymbol{x}}\\boldsymbol{\\beta}. \\]\n Fitting a linear regression model We can use Armadillo to fit a linear regression model, and to solve the least squares optimisation problem \\[ \\hat{\\boldsymbol{\\beta}} := \\operatorname{argmin}_{\\boldsymbol{\\beta}}\\|{\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\|}^2, \\] which has solution \\[ \\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}. \\] This can be implemented in C using QR decomposition of \\(\\boldsymbol{X}\\).\nlibrary(Rcpp) sourceCpp(code=\u0026#39; // [[Rcpp::depends(RcppArmadillo)]] #include \u0026lt;RcppArmadillo.h\u0026gt; #include \u0026lt;Rcpp.h\u0026gt; using namespace Rcpp; // [[Rcpp::export(name=\u0026quot;lm_cpp\u0026quot;)]] arma::vec lm_cpp_I(arma::vec\u0026amp; y, arma::mat\u0026amp; X) { arma::mat Q, R; arma::qr_econ(Q, R, X); arma::vec beta = solve(R, (trans(Q) * y)); return beta; }\u0026#39;) ls = function(formula, data){ y = data[,all.vars(formula)[1]] x = model.matrix(formula, data) lm_cpp(y, x) } Note that the sourceCpp function has a differing argument code, where instead of reading code from a file in the directory, the code is supplied directly here. Running sourceCpp adds the lm_cpp function to the environment. An R function is set up which takes the inputs formula and data and runs the Rcpp function with the given inputs. This makes it comparable to lm. Firstly though, we can see that both functions output the same parameter estimates.\nls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU) ## [,1] ## [1,] -6.26275685 ## [2,] 0.86440391 ## [3,] -0.01757599 ## [4,] -5.91806924 ## [5,] 6.14298863 lm(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU)$coef ## (Intercept) tod I(tod^2) toy I(toy^2) ## -6.26275685 0.86440391 -0.01757599 -5.91806924 6.14298863 But which is faster?\nmicrobenchmark( R_lm = lm(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU), C_lm = ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU), times = 500 ) ## Unit: milliseconds ## expr min lq mean median uq max neval ## R_lm 3.531021 4.047208 5.079068 4.205777 5.222140 57.54048 500 ## C_lm 2.522135 2.927327 3.615145 3.026044 3.654492 37.26725 500 So the C code is approximately twice as fast with this method, and would be even faster without the R wrapper function. However, the R function lm performs a number of checks and computes a number of statistics that the C++ code does not, which explains part of the performance gap.\nFor a more fair comparison, we can set up the model matrices in advance, and perform the same operations in R and in RCpp.\nR_ls_solve = function(y,X){ QR = qr(X) beta = solve(qr.R(QR), (t(qr.Q(QR)) %*% y)) return(beta) } y = solarAU$logprod X = with(solarAU, cbind(1, tod, tod^2, toy, toy^2)) microbenchmark(R_solve = R_ls_solve(y,X), C_solve = lm_cpp(y, X), times = 500) ## Unit: microseconds ## expr min lq mean median uq max neval ## R_solve 1910.686 2189.3570 5022.8070 2464.270 3779.8105 79083.490 500 ## C_solve 449.401 518.7625 690.3906 584.342 687.4495 8261.433 500 So this has come with an approximate speed up of ten times, exemplifying how much more computationally efficient code written in C++ is over R. However, there is a (non-computational) problem with the model. A plot of the residuals from the linear model shows this.\nbeta = ls(logprod ~ tod + I(tod^2) + toy + I(toy^2), data = solarAU) res = y - X %*% beta predplot = ggplot(solarAU, mapping = aes(x=toy, y=tod, z= X%*%beta)) + stat_summary_hex() + xlab(\u0026quot;Time of Year\u0026quot;) + ylab(\u0026quot;Time of Day\u0026quot;) + theme(legend.title = element_blank()) resplot = ggplot(solarAU, mapping=aes(x=toy, y = tod, z = res)) + stat_summary_hex() + xlab(\u0026quot;Time of Year\u0026quot;) + ylab(\u0026quot;Time of Day\u0026quot;) + theme(legend.title = element_blank()) grid.arrange(predplot, resplot, ncol=2) There is a pattern in the residuals! This means that there is a feature not included in the model that can explain some of the noise. We need to extend this model to account for this.\n Local Regression We can improve the model fit by adopting a local regression approach, that is, making the parameter estimates depend on the covariates \\(\\boldsymbol{x}\\), i.e. \\(\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}(\\boldsymbol{x})\\). This is achieved by minimising \\(\\hat{\\boldsymbol{\\beta}}(\\boldsymbol{x}_0)\\) for a fixed \\(\\boldsymbol{x}_0\\): \\[ \\hat{\\boldsymbol{\\beta}}(\\boldsymbol{x}_0) = \\operatorname{argmin}_{\\boldsymbol{\\beta}} \\sum^n_{i=1}\\kappa_{\\boldsymbol{H}}(\\boldsymbol{x}_0 - \\boldsymbol{x}_i)(y_i-\\tilde{\\boldsymbol{x}}_i^T\\boldsymbol{\\beta})^2, \\] for a density kernel \\(\\kappa_{\\boldsymbol{H}}\\) with positive definite bandwidth matrix \\(\\boldsymbol{H}\\). Fitting this model involves re-fitting the linear model once for each row in the data set, which for large data sets is not viable, but shows the need of computationally efficient code in C++. Now we can write the local regression function in RCpp.\nvec local_lm_I(vec\u0026amp; y, rowvec x0, rowvec X0, mat\u0026amp; x, mat\u0026amp; X, mat\u0026amp; H) { mat Hstar = chol(H, \u0026quot;lower\u0026quot;); vec w = dmvnInt(x, x0, Hstar); vec beta = lm_cpp_I(y % sqrt(w), X.each_col() % sqrt(w)); return X0 * beta; } // [[Rcpp::export(name=\u0026quot;local_lm_fit\u0026quot;)]] vec local_lm_fit_I(vec\u0026amp; y, mat x0, mat X0, mat\u0026amp; x, mat\u0026amp; X, mat\u0026amp; H) { int n0 = x0.n_rows; vec out(n0); for(int ii=0; ii \u0026lt; n0; ii++) { rowvec x00 = x0.row(ii); rowvec X00 = X0.row(ii); out(ii) = as_scalar(local_lm_I(y, x00, X00, x, X, H)); if(ii % 50 == 0) {R_CheckUserInterrupt();} } return out; } These two functions, as well as lm_cpp from earlier all combine to implement local regression. local_lm_fit_I loops over all rows in x0 and X0 and fits linear regression for a constant \\(\\boldsymbol{x}_0\\), using weights from a Gaussian kernel, imeplemented by a multivariate normal density (given by dmvnInt, defined separately). local_lm_I simply calculates the weights and pre-multiplies them by \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{X}\\) to go into the fitting function. We can source these functions and run these for a subsetted data set.\nsourceCpp(\u0026#39;lm_cpp.cpp\u0026#39;) nsub = 2000 sub = sample(1:nrow(solarAU), nsub) y = solarAU$logprod x = as.matrix(solarAU[c(\u0026quot;tod\u0026quot;, \u0026quot;toy\u0026quot;)]) X = model.matrix(~tod+toy+I(tod^2)+I(toy^2),data=solarAU) x0 = x[sub, ] X0 = X[sub, ] H = diag(c(1,0.01)) cpp_pred_local = local_lm_fit(y, x0, X0, x, X, H) Of which we can plot, for both the predictions and the residuals.\npredPlot = ggplot(mapping=aes(x=x0[,2], y = x0[,1], z = cpp_pred_local)) + stat_summary_hex() + xlab(\u0026quot;Time of Year\u0026quot;) + ylab(\u0026quot;Time of Day\u0026quot;) + theme(legend.title = element_blank()) resPlot = ggplot(mapping=aes(x=x0[,2], y = x0[,1], z = y[sub] - cpp_pred_local)) + stat_summary_hex() + xlab(\u0026quot;Time of Year\u0026quot;) + ylab(\u0026quot;Time of Day\u0026quot;) + theme(legend.title = element_blank()) grid.arrange(predPlot, resPlot, ncol=2) These look a lot better! There isn’t a systematic pattern in these residuals which show that the model isn’t missing anything important. Now we can check this C++ code against the basic R code.\nlibrary(mvtnorm) lmLocal \u0026lt;- function(y, x0, X0, x, X, H){ w \u0026lt;- dmvnorm(x, x0, H) fit \u0026lt;- lm(y ~ -1 + X, weights = w) return( t(X0) %*% coef(fit) ) } R_pred_local \u0026lt;- sapply(1:nsub, function(ii){ lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H = diag(c(1, 0.1)^2)) }) all.equal(R_pred_local, as.vector(cpp_pred_local)) ## [1] TRUE Since all these predictions are equal in R and Rcpp, how does the speed difference compare? This function takes a long time to run, so we will only time each of them once.\nsystem.time(sapply(1:nsub, function(ii){ lmLocal(y = y, x0 = x0[ii, ], X0 = X0[ii, ], x = x, X = X, H = diag(c(1, 0.1)^2)) })) ## user system elapsed ## 23.527 0.076 23.606 system.time( local_lm_fit(y, x0, X0, x, X, H) ) ## user system elapsed ## 872.113 0.309 145.901 So the Rcpp code has come with an approximate ten times speed up again. However, this model could still be improved. We have chosen the bandwidth matrix \\(\\boldsymbol{H}\\) arbitrarily, but this could be improved with cross-validation. Whilst this is not shown here, a similar approach to that given in section 2 could be implemented.\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7e0548757f01216010fdcb03325cd4aa","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/rcpp/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc2/rcpp/","section":"portfolios","summary":"Introduction Rcpp sugar and Armadillo are libraries included in Rcpp that allow different processes:\n Sugar provides an array of basic functions in Rcpp that are similar to inbuilt base R functions.","tags":null,"title":"Local Polynomial Regression with Rcpp","type":"docs"},{"authors":null,"categories":null,"content":" Introduction The first aspect to being a good programmer is to ensure that your code is reproducible. That is, if an academic unrelated to you, on the other side of the world, were to have access to your conclusions and your dataset, would they be able to reproduce your results? It is important that this is the case, otherwise it may take weeks or even months of effort for someone to catch up to the research that you have already carried out, which makes it difficult for that person to then build on the existing work.\nIf you have supplied your code with any publication you have created, it is also important you follow some basic literate programming guidelines. In short, this ensures that whoever reads your code will be able to figure out what it does without too much effort. This can be done with documentation, proper commenting, or writing an Markdown document.\nIt is surprising nowadays how many scientific articles are released without some form of code alongside them. If code is released alongside a piece of research, every academic who reads that research should be able to reproduce the results in the article without a large amount of hassle. This would allow other scientists to build on the research, and advance the scientific community as a whole. If you don’t include your code however, your research may never be built on, as you would be making it more difficult for someone to enter your research field.\nAn example of literate programming and reproducibility is given below. A least square solver has been implemented on a Prostate Cancer dataset. Firstly, the example goes through the mathematics of least squares estimation, then an example is given on how to code this in R. Each step is explained, and through explaining these steps, it should be possible to reproduce the code through a greater understanding.\n Reproducibility Example: Implementing a least square solver in R A least squares estimator will provide coefficients that make up a function which can be used to predict some form of an output. Calling the output \\(\\boldsymbol{y}\\), the series of inputs \\(\\boldsymbol{x} = (\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_n)^T\\), and the coefficients to be estimated as \\(\\boldsymbol{w} = (w_0, \\boldsymbol{w_1})\\). Here, \\(w_0\\) is the intercept, or the bias parameter and \\(n\\) is the length of the input vector. The equation that we need to solve is: \\[ \\boldsymbol{w}_{LS} := \\text{argmin}_{\\boldsymbol{w}} \\sum_{i \\in D_0} \\left\\{y_i - f(\\boldsymbol{x}_i; \\boldsymbol{w})\\right\\}^2. \\] This has a solution of \\[ \\boldsymbol{w}_{LS} = (\\boldsymbol{XX}^T)^{-1}\\boldsymbol{Xy}^T. \\] Which will minimise the squared error between the actual output values \\(\\boldsymbol{y}\\) and the expected output values given by the input values \\(\\boldsymbol{x}\\).\nProstate Cancer Data To give an example of using a least squares solver, we can use prostate cancer data given by the lasso2 package. If this package is not installed, we can use\ninstall.packages(\u0026quot;lasso2\u0026quot;) to install it. Once this package is installed, we obtain the dataset by running the commands\nlibrary(lasso2) data(Prostate) which gives us the dataset in our R environment. We start by inspecting the dataset, which can be done with\nhead(Prostate) ## lcavol lweight age lbph svi lcp gleason pgg45 lpsa ## 1 -0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 -0.4307829 ## 2 -0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 -0.1625189 ## 3 -0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 -0.1625189 ## 4 -1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 -0.1625189 ## 5 0.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 0.3715636 ## 6 -1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 0.7654678 Here we can see all the different elements of the dataset, which are:\n lcavol: The log of the cancer volume\n lweight: The log of the prostate weight\n age: The individual’s age\n lbph: The log of the benign prostatic hyperplasia amount\n svi: The seminal vesicle invasion\n lcp: The log of the capsular penetration\n gleason: The Gleason score\n pgg45: The percentage of the Gleason scores that are 4 or 5\n lpsa: The log of the prostate specific antigen  We don’t need to understand what all of these are, but we need to define what the inputs and outputs are. We are interested in the cancer volume, so setting \\(\\boldsymbol{y}\\) as lcavol and \\(\\boldsymbol{x}\\) as the other variables would measure how these other variables affect cancer volume. We can set this by running\ny = as.vector(Prostate$lcavol) X = model.matrix(~., data=Prostate) which gives us our inputs and outputs. We are selecting the \\(X\\) matrix going from 2 to dim(Prostate)[2], which means that we are selecting the Prostate dataset from the second column to the last one (not including the response as a predictor). If our output variable was in a different column, we would exclude a different column using similar methods.\nNow that we have set up the inputs and outputs, we can use the solution \\[ \\boldsymbol{w}_{LS} = (\\boldsymbol{XX}^T)^{-1}\\boldsymbol{Xy}^T. \\] to find the coefficients \\(\\boldsymbol{w_{LS}}\\). Converting this equation into R is done with the command:\nwLS = solve(t(X) %*% X) %*% t(X) %*% y The t(X) function simply takes the transpose of the matrix. The %*% function is just selecting matrix multiplication, as opposed to using * which would be element-wise multiplication. The solve function finds the inverse of a matrix, so solve(X) would give \\(X^{-1}\\). Now we have our coefficients\nwLS ## [,1] ## (Intercept) -2.997602e-14 ## lcavol 1.000000e+00 ## lweight -1.214306e-14 ## age -1.049508e-16 ## lbph 6.045511e-16 ## svi -2.952673e-14 ## lcp 4.538037e-15 ## gleason 1.547373e-15 ## pgg45 -2.255141e-17 ## lpsa 1.706968e-15 which can be used to get the predicting function \\(f(\\boldsymbol{x};\\boldsymbol{w})\\) for these given inputs.\nf = X %*% wLS We can see how close the predicted output is to the actual output with a plot. If our predictions are accurate, then they will be close to the actual output, and lie on a diagonal line. We can plot these two values against each other using the command:\nplot(f,y,xlab = \u0026quot;Predicted Values\u0026quot;, ylab = \u0026quot;Actual Values\u0026quot;, main = \u0026quot;Log of Cancer Volume\u0026quot;) abline(0,1) These match up almost perfectly, so our predictions are accurate.\n Cross-validation Cross-validation is a general purpose method for evaluating the method of the predicting function \\(f(\\boldsymbol{x};\\boldsymbol{w})\\). This is done by leaving \\(n\\) points out of the dataset, fitting the coefficients to the remaining dataset, then using the new coefficients to create a new predicting function which can predict the data point that was missed out. The difference between this predicted point and the actual observed point can be used as a metric to judge the accuracy of the predicting function.\nIn more technical terms, we split the dataset \\(D\\) into \\(k\\) distinct subsets \\(D_1, \\dots, D_k\\), and fit \\(f_{-i}(\\boldsymbol{x}_{-i};\\boldsymbol{w})\\) for each dataset and for \\(i=0,\\dots k\\). We then take the squared error between \\(y_i\\) and \\(f(\\boldsymbol{x}_{i};\\boldsymbol{w})\\), and average across all subsets, i.e. \\[ \\frac{1}{k+1} \\sum^k_{i=1} \\left[ f(\\boldsymbol{x}_{i};\\boldsymbol{w}) - y_i \\right]^2 \\] To do this in R, we can set up a loop that will create a new data subset on each iteration, carry out the same procedure as detailed before to obtain the coefficients and the predicting function, measure the error and repeat \\(k\\) times. This is done with:\nN = length(y) error = numeric(N) for(i in 1:N){ Xmini = X[-i,] ymini = y[-i] wLSmini = solve(t(Xmini) %*% Xmini) %*% t(Xmini) %*% ymini fout = X[i,] %*% wLSmini error[i] = (fout - y[i])^2 } Using the indexing of X[-i,] will subset X to all rows except the \\(i\\)-th one. With this reduced data set, as well as only using the \\(y_{-i}\\) inputs, we estimate the coefficients at each iteration and then use that to calculate the squared error, which is saved in the error array. To find the overall error, we just take the mean of this array.\nmean(error) ## [1] 3.701126e-27  Removing Features We can use this overall squared error as a reference point, because it can be compared against the same score when removing certain inputs (columns of X) to see if the prediction function is more accurate without some of these included. We can do this by setting up another loop that loops over the number of inputs, and performing the same methods as before to calculate the averages. This is done with:\nD = dim(X)[2] error_all = numeric(D-1) for(d in 2:D){ Xd = X[,-d] error_d = numeric(N) for(i in 1:(dim(X)[1])){ Xmini = Xd[-i,] ymini = y[-i] wLSmini = solve(t(Xmini) %*% Xmini) %*% t(Xmini) %*% ymini fout = Xd[i,] %*% wLSmini error_d[i] = (fout - y[i])^2 } error_all[d-1] = mean(error_d) }  So this starts with setting the array error_all to the length of the number of inputs. Then there is a loop which iterates over the number of inputs (excluding the first column, which corresponds to the intercept), and removes a column from X, renaming it Xd. Another array is set up, called error_d, which is equivalent to the error array from the previous section. The nested loop performs the same operation as the cross-validation before. We can inspect this array, and compare it against the previous cross-validation error:\nerror_all ## [1] 5.351267e-01 2.736666e-27 3.654340e-27 3.473490e-27 3.802165e-27 ## [6] 4.893871e-27 9.457742e-28 1.465294e-27 3.635496e-27 mean(error) ## [1] 3.701126e-27 So we can see that some of the cross-validation errors with a feature removed has a lower error than the original error. We can remove these variables if we want to, as it would reduce the cross-validation error. However, there is grounds for keeping the variables in the model, as the difference between errors is rather small, which we can see here.\nrbind(colnames(X)[2:d],(error_all-mean(error))) ## [,1] [,2] [,3] ## [1,] \u0026quot;lcavol\u0026quot; \u0026quot;lweight\u0026quot; \u0026quot;age\u0026quot; ## [2,] \u0026quot;0.535126651390945\u0026quot; \u0026quot;-9.64460028673121e-28\u0026quot; \u0026quot;-4.67853198901128e-29\u0026quot; ## [,4] [,5] [,6] ## [1,] \u0026quot;lbph\u0026quot; \u0026quot;svi\u0026quot; \u0026quot;lcp\u0026quot; ## [2,] \u0026quot;-2.27635884835842e-28\u0026quot; \u0026quot;1.01038873950454e-28\u0026quot; \u0026quot;1.1927448494611e-27\u0026quot; ## [,7] [,8] [,9] ## [1,] \u0026quot;gleason\u0026quot; \u0026quot;pgg45\u0026quot; \u0026quot;lpsa\u0026quot; ## [2,] \u0026quot;-2.75535156416629e-27\u0026quot; \u0026quot;-2.23583162435088e-27\u0026quot; \u0026quot;-6.5629761986902e-29\u0026quot; This shows what the cross-validation error would be if we removed each of these predictors. We would generally prefer a smaller model, and the covariates with a low magnitude are likely not providing much information in the model, so we can exclude them. Overall, this would leave us with lcp and lpsa.\nThis is only accounting for removing one predictor and leaving the others in. We could now want to consider what different combinations of input variables we could include that would result in the lowest overall error. Another thing to consider is to relax the condition of linear least squares, so that the output \\(\\boldsymbol{y}\\) could depend on some feature transform \\(\\phi(\\boldsymbol{x})\\), which could improve the accuracy. In practice, this is an incredibly large amount of combinations, and would be nearly impossible to compare the different errors that would result in an overall ‘optimal’ model.\n Conclusions Using the least squares method, we have reduced the model down to having two inputs, the log of the capsular penetration and the log of the prostate specific antigen. These were the only covariates that provided sufficient information about the output, the log of the cancer volume. The parameter estimates for these covariates in a reduced model is\nX.new = model.matrix(~lcp+lpsa,data=Prostate) wLS.new = solve(t(X.new) %*% X.new) %*% t(X.new) %*% y wLS.new ## [,1] ## (Intercept) 0.09134598 ## lcp 0.32837479 ## lpsa 0.53162109 These are reasonably large and positive values, meaning that if an individual has higher values of lcp and lpsa, they likely of having a higher cancer volume.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"dee0c4162c380dcca9011dbdcc3c929c","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/reproducibility/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/reproducibility/","section":"portfolios","summary":"Introduction The first aspect to being a good programmer is to ensure that your code is reproducible. That is, if an academic unrelated to you, on the other side of the world, were to have access to your conclusions and your dataset, would they be able to reproduce your results?","tags":null,"title":"Reproducibility","type":"docs"},{"authors":null,"categories":null,"content":" RStudio can be used to efficiently make a package in R, and allows an accessible way of implementing version control and git integration. As an example of these processes, I have created an R package which implements a basic form of least squares regression. The following function obtains parameter estimates given a dataset and formula:\nLS.model = function(formula, data){ ys = all.vars(formula)[1] y = data[,ys] X = model.matrix(formula, data) wLS = solve(t(X) %*% X) %*% t(X) %*% y return(list(Parameters = wLS, df = X, y = y)) } Other functions in the package include LS.predict to get predictions and LS.plot to plot the predicting function on top of the data.\nLS.predict = function(model, newdata=NULL){ if(is.null(newdata)) return(model$df %*% model$Parameters) if(!is.null(newdata)) { nd = cbind(1,newdata) return((nd) %*% model$Parameters) } } LS.plot = function(model, var = NULL, ...){ X = model$df y = model$y d = dim(X) if(is.null(var)){ print(\u0026quot;var not specified, taking first input value\u0026quot;) names = colnames(X)[colnames(X)!=\u0026quot;(Intercept)\u0026quot;] var = names[1] } preds = LS.predict(model) o = order(preds) plot(X[,var], y, xlab = var, ...) lines(X[o,var], preds[o], col=\u0026quot;red\u0026quot;, lwd=2) } Least Squares Example To see the usage of this package, see the following example using the prostate cancer dataset from the lasso2 package. Firstly, starting by fitting the model:\nlibrary(lasso2) data(Prostate) fit = LS.model(lpsa ~ lcavol, data = Prostate) The output fit can be passed into LS.plot and LS.predict.\nhead(LS.predict(fit)) ## [,1] ## 1 1.0902222 ## 2 0.7921115 ## 3 1.1398502 ## 4 0.6412553 ## 5 2.0478064 ## 6 0.7521390 LS.plot(fit, var=\u0026quot;lcavol\u0026quot;)  Creating the Package RStudio allows the creation of a package to be relatively straightforward, with an option to create a template for an R package. The package structure consists of:\n DESCRIPTION: plain text file that contains information about the title of the package, the author, version etc. LICENSE: description of copyright and licensing information for the package NAMESPACE: describes imports and exports, for example you can import another package if you are using it for your own package R folder: folder which contains all the code for the package man folder: contains documentation files to describe your functions test folder: contains testing functions for testing the packages  Documentation After installing the devtools package, the roxygen package can be used to automatically generate a documentation structure and populate the NAMESPACE file. In RStudio, you can go to Code -\u0026gt; Insert Roxygen skeleton when the cursor is inside a function to create the documentation skeleton for each function, and manually fill it in to describe the inputs, outputs, descriptions etc. of the function. Within this structure, fields are defined by the @ symbol, so for example @param will define the input parameter of the model. As well as this, you can use @import \u0026lt;package_name\u0026gt; to get Roxygen to add a particular package to the NAMESPACE file. For example, the roxygen structure for LS.model is:\n#\u0026#39; Least Squares Regression #\u0026#39; #\u0026#39; @param formula an object of class \u0026quot;formula\u0026quot; #\u0026#39; @param data data frame to which the formula relates #\u0026#39; @return list containing three elements: Parameters, df, y #\u0026#39; @import stats #\u0026#39; @examples #\u0026#39; df = data.frame(y = c(1,2,3,4), x = c(2,5,3,1)) #\u0026#39; LS.model(y~x, data=df) The documentation can be generated by running the command devtools::document() (or pressing Ctrl + Shift + D in RStudio).\n Testing In most cases, testing is done manually. After creating a function, you can put a certain amount of inputs in, and make sure that the outputs match up with what you were expecting. This can be automated with the testthat package. This allows testing to be consistent throughout code changes, so if you change some code, you can run the test again to see if the outputs match with what you were expecting, without having to manually test again. The command usethis::use_test(\u0026quot;\u0026lt;name\u0026gt;\u0026quot;) can be used to populate the tests directory, where the testing functions are stored.\nFor the LS.model function, some useful tests were to ensure that the output dimension \\(n\\) matched the input dimension. Using the test_that and expect_equal function achieved this functionality:\nlibrary(testthat) test_that(\u0026quot;output dimension (n)\u0026quot;, { df = data.frame(y=c(1,2,3,4),x=c(4,5,6,7)) m = LS.model(y~x,data=df) expect_equal(dim(as.matrix(df))[1], 4) }) Other tests were also implemented for checking this function as well as the other functions. You can run all the tests by running devtools:test() (or Ctrl + Shift + T in RStudio):\n\u0026gt; devtools::test() Loading simpleLS Testing simpleLS ✔ | OK F W S | Context ✔ | 4 | LS.model ✔ | 2 | LSr ══ Results ═══════════════════════════════════════════════════════════ Duration: 0.2 s OK: 6 Failed: 0 Warnings: 0 Skipped: 0 This shows the tests that were passed, and can show the tests that were unsuccessful. If tests do not pass, then details will be given why, so that you know where something has gone wrong.\n Coverage Another useful functionality is to test how much your tests actually test. The coverage of your tests (as a percentage) will tell you how much code is not being tested, so generally higher coverage is better. This can be implemented with the covr package. Running covr::report() will generate a report. For this package, this received\nsimpleLS coverage - 95.45%  Git Integration Git and Github allow easy access to version control, and online storage and supply of an R package. By initialising a repository for the package directory, and allowing access to it on Github, your code and package is freely available online. ‘Committing’ and then ‘pushing’ your changes and files to your repository will update your package to the latest version, and you are able to view older versions of code and previous changes you made in case something goes wrong. This is very useful in software development, for example if you want to revert to the last stable version.\nThe repository for this package can be found at:\nhttps://github.com/DanielWilliamsS/simpleLS  Travis CI integration A publicly available R package can be tested online using a tool known as Travis CI (CI - Continuous Integration). When a pull request is made, or new changes are pushed to the Github repository, Travis CI will automatically test the code using the testing functions described previously. This allows someone who downloads the package to be sure that the code works, and provides a way of automatically testing new versions of code. This is especially useful in collaborative coding projects.\nEnvironmental variables can be included in the Travis CI settings, which allows Travis to do other things. For example, one environmental variable will test the coverage of the code testing, as described previously. Another environmental variable can enable Travis to build RMarkdown pages and deploy them to a Github pages website, allowing you to publish your html RMarkdown document online.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"4ef526f61e546492869e30413840f261","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/github/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/github/","section":"portfolios","summary":"RStudio can be used to efficiently make a package in R, and allows an accessible way of implementing version control and git integration. As an example of these processes, I have created an R package which implements a basic form of least squares regression.","tags":null,"title":"Git and GitHub","type":"docs"},{"authors":null,"categories":null,"content":" Introduction ‘Normal’ programming can deal with performing operations one at a time, i.e. writing code in such a way that it can be seen as a set of instructions to be performed sequentially. A more efficient process will be maximising the use of the cores in your processor so that multiple operations are performed at once, with different processes happening on different cores.\nOpenMP is a way of performing parallel computation for C, C++ and Fortran, but this portfolio will go over the use of OpenMP in C++.\n Basics A simple C++ script which uses OpenMP for parallel computing will look like this\n#include \u0026lt;iostream\u0026gt; #ifdef _OPENMP #include \u0026lt;omp.h\u0026gt; #else #define omp_get_num_threads() 0 #define omp_get_thread_num() 0 #endif int main(int argc, const char **argv) { std::cout \u0026lt;\u0026lt; \u0026quot;Hello I am here safe and sound home in the main thread.\\n\u0026quot;; #pragma omp parallel { int nthreads = omp_get_num_threads(); int thread_id = omp_get_thread_num(); std::cout \u0026lt;\u0026lt; \u0026quot;Help I am trapped in thread number \u0026quot; \u0026lt;\u0026lt; thread_id \u0026lt;\u0026lt; \u0026quot; out of a total \u0026quot; \u0026lt;\u0026lt; nthreads \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; \u0026quot;Thank god I\u0026#39;m safe back home now.\\n\u0026quot;; return 0; } The important parts here are the ifdef _OPENMP section at the start, and the #pragma omp parallel line before the process of the script. The comments succeeding the hash symbol can be seen as a ‘hint’ to the compiler of what to do. The compiler is free to ignore this if need be.\nThe if_def _OPENMP line checks if we are using OpenMP or not, and if so, includes the omp.h header file. The pragma omp parallel line tells the compiler to run the section enclosed in braces {} a certain amount of times, depending on the input number of threads.\nWe can compile this code (which is saved in basic_openmp.cpp) to see what happens.\ng++ -fopenmp basic_openmp.cpp -o basic_openmp export OMP_NUM_THREADS=1 ./basic_openmp ## Hello I am here safe and sound home in the main thread. ## Help I am trapped in thread number 0 out of a total 1 ## Thank god I\u0026#39;m safe back home now. g++ -fopenmp basic_openmp.cpp -o basic_openmp export OMP_NUM_THREADS=8 ./basic_openmp ## Hello I am here safe and sound home in the main thread. ## Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number Help I am trapped in thread number 462 out of a total 8 out of a total out of a total 8 ## ## 8 ## 3 out of a total 8 ## Help I am trapped in thread number 0 out of a total 8 ## Help I am trapped in thread number 5 out of a total 8 ## 7 out of a total 8 ## Help I am trapped in thread number 1 out of a total 8 ## Thank god I\u0026#39;m safe back home now. This person really got trapped in a time loop. This is because the threads do not run sequentially, so each thread is printing out what it is asked as soon as it can, and these commands are being run at the same time.\n Sections and Loops OpenMP sections are ways of telling the compiler that each section can be operated on different threads. This is done by adding the line #pragma omp section and braces {} to each section that can be operated on individually, but only once. This is a way of breaking your code into parallel ‘chunks’ without executing the same code a lot of times by each thread.\nA loop can be run in parallel by writing the line #pragma omp for above the loop. This specifies that each iteration in the loop is independent and can be run separately. Then each thread runs on a different iteration of the loop at the same time. Below is an example of a loop running in parallel.\n#include \u0026lt;iostream\u0026gt; #ifdef _OPENMP #include \u0026lt;omp.h\u0026gt; #else #define omp_get_thread_num() 0 #endif int main(int argc, const char **argv) { #pragma omp parallel { int nloops = 0; #pragma omp for for (int i=0; i\u0026lt;1000; ++i) { ++nloops; } int thread_id = omp_get_thread_num(); #pragma omp critical { std::cout \u0026lt;\u0026lt; \u0026quot;Thread \u0026quot; \u0026lt;\u0026lt; thread_id \u0026lt;\u0026lt; \u0026quot; performed \u0026quot; \u0026lt;\u0026lt; nloops \u0026lt;\u0026lt; \u0026quot; iterations of the loop.\\n\u0026quot;; } } return 0; } This code will display how many iterations of the loop each thread is performing. Note that the #pragma omp critical line is specifying that only one thread can enter the code in braces {} at a time, just so that the printed output does not get jumbled up like it has done previously. In most code, this line will not be added as it will come with a significant slow down, I have only included it here for illustration purposes. We can compile and then run this code.\ng++ -fopenmp loop_openmp.cpp -o loop_openmp export OMP_NUM_THREADS=4 ./loop_openmp ## Thread 0 performed 250 iterations of the loop. ## Thread 1 performed 250 iterations of the loop. ## Thread 2 performed 250 iterations of the loop. ## Thread 3 performed 250 iterations of the loop. So these threads have split the job evenly, but in some cases (maybe for a large amount of threads) the job would not be split evenly.\n Monte Carlo Exercise For an exercise, consider using an OpenMP parallel program to calculate \\(\\pi\\) using a Monte Carlo algorithm. We can do this by simulating a unit circle and a unit square, since \\(\\pi\\) is the ratio of the area of a circle to the area of a square. By simulating random points for a circle and for a square, provided we have a large enough number of simulations we can estimate this proportion and hence \\(\\pi\\).\n#include \u0026lt;cmath\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;iostream\u0026gt; #ifdef _OPENMP #include \u0026lt;omp.h\u0026gt; #else #define omp_get_thread_num() 0 #endif double rand_one() { return std::rand() / (RAND_MAX + 1.0); } int main() { // declare variables int circle_points = 0; int square_points = 0; int circle_points_loop = 0; int square_points_loop = 0; // set up parallel OpenMP #pragma omp parallel { // run for loop in parallel #pragma omp for for(int ii=0; ii \u0026lt; 100000; ii++) { // get random x and y coordinates double x_coord = (2*rand_one() - 1); double y_coord = (2*rand_one() - 1); // calculate radius double r = std::sqrt(pow(x_coord,2) + pow(y_coord,2)); // if r is less than or equal to 1 then it is within the circle if(r \u0026lt; 1.0) { ++circle_points_loop; } else { ++square_points_loop; } } // use critical when counting the final number of counts for each thread #pragma omp critical { circle_points += circle_points_loop; square_points += square_points_loop; } } // calculate final value of pi using ratios double pi = (4.0*circle_points)/(square_points+circle_points); // print pi std::cout \u0026lt;\u0026lt; pi \u0026lt;\u0026lt; std::endl; return 0; } The comments in the code above will explain why each section of code is run at each point in time. To check this result is valid, we can compile and run it.\ng++ -fopenmp pi.cpp -o pi export OMP_NUM_THREADS=8 ./pi ## 3.13372 Which is not a bad approximation!\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"88e260ec672b7c2b8ea08c8cf208de40","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/openmp/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc2/openmp/","section":"portfolios","summary":"Introduction ‘Normal’ programming can deal with performing operations one at a time, i.e. writing code in such a way that it can be seen as a set of instructions to be performed sequentially.","tags":null,"title":"Intro to OpenMP in C++","type":"docs"},{"authors":null,"categories":null,"content":"       Introduction Debugging is an important part of any programming process. It is unlikely that an individual will write their code correctly on the first write up, and it is generally accepted that the code will only become usable after a few debugging iterations. Perfomance is the measure of how efficient your code is with respect to speed, so that you can do the same operation in as quick a time as possible.\nIn this section of the portfolio, the debugging process and the performance aspects will be explained by way of an example. This example will be a large function that is deliberately inefficiently and incorrectly written, and will be fixed and improved using different methods.\n Example: Least Squares with Feature Transform Take for example least squares regression with the choice of three basis functions; linear, quadratic, or trigonometric. This function can take one output vector \\(y\\), and one input vector \\(x\\), and estimate parameters \\(w_{LS}\\) from the solution \\[ \\boldsymbol{w}_{LS} = \\left(\\boldsymbol{\\phi}(\\boldsymbol{X})\\boldsymbol{\\phi}(\\boldsymbol{X})^T +\\lambda \\boldsymbol{I}\\right)^{-1}\\boldsymbol{\\phi}(\\boldsymbol{X})\\boldsymbol{y}^T, \\] where \\(\\boldsymbol{X}\\) is the model matrix, and \\(\\phi\\) is a feature transform function. The first version of this function looks like\nLS.feature.transform.fit \u0026lt;- function(y, x, ft, b){ if(ft == \u0026quot;Polynomial\u0026quot;) basis_function = function(x) poly(x, b, raw=TRUE) if(ft == \u0026quot;Linear\u0026quot;) basis_function = function(x) x if(ft == \u0026quot;Trigonometric\u0026quot;) { basis_function = function(x){ basis = c() for(i in 1:b){ basis = cbind(basis, sin(i*x), cos(i*x)) } return(basis) } } Phi = matrix(NA, length(x), length(basis_function(x))+1) for(i in 1:length(x)){ Phi[i,] = c(1, basis_function(x[i])) } wLS = solve(t(Phi) %*% Phi) %*% t(Phi) %*% y return(wLS) } This function takes the argument ft, meaning feature transform. The first thing that the function does is try to recognise which feature transform is being input, by a series of three if functions. Then basis_function is assigned to a function corresponding to the correct feature transform. After this, Phi is set up as a matrix with the correct dimensions, that is the length of the data and the dimension of the feature space (including a column of 1’s).\nLet’s test this function. Good practice is to create a series of testing functions or scripts that will test as many aspects of the function as possible. This code chunk below will test the function for each basis function.\nlibrary(lasso2) data(Prostate) LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,\u0026quot;Linear\u0026quot;,1) LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,\u0026quot;Polynomial\u0026quot;,3) LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,\u0026quot;Trigonometric\u0026quot;,1) Debugging Let’s try the function for a linear (identity) feature transform. When this line is run, the following error is returned.\nLS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,\u0026quot;Linear\u0026quot;,1) ## Error in t(Phi) %*% Phi : ## requires numeric/complex matrix/vector arguments This error message is not informative enough to go back and fix our function immediately. We can use the traceback function to get more information.\ntraceback() ## 2: solve(t(Phi) %*% Phi) at #20 ## 1: LS.feature.transform.fit(Prostate$lpsa, Prostate$lcavol, \u0026quot;Linear\u0026quot;, 1) This is not as useful here, because it does not say much more than the previous error message. We do now know that the error is on line #20, but the actual problem could be before that, presumably in the definition of Phi. We can also use another debugging function, called browser(), which allows you to open an interactive debugging environment. From here we can interactively view all elements and find where the problem is. Note that RStudio also allows an interactive debugging environment.\nLet’s take a look at the elements in the function after the definition of Phi. We can first look at the first few rows and columns of Phi. The following code was run after running all other parts of the function up to the definition of Phi:\nPhi[1:5,1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 -0.5798185 1 -0.5798185 1 ## [2,] 1 -0.9942523 1 -0.9942523 1 ## [3,] 1 -0.5108256 1 -0.5108256 1 ## [4,] 1 -1.2039728 1 -1.2039728 1 ## [5,] 1 0.7514161 1 0.7514161 1 The dimension of Phi is wrong, as the columns are being repeated! It should only have two columns, a column of 1’s regarding to the intercept, and a column of each \\(\\phi(x)\\). This error must come from where the dimension is defined, in the line Phi = matrix(NA, length(x), length(basis_function(x))+1). Looking at Phi in this case, we see we do not want to use the length of the basis function, but instead the number of columns that it contains. So instead we can change length(basis_function(x))+1 to dim(as.matrix(basis_function(x)))[2]+1. Now we run the test again.\nLS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol,\u0026quot;Linear\u0026quot;,1) ## [,1] ## [1,] 1.5072975 ## [2,] 0.7193204 Now the function works! We can see if the parameter estimates are correct by comparing to the output from a linear model with lm, since we have the known result that \\(w_{LS} = w_{MLE}\\).\nlm(lpsa ~ lcavol, data = Prostate)$coef ## (Intercept) lcavol ## 1.5072975 0.7193204 And ensuring that our function works for the other two feature transforms.\nLS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol, \u0026quot;Polynomial\u0026quot;, 3) ## [,1] ## [1,] 1.66387139 ## [2,] 0.69613468 ## [3,] -0.18630511 ## [4,] 0.06164228 lm(lpsa ~ poly(lcavol, 3, raw=TRUE), data = Prostate)$coef ## (Intercept) poly(lcavol, 3, raw = TRUE)1 ## 1.66387139 0.69613468 ## poly(lcavol, 3, raw = TRUE)2 poly(lcavol, 3, raw = TRUE)3 ## -0.18630511 0.06164228 LS.feature.transform.fit(Prostate$lpsa,Prostate$lcavol, \u0026quot;Trigonometric\u0026quot;, 2) ## [,1] ## [1,] 2.4198088 ## [2,] 0.3801217 ## [3,] -1.2342905 ## [4,] 0.4748111 ## [5,] 0.3759252 lm(lpsa ~ sin(lcavol) + cos(lcavol) + sin(2*lcavol) + cos(2*lcavol), data = Prostate)$coef ## (Intercept) sin(lcavol) cos(lcavol) sin(2 * lcavol) cos(2 * lcavol) ## 2.4198088 0.3801217 -1.2342905 0.4748111 0.3759252 There are no errors, and our parameter estimates match those from lm, so this debugging has been a success.\n Performance So far we have only tested this for a length \\(n=97\\) dataset, so performance has been relatively fast. We can use the function microbenchmark from the microbenchmark package to test how quickly our function runs, and gives a summary of statistics on how quickly the function runs. Before we do that, we want to have a larger data set to see more obvious difference in how our computation times are. We can do this with some simulated data.\nx.test = runif(10000, 1, 5) y.test = rexp(10000,rate=1.5*x.test-1) library(microbenchmark) # microbenchmark(LS.feature.transform.fit(y.test,x.test,\u0026quot;Polynomial\u0026quot;,5)) This range of speeds is not bad, but could be improved. To see where we can improve the performance of our code, we can do something called profiling. A statistical profiler can determine where the code is spending most of its time being run, by using operating system interrupts. An implementation of profiling in R is provided by the profvis package, and the profvis function. We start by running this on our function.\nlibrary(profvis) x = x.test; y = y.test; ft = \u0026quot;Polynomial\u0026quot;; b = 5 profvis({ if(ft == \u0026quot;Polynomial\u0026quot;) basis_function = function(x) poly(x, b, raw=TRUE) if(ft == \u0026quot;Linear\u0026quot;) basis_function = function(x) x if(ft == \u0026quot;Trigonometric\u0026quot;) { basis_function = function(x){ basis = c() for(i in 1:b){ basis = cbind(basis, sin(i*x), cos(i*x)) } return(basis) } } Phi = matrix(NA, length(x), dim(as.matrix(basis_function(x)))[2]+1) for(i in 1:length(x)){ Phi[i,] = c(1, basis_function(x[i])) } wLS = solve(t(Phi) %*% Phi) %*% t(Phi) %*% y })  {\"x\":{\"message\":{\"prof\":{\"time\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,4,4,4,4,5,5,5,6,6,7,7,7,8,8,8,9,9,9,9,10,10,10,10,11,11,11,11,12,12,12,13,13,13,14,14,14,15,15,15,16,16,16,16,17,17,17,18,18,19,19,19,20,20,20,21,21,21,21,22,22,22,23,23,24,24,24,25,25,25,25,26,26,26,26,27,27,27,27,28,28,29,29,29,30,30,30,31,31,31,32,32,32,33,33,33,34,34,34,34,35,35,35,36,36,36,37,37,38,38,38,39,39,40,40,40,41,41,42,42,42,43,43,43,44,44,45,45,45,46,46,46,47,47,47,47,47,47,48,48,48,49,49,49,50,51,51,51,52,52,52,52,53],\"depth\":[18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,3,2,1,4,3,2,1,4,3,2,1,4,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,2,1,3,2,1,2,1,3,2,1,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,6,5,4,3,2,1,3,2,1,3,2,1,1,3,2,1,4,3,2,1,1],\"label\":[\"delayedAssign\",\"findCenvVar\",\"getInlineInfo\",\"isBaseVar\",\"FUN\",\"lapply\",\"unlist\",\"Filter\",\"findLocalsList\",\"funEnv\",\"make.functionContext\",\"cmpfun\",\"doTryCatch\",\"tryCatchOne\",\"tryCatchList\",\"tryCatch\",\"compiler:::tryCmpfun\",\"basis_function\",\"get\",\"match.fun\",\"outer\",\"poly\",\"basis_function\",\"structure\",\"poly\",\"basis_function\",\"is.array\",\"outer\",\"poly\",\"basis_function\",\"structure\",\"poly\",\"basis_function\",\"poly\",\"basis_function\",\"colnames\",\"poly\",\"basis_function\",\"colnames\",\"\",null,null,null,\"\",\"\",null,\"\",\"\",null,null,\"\",\"\",null,\"\",\"\",\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,null,\"\",\"\",null,null,\"\",\"\",null,null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,null,\"\",\"\",null,\"\",\"\",\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,null,\"\",\"\",null,\"\",\"\",\"\",\"\",null,\"\",\"\",null,null,\"\",\"\",null,null,\"\",\"\",null,null,\"\",\"\",\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",\"\",\"\",null,\"\",\"\",\"\",\"\",null,\"\",\"\",\"\",\"\",null,\"\",\"\",null,\"\",\"\",\"\",\"\",null,\"\",\"\",null,\"\",\"\",null,null,null,null,\"\",\"\",null,\"\",\"\",null,\"\",\"\",\"\",\"\",\"\",\"\",null,null,\"\",\"\",\"\"]},\"interval\":10,\"files\":[{\"filename\":\"\",\"content\":\"library(profvis)\\nx = x.test; y = y.test; ft = \\\"Polynomial\\\"; b = 5\\nprofvis({\\n if(ft == \\\"Polynomial\\\") basis_function = function(x) poly(x, b, raw=TRUE)\\n if(ft == \\\"Linear\\\") basis_function = function(x) x \\n if(ft == \\\"Trigonometric\\\") {\\n basis_function = function(x){\\n basis = c()\\n for(i in 1:b){\\n basis = cbind(basis, sin(i*x), cos(i*x))\\n }\\n return(basis)\\n }\\n }\\n \\n Phi = matrix(NA, length(x), dim(as.matrix(basis_function(x)))[2]+1)\\n for(i in 1:length(x)){\\n Phi[i,] = c(1, basis_function(x[i]))\\n }\\n \\n wLS = solve(t(Phi) %*% Phi) %*% t(Phi) %*% y\\n})\",\"normpath\":\"\"}],\"prof_output\":\"/tmp/RtmpEOHPfa/file294e688a56c6.prof\",\"highlight\":{\"output\":[\"^output\\\\$\"],\"gc\":[\"^$\"],\"stacktrace\":[\"^\\\\.\\\\.stacktraceo(n|ff)\\\\.\\\\.$\"]},\"split\":\"h\"}},\"evals\":[],\"jsHooks\":[]} Looking at this graph, we can see that most of the time and memory is spent in the basis_function. This is likely due to the basis function being run at every iteration in the loop. One way of improving this would be to assign elements of Phi in terms of columns instead of rows. This is because R uses column-major storage, meaning that when a matrix is stored in memory, it is being assigned in chunks that correspond to columns, not rows. Therefore defining a matrix column-wise needs fewer operations than defining a matrix row-wise.\nThe performance can be greatly increased by vectorising so that the basis function need only be applied once instead of many times (as this is where the slowdown was). This eliminates the loop as well, another source of inefficiency. We make the following changes when defining the matrix \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\).\nPhi = as.matrix(cbind(1, basis_function(x))) Here we can see that the dimensions do not need to be set up in advance. Now if we benchmark and profile the function again, we get\n## Unit: milliseconds ## expr min lq ## LS.feature.transform.fit(y.test, x.test, \u0026quot;Polynomial\u0026quot;, 5) 7.071979 15.42255 ## mean median uq max neval ## 23.93168 20.15049 25.85308 171.5225 100  {\"x\":{\"message\":{\"prof\":{\"time\":[1,1,2,2,3,4,5],\"depth\":[2,1,2,1,1,1,1],\"label\":[\"solve.default\",\"solve\",\"solve.default\",\"solve\",\"%*%\",\"%*%\",\"%*%\"],\"filenum\":[null,1,null,1,1,1,1],\"linenum\":[null,32,null,32,32,32,32],\"memalloc\":[11.9669342041016,11.9669342041016,11.9669342041016,11.9669342041016,12.8827972412109,12.8827972412109,12.8827972412109],\"meminc\":[0,0,0,0,0.915863037109375,0,0],\"filename\":[null,\"\",null,\"\",\"\",\"\",\"\"]},\"interval\":10,\"files\":[{\"filename\":\"\",\"content\":\"LS.feature.transform.fit \"}],\"prof_output\":\"/tmp/RtmpEOHPfa/file294e22c0c55d.prof\",\"highlight\":{\"output\":[\"^output\\\\$\"],\"gc\":[\"^$\"],\"stacktrace\":[\"^\\\\.\\\\.stacktraceo(n|ff)\\\\.\\\\.$\"]},\"split\":\"h\"}},\"evals\":[],\"jsHooks\":[]} Here the benchmarks are significantly faster, and whilst the function seems to get stuck in the same place, the times that it gets stuck there is order of magnitudes smaller than it was previously.\n  Closing Thoughts In this portfolio section, we explained and showed an extended example of debugging, profiling and optimising performance. There are better ways of implementing all of these things than what was demonstrated here.\nFor debugging, we simply printed out the code where we thought the errors were, but a more rigorous debugging process would have involved an interactive debugger, which was discussed but not implemented. Using the debug function in R would allow RStudio to go through each line of the function and show results at each point. RStudio also allows breakpoints in functions, so that when the function is run, it will stop at a breakpoint instead of having to go through every line.\nFor optimising performance, R is generally inefficient for user written functions and scripts. Code written would be a lot more efficient and faster if it was written in a language such as C. Many core R routines and packages are written in C, greatly improving their efficiency. This can be achieved with the RCpp package, which provides an accessible way of writing efficient R code in C++.\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"ce544d155b7c54238b503bfa4d196410","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/performance/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/performance/","section":"portfolios","summary":"Introduction Debugging is an important part of any programming process. It is unlikely that an individual will write their code correctly on the first write up, and it is generally accepted that the code will only become usable after a few debugging iterations.","tags":null,"title":"Performance and Debugging","type":"docs"},{"authors":null,"categories":null,"content":" Tidyverse The tidyverse is a set of packages in R that share the same programming philosophy. These packages are\n readr tidyr dplyr ggplot2 magrittr  All these packages provide different functionalities. They can all be loaded at once by loading library(tidyverse). The combination of these packages provide an easier ‘front end’ to R. The tidyverse packages streamline the process of data manipulation compared to base R, as well as providing additional functions to simplify plotting and visualisation. This portfolio will go through an example demonstrating the usage of functions from these packages.\n Example: Energy Output from Buildings This dataset was obtained from the ASHRAE - Great Energy Predictor III. It is a large dataset that contains information about energy meter readings from 1448 buildings, which are classified by their primary use (e.g. education), their square footage, the year they were built and the number of floors they have. Meter readings are taken at all hours of the day, and these are available for a long time period for each building separately.\nThis dataset is very large, the train.csv training dataset file is around 386Mb, and so some processes can be very slow and cumbersome using base R functions. This is a good example of using functions from dplyr and magrittr to manipulate the dataset.\nJoining and Structuring Data We have two datasets to start with - train and metadata, which are the training set data and the building metadata respectively. We can inspect what kind of variables are in these datasets initially by using base R functions.\nhead(train) ## building_id meter timestamp meter_reading ## 1 0 0 2016-01-01 00:00:00 0 ## 2 1 0 2016-01-01 00:00:00 0 ## 3 2 0 2016-01-01 00:00:00 0 ## 4 3 0 2016-01-01 00:00:00 0 ## 5 4 0 2016-01-01 00:00:00 0 ## 6 5 0 2016-01-01 00:00:00 0 head(metadata) ## site_id building_id primary_use square_feet year_built floor_count ## 1 0 0 Education 7432 2008 NA ## 2 0 1 Education 2720 2004 NA ## 3 0 2 Education 5376 1991 NA ## 4 0 3 Education 23685 2002 NA ## 5 0 4 Education 116607 1975 NA ## 6 0 5 Education 8000 2000 NA The first goal is to combine these data sets together, and we can see that both data sets share the building_id column, so these data sets need to be joined together by this. In base R, some combination of the match function and subsetting would be required to do this, involving multiple lines of code and maybe some confusion. With the right_join (or left_join) function from dplyr, the process is much simpler.\nnew.train \u0026lt;- train %\u0026gt;% right_join(metadata, by = \u0026quot;building_id\u0026quot;) The training data set has been updated in one line with the function right_join, and it now includes the columns from both train and metadata.\ncolnames(train) ## [1] \u0026quot;building_id\u0026quot; \u0026quot;meter\u0026quot; \u0026quot;timestamp\u0026quot; \u0026quot;meter_reading\u0026quot; colnames(new.train) ## [1] \u0026quot;building_id\u0026quot; \u0026quot;meter\u0026quot; \u0026quot;timestamp\u0026quot; \u0026quot;meter_reading\u0026quot; ## [5] \u0026quot;site_id\u0026quot; \u0026quot;primary_use\u0026quot; \u0026quot;square_feet\u0026quot; \u0026quot;year_built\u0026quot; ## [9] \u0026quot;floor_count\u0026quot; When joining the data sets, the command %\u0026gt;% was used. This is referred to as a ‘pipe’, and is a part of the magrittr package. It iteratively performs operations such as the ones demonstrated above. In this case, it took the first input train, a dataset, then performed the function right_join, of which the first argument of the function was automatically defined as train. Pipes are very useful in organising and structuring code, and allow you to neatly run a lot of commands in one line.\nNow, imagine that we wanted to look at the average meter reading for each type of building, grouped by respective primary use, at each hour of the day. We first need to restructure our data slightly to achieve this. Since our timestamp column is a string containing the date and the time of the meter reading, we can subset this to just the first two characters of the time, which are in positions 12 and 13.\nnew.train$time = as.numeric(substr(new.train$timestamp, 12, 13)) This has created a new column in the new.train dataframe which corresponds to which hour the meter reading was taken. Now we can pipe the group_by and summarise functions through new.train to average over the different type of building and the time of day.\nmeanreadings = new.train %\u0026gt;% group_by(time, primary_use) %\u0026gt;% summarise(mean=mean(meter_reading)) meanreadings ## # A tibble: 384 x 3 ## # Groups: time [24] ## time primary_use mean ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 Education 4488. ## 2 0 Entertainment/public assembly 448. ## 3 0 Food sales and service 335. ## 4 0 Healthcare 757. ## 5 0 Lodging/residential 271. ## 6 0 Manufacturing/industrial 273. ## 7 0 Office 484. ## 8 0 Other 127. ## 9 0 Parking 183. ## 10 0 Public services 264. ## # … with 374 more rows Note that this has resulted in a tibble, which is a form of a data frame. The same goals can be achieved with a tibble that would otherwise be achieved with the normal dataframe type.\n Plotting and Visualising Data To visualise the average meter readings over the course of the day, we can use the ggplot function, from the ggplot2 package. This is a plotting package that provides easy access to different types of graphics. It also allows structure within plots, as you can save a plot object as a variable. Additions to the plot can done by simply adding other layers to the existing object. This can be seen in action here.\npl \u0026lt;- ggplot(meanreadings) + geom_line(mapping = aes(time, mean, col=primary_use), size=1.5) Initially, the ggplot function was called on the data frame meanreadings, initialising the plotting sequence, then the geom_line layer was added. The mapping argument defines what goes into the plot, so that time is on the \\(x\\)-axis, and mean is on the \\(y\\)-axis. The specification of col=primary_use in mapping separated the different categories and plotted their lines separately on the same plot. We can see the plot by inspecting the pl object.\npl This is an interesting plot, but doesn’t tell us much about the variation in meter readings on average during the day, for each building type. It does show that the ‘Education’ and ‘Services’ types of buildings on average require a lot more energy (or just have higher meter readings). To look at the variation between building types more closely, we can normalise each building type to be centred on zero by subtracting the mean across the average day and dividing by the standard deviation. This can be achieved by first creating a new data frame which includes both the mean and standard deviation.\nmeanreadings2 \u0026lt;- meanreadings %\u0026gt;% group_by(primary_use) %\u0026gt;% summarise(mean2 = mean(mean), sd = sd(mean)) %\u0026gt;% right_join(meanreadings, by = \u0026quot;primary_use\u0026quot;) meanreadings2$norm.mean \u0026lt;- (meanreadings2$mean - meanreadings2$mean2)/meanreadings2$sd ggplot(meanreadings2) + geom_line(aes(time, norm.mean, col=primary_use),size=1.5) On inspection of this plot, we can make some interpretations about the mean daily temperature based on the primary use of the building. Most of these buildings seem to follow a sine curve, where the meter readings increase at midday at a time of around 0600 to 1900. The meter readings also seem to be periodic, as at the end of the day they finish at around the value they started at. Most building types follow the same periodic structure, but we can see that the Manufacturing/industrial category has a higher peak in the morning, and the parking category has a lower peak in the evening.\nWe can also split this analysis by season, and inspect how the season affects the mean meter readings per day. This can be achieved by first creating a new ‘season’ column in the original data frame.\nnew.train$month = substr(new.train$timestamp, 6, 7) new.train$season = new.train$month new.train$season[new.train$season==\u0026quot;02\u0026quot; | new.train$season==\u0026quot;03\u0026quot; | new.train$season==\u0026quot;04\u0026quot;] = \u0026quot;Spring\u0026quot; new.train$season[new.train$season==\u0026quot;05\u0026quot; | new.train$season==\u0026quot;06\u0026quot; | new.train$season==\u0026quot;07\u0026quot;] = \u0026quot;Summer\u0026quot; new.train$season[new.train$season==\u0026quot;08\u0026quot; | new.train$season==\u0026quot;09\u0026quot; | new.train$season==\u0026quot;10\u0026quot;] = \u0026quot;Autumn\u0026quot; new.train$season[new.train$season==\u0026quot;01\u0026quot; | new.train$season==\u0026quot;11\u0026quot; | new.train$season==\u0026quot;12\u0026quot;] = \u0026quot;Winter\u0026quot; Then by using the group_by, summarise and right_join functions, we will make a data set that is grouped by season, time and primary use, to go into a plot.\nseasonal_readings \u0026lt;- new.train %\u0026gt;% group_by(season, time, primary_use) %\u0026gt;% summarise(mean_reading=mean(meter_reading)) seasonal_readings \u0026lt;- seasonal_readings %\u0026gt;% group_by(primary_use) %\u0026gt;% summarise(mean_mean_reading = mean(mean_reading), sd = sd(mean_reading)) %\u0026gt;% right_join(seasonal_readings, by = \u0026quot;primary_use\u0026quot;) seasonal_readings$norm.mean \u0026lt;- (seasonal_readings$mean_reading - seasonal_readings$mean_mean_reading)/seasonal_readings$sd ggplot(seasonal_readings, aes(time, norm.mean, group=1,col=primary_use)) + geom_line(aes(group=primary_use), size=1.2)+ facet_wrap(~season)  The first thing to note here is that the normalisation was done before the seasonal split, which is why some building types have higher normalised mean in some seasons. For example, we can see that the education category uses on average more energy in the summer than in other seasons. There is also a general shift upwards for meter readings in winter, and downwards for summer, which could be due to requiring more energy for central heating.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"4be0a3f623e1613a2d6238130ecd4e1e","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/tidyverse/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/tidyverse/","section":"portfolios","summary":"Tidyverse The tidyverse is a set of packages in R that share the same programming philosophy. These packages are\n readr tidyr dplyr ggplot2 magrittr  All these packages provide different functionalities.","tags":null,"title":"Tidyverse","type":"docs"},{"authors":null,"categories":null,"content":" Matrices A matrix is a two-dimensional data structure. The matrix function is used to create matrices, and can have multiple arguments. You can specify the names of the columns and rows by supplying a list to the dimnames argument, and can choose to populate the matrix by column (default) or by row with byrow=TRUE. The function as.matrix will convert a relevant argument to a matrix, and is.matrix results a TRUE or FALSE if the argument is or isn’t a matrix. We can see these here:\nA = matrix(1:12, nrow=3, ncol=4) A ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 A = matrix(1:12, nrow=3, ncol=4, byrow=TRUE) A ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 A = matrix(1:12, nrow=3, ncol=4, dimnames = list(c(\u0026quot;Row1\u0026quot;, \u0026quot;Row2\u0026quot;, \u0026quot;Row3\u0026quot;), c(\u0026quot;Column1\u0026quot;, \u0026quot;Column2\u0026quot;, \u0026quot;Column3\u0026quot;, \u0026quot;Column4\u0026quot;))) A ## Column1 Column2 Column3 Column4 ## Row1 1 4 7 10 ## Row2 2 5 8 11 ## Row3 3 6 9 12 Accessing elements in a matrix can be done by indexing over either the column or the row. A[,i] will access the \\(i\\)-th column, and A[i,] will access the \\(i\\)-th row. These arguments will return a vector, and will lose the structure of the matrix. For example, if we take the 1st column of A we get\nA[,1] ## Row1 Row2 Row3 ## 1 2 3 which is not a column any more! This is important to consider when working with matrices. To keep the structure of the matrix intact, we can specify drop=FALSE when indexing, e.g.\nA[,1,drop=FALSE] ## Column1 ## Row1 1 ## Row2 2 ## Row3 3 The array function in R works like a ‘stack of matrices’, and any number of dimensions can be specified. Instead of the matrix function, array takes one argument corresponding to the dimension, which is a vector; each element being the length of the corresponding dimension, i.e.\narray(1:27,c(3,3,3)) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 Matrix multiplication between more than 2 matrices can also be sped up by precisely choosing the location of your brackets. Since matrix multiplication works right-to-left, the brackets need to be on the right side. For a large matrix, if we test the speeds of two forms of multiplication, we get\nN = 1000 M1 = matrix(rnorm(N^2),N,N) M2 = matrix(rnorm(N^2),N,N) M3 = matrix(rnorm(N^2),N,N) system.time(M1 %*% M2 %*% M3) ## user system elapsed ## 0.462 0.172 0.194 system.time(M1 %*% (M2 %*% M3)) ## user system elapsed ## 0.431 0.159 0.195 all.equal(M1 %*% M2 %*% M3, M1 %*% (M2 %*% M3)) ## [1] TRUE So specification of brackets is quite a bit faster, and can speed up computation times for larger problems. Note that the function all.equal checks whether the two arguments are the same within some tolerance, as they are not exactly the same (see later section on Numerical types in R).\nSolving linear systems Often a linear algebra problem we are interested in is solving \\(A\\boldsymbol{x} =\\boldsymbol{b}\\) for \\(\\boldsymbol{x},\\boldsymbol{b} \\in \\mathbb{R}^n\\) and \\(A \\in \\mathbb{R}^{n\\times n}\\). One solution to this is simply \\(\\boldsymbol{x} = A^{-1}\\boldsymbol{b}\\), but the problem here is that getting the matrix inverse, as it will take of order \\(n^3\\) operations. For example, a 1000x1000 matrix (which is not uncommon) will take around 1000\\(^3\\) = 1,000,000,000 operations, which is inefficient. If you did want to solve the system this way, the function for inverting a matrix in R is solve, e.g.\nA = matrix(rnorm(9),3,3) solve(A) ## [,1] [,2] [,3] ## [1,] -8.888322 2.915525 -1.461660 ## [2,] 30.044457 -10.021586 3.193355 ## [3,] -11.184011 3.327956 -1.104080 This function can also take a second argument, being the right hand side of the system, which in our case is \\(b\\). This will roughly be the same as solve(A) %*% b.\nb = c(1,2,3) solve(A) %*% b ## [,1] ## [1,] -7.442254 ## [2,] 19.581350 ## [3,] -7.840339 solve(A, b) ## [1] -7.442254 19.581350 -7.840339 Although you can see the dimension of the output is different, solve(A) %*% b maintains the column structure. However, the method solve(A,b) is faster than solve(A) %*% b.\n Numerical types in R If we were to find the ‘type’ of a normal integer in R, we get\ntypeof(2) ## [1] \u0026quot;double\u0026quot; What does it mean by a ‘double’? This means that it is a binary64 floating point number, i.e. the information stored in the computer for this value is stored in 64 bits; 1 bit for the sign of the number, 11 bits for the exponent and 52 bits for the significant precision. So the largest number we can store is 2^1023, since\n2^1024 ## [1] Inf simply returns Inf. We know this number isn’t actually infinity, but R recognises that it is too large, and anything over the largest number is stored as the highest possible value. This also means that really small numbers aren’t stored correctly either. There is always some form of floating point error in R, of order 2\\(^{-52}\\). Showing this in practice:\nprint(1 + 2^(-52), digits=22) ## [1] 1.000000000000000222045 print(1 + 2^(-53), digits=22) ## [1] 1 Another format R can store numbers in is in the format ‘Long’, specified by a L after the number.\nEffect on Matrices Any form of arithmetic in R is going to be affected by floating point error. Most of the time it does not cause any problems though, as it will only affect things at really small or really large magnitudes. Matrices are specifically succeptible to floating point errors however, as matrix multiplication contains many operations.\nLet’s look at some simple matrix multiplication on large matrices and inspect whether there are floating point errors.\nN = 100 # Square Number A = matrix(rnorm(N),sqrt(N),sqrt(N)) B = matrix(rnorm(N),sqrt(N),sqrt(N)) C = matrix(rnorm(N),sqrt(N),sqrt(N)) test = c(solve(A %*% (B %*% C)) - solve(A %*% B %*% C)) Since these two operations are the same, all entries in the matrix test should be zero. However, this is not the case, as seen below.\nsummary(test) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.774e-11 -1.969e-12 2.685e-13 3.719e-14 2.575e-12 1.345e-11 This is due to floating point error. Not all entries in test are zero, but they are very small. Most of the time, this might not make much of a difference, but when performing calculations involving small numbers this is important to consider.\n   Sparse Matrices A sparse matrix is one where most of the entries are zero. The problem with sparse matrices in programming is that a very large matrix (for example a \\(10000 \\times 10000\\) matrix), the computer would store every element in the matrix, even though most are zero. There are various package for dealing with sparse matrices in a better way in R, but the most popular is the Matrix package. This package extends the base R functionality with both sparse and dense matrices, allolwing more operations and more efficient calculations. In this package, we can use the function rankMatrix to return the rank of a input matrix. For example:\nA = matrix(rnorm(25),5,5) c(rankMatrix(A)) ## [1] 5 A sparse matrix can be stored as a dgCMatrix (where the C stands for storing by column, other options are row or triplet). Let’s look at the difference between storing a sparse matrix in this way against the default way.\nA = matrix(0, 1000, 1000) for(i in 1:1000) A[sample(1:1000,50,1),i] = sample(1:10,50,replace=TRUE) B = Matrix(A, sparse=TRUE) A[1:4,1:15] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## [1,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 0 0 0 3 0 0 0 0 ## [3,] 0 0 0 7 0 0 0 0 0 0 6 0 0 0 ## [4,] 4 0 0 0 0 0 10 0 0 0 0 0 0 0 ## [,15] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 B[1:4,1:15] ## 4 x 15 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## ## [1,] . . . . . . . . . . . . . . . ## [2,] . . . . . . . . . 3 . . . . . ## [3,] . . . 7 . . . . . . 6 . . . . ## [4,] 4 . . . . . 10 . . . . . . . . c(object.size(A),object.size(B)) ## [1] 8000216 590616 So the sparse matrix is stored at a much smaller object size than a normal matrix. Note that the Matrix function is part of the Matrix package, not to be confused with matrix from base R. The conversion to being stored as a sparse dgCMatrix was done after construction of the matrix, but it could be constructed as a sparse matrix from the start. We can inspect the dgCMatrix object.\nstr(B) ## Formal class \u0026#39;dgCMatrix\u0026#39; [package \u0026quot;Matrix\u0026quot;] with 6 slots ## ..@ i : int [1:48759] 3 15 46 61 70 72 87 116 144 170 ... ## ..@ p : int [1:1001] 0 50 100 148 197 245 294 344 394 444 ... ## ..@ Dim : int [1:2] 1000 1000 ## ..@ Dimnames:List of 2 ## .. ..$ : NULL ## .. ..$ : NULL ## ..@ x : num [1:48759] 4 10 5 6 6 8 6 9 8 4 ... ## ..@ factors : list() This has a few pieces of information relating to the non-zero elements of the matrix B. The often most interesting ones being the i attribute: the locations of the non-zero entries, and the x attribute: the non-zero entries in these corresponding spots.\n Example Sparse matrices can have relevant application in many scenarios. For example, in a modelling problem where you want to model the effects of different categorical predictors, you can use ‘one-hot encoding’. This replaces a multi-class input \\(\\boldsymbol{x} \\in \\{0, 1, \\dots, K \\}^n\\) with a vector of 1’s and 0’s, where the location of the 1’s correspond to which class is being represented. For example, if we set up a vector of factors in R, we have\nx.factor = factor(sample(c(\u0026quot;Class1\u0026quot;, \u0026quot;Class2\u0026quot;, \u0026quot;Class3\u0026quot;, \u0026quot;Class4\u0026quot;), 20, replace=TRUE)) x.factor ## [1] Class1 Class4 Class3 Class1 Class2 Class1 Class3 Class3 Class2 Class2 ## [11] Class3 Class3 Class1 Class1 Class1 Class2 Class2 Class2 Class2 Class3 ## Levels: Class1 Class2 Class3 Class4 When fitting a model, we typically will add these entries to a model matrix, and work with that. The model.matrix function in R will automatically set up one-hot encoding in this scenario.\nM = model.matrix(~x.factor) M ## (Intercept) x.factorClass2 x.factorClass3 x.factorClass4 ## 1 1 0 0 0 ## 2 1 0 0 1 ## 3 1 0 1 0 ## 4 1 0 0 0 ## 5 1 1 0 0 ## 6 1 0 0 0 ## 7 1 0 1 0 ## 8 1 0 1 0 ## 9 1 1 0 0 ## 10 1 1 0 0 ## 11 1 0 1 0 ## 12 1 0 1 0 ## 13 1 0 0 0 ## 14 1 0 0 0 ## 15 1 0 0 0 ## 16 1 1 0 0 ## 17 1 1 0 0 ## 18 1 1 0 0 ## 19 1 1 0 0 ## 20 1 0 1 0 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 0 1 1 1 ## attr(,\u0026quot;contrasts\u0026quot;) ## attr(,\u0026quot;contrasts\u0026quot;)$x.factor ## [1] \u0026quot;contr.treatment\u0026quot; This matrix is primarily made up of 1’s and 0’s, and so would be better suited to being stored as a sparse matrix.\nMatrix(M, sparse=TRUE) ## 20 x 4 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; ## (Intercept) x.factorClass2 x.factorClass3 x.factorClass4 ## 1 1 . . . ## 2 1 . . 1 ## 3 1 . 1 . ## 4 1 . . . ## 5 1 1 . . ## 6 1 . . . ## 7 1 . 1 . ## 8 1 . 1 . ## 9 1 1 . . ## 10 1 1 . . ## 11 1 . 1 . ## 12 1 . 1 . ## 13 1 . . . ## 14 1 . . . ## 15 1 . . . ## 16 1 1 . . ## 17 1 1 . . ## 18 1 1 . . ## 19 1 1 . . ## 20 1 . 1 . In the case of a large data set, when using categorical variables, this will speed up computation time quite significantly.\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"8189b4c888a3af3375688336259b0cea","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/matrices/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/matrices/","section":"portfolios","summary":"Matrices A matrix is a two-dimensional data structure. The matrix function is used to create matrices, and can have multiple arguments. You can specify the names of the columns and rows by supplying a list to the dimnames argument, and can choose to populate the matrix by column (default) or by row with byrow=TRUE.","tags":null,"title":"(Sparse) Matrices","type":"docs"},{"authors":null,"categories":null,"content":" Introduction Object oriented programming (OOP) is a programming language model that defines objects; which are elements in R that contain attributes, or fields, which have some specification in the definition of the object itself. Objects are defined in advance, and are very useful in conceptualising coding goals, and allowing the end-user a better experience when using your functions and/or code.\nBase R has three different ways of defining objects, which are the three different models:\n S3 S4 Reference Class  All of which have their merits and disadvantages. S3 is the simplest model, and is useful for defining a basic object. S4 is more complex, as classes have to be defined explicitly, but adds more clarity and allows inclusion of integrity checks. Reference Class is more complex again, but further improves on teh structure of the class definition, through incorporation of a higher degree of encapsulation.\n Examples: Dealing Cards in Reference Class For this example, I will be using the Reference Class model. This example is concerned with being able to deal a card from a standard card deck. To start with, we make a class called Card which will contain two properties; the suit and the value. This is set up as follows.\nCard \u0026lt;- setRefClass(\u0026quot;Card\u0026quot;, fields = c( suit = \u0026quot;character\u0026quot;, value = \u0026quot;numeric\u0026quot;, pairs = \u0026quot;numeric\u0026quot; )) The setRefClass function is used to create this class, and it has the two attributes that are required of a standard card. We can set up a function to deal a random card from a deck by now specifying two more commands.\ndealHand = function(n){ y \u0026lt;- Card$new(n) return(y) } Card$methods( initialize = function(n){ suits \u0026lt;- c(\u0026quot;Diamonds\u0026quot;,\u0026quot;Hearts\u0026quot;,\u0026quot;Clubs\u0026quot;,\u0026quot;Spades\u0026quot;) s \u0026lt;- sample(0:51, n) .self$suit \u0026lt;- suits[(s %/% 13) + 1] .self$value \u0026lt;- (s %% 13)+1 } ) The function dealHand has its only input as n, which is the size of the hand. The assignment here is given by the initialize method in the $methods substructure of Card. By setting the method of initialize to randomly sample both value and suit, this will deal a random card every time that dealCard(n) is run. For example:\ndealHand(5) ## Reference class object of class \u0026quot;Card\u0026quot; ## Field \u0026quot;suit\u0026quot;: ## [1] \u0026quot;Diamonds\u0026quot; \u0026quot;Spades\u0026quot; \u0026quot;Clubs\u0026quot; \u0026quot;Hearts\u0026quot; \u0026quot;Spades\u0026quot; ## Field \u0026quot;value\u0026quot;: ## [1] 9 7 5 4 11 ## Field \u0026quot;pairs\u0026quot;: ## numeric(0) We can also add another method that will recognise if there are any pairs in the hand that has been dealt. This is done by adding an additional method to Card$methods:\nCard$methods( initialize = function(n){ suits \u0026lt;- c(\u0026quot;Diamonds\u0026quot;,\u0026quot;Hearts\u0026quot;,\u0026quot;Clubs\u0026quot;,\u0026quot;Spades\u0026quot;) s \u0026lt;- sample(0:51, n) .self$suit \u0026lt;- suits[(s %/% 13) + 1] .self$value \u0026lt;- (s %% 13) + 1 }, getPairs = function(){ .self$pairs \u0026lt;- as.numeric(names(table(.self$value))[table(.self$value)\u0026gt;=2]) } ) So that now, if we are dealt a hand , we can see how many pairs there are in the hand, and what the value of the pair is:\nset.seed(2) hand = dealHand(5) hand$getPairs() hand ## Reference class object of class \u0026quot;Card\u0026quot; ## Field \u0026quot;suit\u0026quot;: ## [1] \u0026quot;Hearts\u0026quot; \u0026quot;Hearts\u0026quot; \u0026quot;Diamonds\u0026quot; \u0026quot;Spades\u0026quot; \u0026quot;Clubs\u0026quot; ## Field \u0026quot;value\u0026quot;: ## [1] 8 2 6 11 6 ## Field \u0026quot;pairs\u0026quot;: ## [1] 6 Our hand here contains the 8 of Hearts, the 2 of Hearts, the 6 of Diamonds, the Jack (11) of Spades, and the 6 of Clubs. So we have two sixes, and one pair. The class hand now has a new entry, a field named pairs, which contains the number 6, showing 6 is our only pair.\n Functional Programming Functional programmings is (obviously) focused on using functions. We call functions ‘first class’, because they\n Can be embedded into lists/dataframes Can be an argument to another function Can be returned by other functions And more  You can consider functions as another type of variable, as you would store and use them in similar ways. For example, consider the list of functions\nmylist = list(add_function = function(x,y) x+y, subtract_function = function(x,y) x-y) mylist$add_function(1,2) ## [1] 3 mylist$subtract_function(2,1) ## [1] 1 You can see how this could be useful, in setting a list of different functions. Applications could include including a list of link functions in some form of regression, or basis functions. Functions can also return other functions, consider\nmake_link_function = function(which_f){ if(which_f == \u0026quot;exponential\u0026quot;) f = function(x) exp(x) if(which_f == \u0026quot;identity\u0026quot;) f = function(x) x return(f) } link_function = make_link_function(\u0026quot;exponential\u0026quot;) link_function(1:5) ## [1] 2.718282 7.389056 20.085537 54.598150 148.413159 This is a style of function output that could be used in making a general linear model, for example, to link the parameter to the predictor.\nIn regards to functional programming, there are some important definitions:\n Pure functions: A function which always gives the same output for the same inputs, and does not have any side effects. An impure function will be one that, for example, generates (pseudo) random numbers. Closures: A function that outputs another function, but contains a closed variable that is defined only within the main function itself and not in global variables. Lazy evaluation: The inputs to a function come from the global variables, instead of what was previously defined. For example the function function(exp) function(x) x^exp returns a function that will raise x to the power of exp. If you change the value of exp after defining the first function, it will change what power x is raised to later on, even though the function was defined before exp was changed.   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"801ceb142c92d88d9482dbdae0cf2b25","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/oop/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/oop/","section":"portfolios","summary":"Introduction Object oriented programming (OOP) is a programming language model that defines objects; which are elements in R that contain attributes, or fields, which have some specification in the definition of the object itself.","tags":null,"title":"Object Oriented and Functional Programming","type":"docs"},{"authors":null,"categories":null,"content":" Numerical Optimisation The general idea in optimisation is to find a minimum (or maximum) of some function. Generally, our problem has the form \\[ \\min_{\\boldsymbol{x}} f(\\boldsymbol{x}). \\] Sometimes our problem can be constrained, which would take the general form \\[ \\min_{\\boldsymbol{x}} f(\\boldsymbol{x}) \\]\\[ \\text{subject to } g_i(x) \\leq 0 \\] for \\(i=1,\\dots,m\\), \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\). These are important problems to solve, and it is often that there is no analytical solution to the problem, or the analytical solution is unavailable. This portfolio will explain the most popular numerical optimisation methods, and those readily available in R.\nOptimising a complicated function To demonstrate the different optimisation methods, the speeds and abilities of each, consider optimising the Rastrigin function. This is a non-convex function that takes the form \\[ f(\\boldsymbol{x}) = An + \\sum^n_{i=1} [x_i^2-A\\cos(2\\pi x_i)], \\]\nwhere \\(n\\) is the length of the vector \\(\\boldsymbol{x}\\). We can plot this function in 3D using the plotly package to inspect it.\nf = function(x) A*n + sum(x^2 - A*cos(2*pi*x)) A = 5 n = 2 x1 = seq(-10,10,length=100) x2 = seq(-10,10,length=100) xy = expand.grid(x1,x2) z = apply(xy,1,f) dim(z) = c(length(x1),length(x2)) z.plot = list(x=x1, y=x2, z=z) image(z.plot, xlab = \u0026quot;x\u0026quot;, ylab = \u0026quot;y\u0026quot;, main = \u0026quot;Rastrigin Function\u0026quot;) So we are interested in optimising the function f. We can see from inspection of the plot that there is a global minimum at \\(\\boldsymbol{x} = \\boldsymbol{0}\\), where \\(f(\\boldsymbol{0}) = 0\\), and likewise:\nf(c(0,0)) ## [1] 0 So we will be evaluating optimisation methods based on how close they get to this true solution. We continue this portfolio by explaining the different optimisation methods, and evaluating their performance in finding the global minimum of the Rastrigin function.\nWhen \\(n=2\\), the gradient and hessian for this function can be calculated analytically: \\[ \\nabla f(\\boldsymbol{x}) = \\begin{pmatrix} 2 x_1 + 2\\pi A \\sin(2\\pi x_1) \\\\ 2 x_2 + 2\\pi A \\sin(2\\pi x_2) \\end{pmatrix} \\] \\[ \\nabla^2 f(\\boldsymbol{x}) = \\begin{pmatrix} 2 + 4\\pi^2 A \\cos (2\\pi x_1) \u0026amp; 0 \\\\ 0 \u0026amp; 2 + 4\\pi^2 A \\cos (2\\pi x_2) \\end{pmatrix} \\] We can construct these functions in R.\ngrad_f = function(x) { c(2*x[1] + 2*pi*A*sin(2*pi*x[1]), 2*x[2] + 2*pi*A*sin(2*pi*x[2]) ) } hess_f = function(x){ H11 = 2 + 4*pi^2*A*sin(2*pi*x[1]) H22 = 2 + 4*pi^2*A*sin(2*pi*x[2]) return(matrix(c(H11,0,0,H22),2,2)) } These analytical forms of the gradient and hessian can be supplied to various optimisation algorithms to speed up convergence.\nOptimisation problems can be one or multi dimensional, where the dimension refers to the size of the parameter vector, in our case \\(n\\). Generally, one-dimensional problems are easier to solve, as there is only one parameter value to optimise over. In statistics, we are often interested in multi-dimensional optimisation. For example, in maximum likelihood estimation we are trying to find parameter values that maximise a likelihood function, for any number of parameters. For the Rastrigin function in our example, we have taken the dimension \\(n=2\\).\n  Optimisation Methods Gradient Descent Methods Iterative algorithms take the form \\[ \\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + t \\boldsymbol{d}_k, \\: \\: \\text{ for iterations } k=0,1,\\dots, \\] \\(\\boldsymbol{d}_k \\in \\R^n\\) is the descent direction, \\(t_k\\) is the stepsize. \\[ f\u0026#39;(\\boldsymbol{x}; \\boldsymbol{d})=\\nabla f(\\boldsymbol{x})^T \\boldsymbol{d} \u0026lt; 0. \\] So moving \\(\\boldsymbol{x}\\) in the descent direction for timestep \\(t\\) decreases the function, so we move towards a minimum. The is the negative gradient of \\(f\\), i.e. \\(\\boldsymbol{d}_k = -\\nabla f(\\boldsymbol{x}_k)\\), or normalised \\(\\boldsymbol{d}_k = {-\\nabla f(\\boldsymbol{x}_k)}/{\\norm{\\nabla f(\\boldsymbol{x})}}\\). We can construct a general gradient descent method in R and evaluate performance on optimising the Rastrigin function.\ngradient_method = function(f, x, gradient, eps=1e-4, t=0.1, maxiter=1000){ converged = TRUE iterations = 0 while((!all(abs(gradient(x)) \u0026lt; eps))){ if(iterations \u0026gt; maxiter){ cat(\u0026quot;Not converged, stopping after\u0026quot;, iterations, \u0026quot;iterations \\n\u0026quot;) converged = FALSE break } gradf = gradient(x) d = -gradf/abs(gradf) x = x - t*gradf iterations = iterations + 1 } if(converged) {cat(\u0026quot;Number of iterations:\u0026quot;, iterations, \u0026quot;\\n\u0026quot;) cat(\u0026quot;Converged!\u0026quot;)} return(list(f=f(x),x=x)) } This code essentially will continue running the while loop until the tolerance condition is satisfied, where the change in \\(\\boldsymbol{x}\\) from one iteration to another is negligible. Now we can see in which cases this will provide a solution to the problem of the Rastrigin function.\ngradient_method(f, x = c(1, 1), grad_f) ## Not converged, stopping after 1001 iterations ## $f ## [1] 20.44268 ## ## $x ## [1] -3.085353 -3.085353 gradient_method(f, x = c(.01, .01), grad_f) ## Not converged, stopping after 1001 iterations ## $f ## [1] 17.82949 ## ## $x ## [1] -2.962366 -2.962366 Even when the initial guess of \\(x\\) was very close to zero, the true solution, this function did not converge. This shows that under a complex and highly varying function such as the Rastrigin function, the gradient method has problems. This can be improved by including a backtracking line search to dynamically change the value of the stepsize \\(t\\) to \\(t_k\\) for each iteration \\(k\\). This method reduces the stepsize \\(t\\) for each iteration \\(k\\) via \\(t_k = \\beta t_k\\) for \\(\\beta \\in (0,1)\\) while \\[ f(\\boldsymbol{x}_k) - f(\\boldsymbol{x}_k + t_k \\boldsymbol{d}_k) \u0026lt; -\\alpha\\nabla f(\\boldsymbol{x}_k)^T \\boldsymbol{d}_k. \\] and for \\(\\alpha \\in (0,1)\\). We can add this to the gradient method function with the line while( (f(x) - f(x + t*d) ) \u0026lt; (-alpha*t * t(gradf)%*%d)) t = beta*t. Meaning we need to specify \\(\\alpha\\) and \\(\\beta\\). After this is added to the function, we have\ngradient_method(f, c(0.01,0.01), grad_f, maxiter = 10000) ## Number of iterations: 1255 ## Converged! ## $f ## [1] 5.002503e-09 ## ## $x ## [1] 5.008871e-06 5.008871e-06 Now we finally have convergence! However, this is for when the initial guess was very close to the actual solution, and so in more realistic cases where we don’t know this true solution, this method is likely inefficient and inaccurate. The Newton method is an advanced form of the basic gradient descent method.\n Newton Methods The Newton method seeks to solve the optimisation problem using evaluations of Hessians and a quadratic approximation of a function \\(f\\) around \\(\\boldsymbol{x}_k\\). This is under the assumption is that the Hessian \\(\\nabla^2 f(\\boldsymbol{x}_k)\\) is . The unique minimiser of the quadratic approximation is \\[ \\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k - (\\nabla^2 f(\\boldsymbol{x}_k))^{-1} \\nabla f(\\boldsymbol{x}_k), \\] which is known as . Here you can consider \\((\\nabla^2 f(\\boldsymbol{x}_k))^{-1} \\nabla f(\\boldsymbol{x}_k)\\) as the descent direction in a scaled gradient method. The nlm function from base R uses the Newton method. It is an expensive algorithm to run, because it involves inverting a matrix, the hessian matrix of \\(f\\). Newton methods work a lot better if you can supply an algebraic expression for the hessian matrix, so that you do not need to numerically calculate the gradient on each iteration. We can use nlm to test the Newton method on the Rastrigin function.\nf_fornlm = function(x){ out = f(x) attr(out, \u0026#39;gradient\u0026#39;) \u0026lt;- grad_f(x) attr(out, \u0026#39;hessian\u0026#39;) \u0026lt;- hess_f(x) return(out) } nlm(f, c(-4, 4), check.analyticals = TRUE) ## $minimum ## [1] 3.406342e-11 ## ## $estimate ## [1] -4.135221e-07 -4.131223e-07 ## ## $gradient ## [1] 1.724132e-05 1.732303e-05 ## ## $code ## [1] 2 ## ## $iterations ## [1] 3 So this converged to the true solution in a surprisingly small number of iterations. The likely reason for this is due to Newton’s method using a quadratic approximation, and the Rastrigin function taking a quadratic form.\n BFGS In complex cases, the hessian cannot be supplied analytically. Even if it can be supplied analytically, in high dimensions the hessian is a very large matrix, which makes it computationally expensive to invert for each iteration. The BFGS method approximates the hessian matrix, increasing computability and efficiency. The BFGS method is the most common quasi-Newton method, and it is one of the methods that can be suppled to the optim function. It approximates the hessian matrix with \\(B_k\\), and for iterations \\(k=0,1,\\dots\\), it has the following basic algorithm:\nInitialise \\(B_0 = I\\) and initial guess \\(x_0\\).\nObtain a direction \\(\\boldsymbol{d}_k\\) through the solution of \\(B_k \\boldsymbol{d}_k = - \\nabla f(\\boldsymbol{x}_k)\\) Obtain a stepsize \\(t_k\\) by line search \\(t_k = \\text{argmin} f(\\boldsymbol{x}_k + t\\boldsymbol{d}_k)\\) Set \\(s_k = t_k \\boldsymbol{d}_k\\) Update \\(\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\boldsymbol{s}_k\\) Set \\(\\boldsymbol{y}_k = \\nabla f(\\boldsymbol{x}_{k+1}) - \\nabla f(\\boldsymbol{x}_k)\\) Update the hessian approximation \\(B_{k+1} = B_k + \\frac{\\boldsymbol{y}_k\\boldsymbol{y}_k^T}{\\boldsymbol{y}_k^T \\boldsymbol{s}_k} - \\frac{B_k \\boldsymbol{s}_k \\boldsymbol{s}_k^T B_k}{\\boldsymbol{s}_k^T B_k \\boldsymbol{s}_k}\\)  BFGS is the fastest method that is guaranteed convergence, but has its downsides. BFGS stores the matrices \\(B_k\\) in memory, so if your dimension is high (i.e. a large amount of parameters), these matrices are going to be large and storing them is inefficient. Another version of BFGS is the low memory version of BFGS, named ‘L-BGFS’, which only stores some of the vectors that represent \\(B_k\\). This method is almost as fast. In general, you should use BFGS if you can, but if your dimension is too high, reduce down to L-BFGS.\nThis is a very good but complicated method. Luckily, the function optim from the stats package in R has the ability to optimise with the BFGS method. Testing this on the Rastrigin function gives\noptim(c(1,1), f, method=\u0026quot;BFGS\u0026quot;, gr = grad_f) ## $par ## [1] 0.9899629 0.9899629 ## ## $value ## [1] 1.979932 ## ## $counts ## function gradient ## 19 3 ## ## $convergence ## [1] 0 ## ## $message ## NULL optim(c(.1,.1), f, method=\u0026quot;BFGS\u0026quot;, gr = grad_f) ## $par ## [1] 4.61081e-10 4.61081e-10 ## ## $value ## [1] 0 ## ## $counts ## function gradient ## 31 5 ## ## $convergence ## [1] 0 ## ## $message ## NULL So the BFGS method actually didn’t find the true solution for an initial value of \\(\\boldsymbol{x} = (1,1)\\), but did for when the initial value was \\(\\boldsymbol{x} = (0.1,0.1)\\).\n  Non-Linear Least Squares Optimisation The motivating example we have used throughout this section was concerned with optimising a two-dimensional function, of which we were only interested in two variables that controlled the value of the function \\(f(\\boldsymbol{x})\\). In many cases, we have a dataset \\(D = \\{\\boldsymbol{y},\\boldsymbol{x}_i\\}\\), where we decomopose the ‘observations’ as \\(\\boldsymbol{y} = g(\\boldsymbol{x}) + \\epsilon\\), where \\(\\epsilon\\) is a random noise parameter. In this case we are interested in finding an approximation to the data generating function \\(g(\\boldsymbol{x})\\), which we call \\(f(\\boldsymbol{x},\\boldsymbol{\\beta})\\), and \\(\\boldsymbol{\\beta}\\) are some parameters of whose relationship with \\(\\boldsymbol{x}\\) we model to make this approximation, so we are interested in optimising over these parameters. The objective function we are minimising over is \\[ \\min_{\\boldsymbol{\\beta}} \\sum^n_{i=1} r_i^2 = \\min_{\\boldsymbol{\\beta}} \\sum^n_{i=1} (y_i - f(x_i,\\boldsymbol{\\beta}))^2, \\] i.e. the squared difference between the observed dataset and the approximation to the data generating function that defines that dataset. Here, \\(r_i = y_i - f(x_i,\\boldsymbol{\\beta})\\) is known as the residuals, and it is of the most interest in a least squares setting. Many optimisation methods are specifically designed to optimise the least squares problem, but all optimisation methods can be used (provided they find a minimum). Some of the most popular algorithms for least squares are the Gauss-Newton algorithm and the Levenberg-Marquardt algorithm. Both of these algorithms are extensions of Newton’s method for general optimisation. The general form of the Gauss-Newton method is \\[ \\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - (J_r^TJ_r)^{-1}J_r^Tr_i, \\] where \\(J_r\\) is the Jacobian matrix of the residue \\(r\\), defined as \\[ J_r = \\frac{\\partial r}{\\partial \\boldsymbol{\\beta}}. \\] So this is defined as the matrix of partial derivatives with respect to each coefficient \\(\\beta_i\\). The Levenberg-Marquardt algorithm extends this approach by including a diagonal matrix of small entries to the \\(J_r^TJ_r\\) term, to eliminate the possibility of this being a singular matrix. This has the update process of \\[ \\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - (J_r^TJ_r+\\lambda I)^{-1}J_r^Tr_i, \\] where \\(\\lambda\\) is some small value. In the simple case where \\(\\lambda = 0\\), this reduces to the Gauss-Newton algorithm. This is a highly efficient method, but in the case where our dataset is large, we may want to use stochastic gradient descent.\nStochastic Gradient Descent Stochastic Gradient Descent (SGD) is a stochastic approximation to the standard gradient descent method. Instead of calculating the gradient for an entire dataset (which can be extremely large) it calculates the gradient for a lower-dimensional subset of the dataset; picked randomly or deterministically. The form of this method is \\[ \\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k - t \\nabla f_i(\\boldsymbol{x}_k) \\] where \\(i\\) is an index that refers to cycling through all points \\(i \\in D\\), the points in the dataset. This can be in different sizes of groups, so depending on the problem, \\(i\\) can be large or small (relative to the size of the dataset). Stochastic gradient methods are useful in the setting where your dataset is very large, otherwise it could be unnecessary.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"8454234890613fc14537beac43b65b98","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/optimisation/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/optimisation/","section":"portfolios","summary":"Numerical Optimisation The general idea in optimisation is to find a minimum (or maximum) of some function. Generally, our problem has the form \\[ \\min_{\\boldsymbol{x}} f(\\boldsymbol{x}). \\] Sometimes our problem can be constrained, which would take the general form \\[ \\min_{\\boldsymbol{x}} f(\\boldsymbol{x}) \\]\\[ \\text{subject to } g_i(x) \\leq 0 \\] for \\(i=1,\\dots,m\\), \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\).","tags":null,"title":"Optimisation","type":"docs"},{"authors":null,"categories":null,"content":" Numerical Integration Calculating a definite integral of the form \\[ \\int^b_a f(x) dx \\] can be difficult when an analytical solution is not possible. We are primarily interested in integration in statistics because we want to be able to compute expectations, i.e. \\[ \\mathbb{E}(X) = \\int xf(x)dx. \\]\nIt is easier in one dimension, but as the number of dimensions increases then the methods required become more complex.\n One-dimensional Case In one dimension, we could use some method involving solving an ODE, of which many methods exist. Another way is quadrature; approximating the integral using multiple points across the curve and taking areas at each point.\nQuadrature One of the most basic methods to approximate an integral involves collecting a series of shapes or ‘bins’ underneath the a curve, of which the area is known for each bin, and approximating the integral as the sum of the area of these shapes. To do this, we need to estimate the curve for which we are integrating, so we can get points at which to estimate these bins. This can be estimated with polynomial methods. The Weierstrass Approximation Theorem states that there exists a polynomial which can be used to approximate a given continuous function, within a tolerance. More formally, for \\(f \\in C^0([a,b])\\), there exists a sequence of polynomials \\(p_n\\) that converges uniformly to \\(f\\) on the interval \\([a,b]\\), i.e.\n\\[ ||f-p_n||_{\\infty} = \\max_{x \\in [a,b]} |f(x)-p_n(x)| \\to 0. \\]\nThere are many ways to approximate this polynomial, and the obvious way of doing this is by uniformly sampling across the curve, and estimating the polynomial based on these points, but we will show that this is not accurate in most cases. For example, the function \\[ f(x) = \\frac{1}{50+25\\sin{[(5x)^3]}}, \\qquad x \\in [-1,1] \\] has a very complex integral to solve analytically. Wolfram Alpha gives this solution as \\[ \\int f(x) dx = -\\frac{2}{375}i \\sum_{\\omega:\\: \\omega^6 - 3\\omega^4- 16i\\omega^3 + 3\\omega^2 - 1=0}\\frac{2\\omega \\tan^{-1}\\left( \\frac{\\sin 5x}{\\cos 5x - \\omega}\\right) - i \\omega \\log(-2\\omega \\cos 5x + \\omega^2 + 1)}{\\omega^4 - 2\\omega^2- 8 i \\omega +1} , \\] which would be an extreme effort to solve without a computer. With quadrature methods, the integral in this one-dimensional case can be approximated with small error due to the Weierstrass Approximation theorem.\nWe first start by approximating the polynomial \\(p_n\\) with a basic method of uniformly sampling across the range of \\(x\\). We can use Lagrange polynomials to approximate the polynomial across these uniform points. A Lagrange polynomial takes the form \\[ p_{k-1}(x) := \\sum^k_{i=1} \\ell_i (x) f_i(x_i), \\: \\: \\: \\: \\text{ where } \\:\\:\\:\\: \\ell_i(x) = \\prod^k_{j=1, j \\neq i} \\frac{x-x_j}{x_i-x_j}, \\] where \\(\\ell_i\\) are the Lagrange basis polynomials. We start by setting up the function.\nx = seq(-1, 1, len=100) f = function(x) 1/(50+25*sin(5*x)^3) A Lagrange polynomial function can be set up in R. This function will return an approximating function, of which values of \\(x\\) can be supplied, just like the original function f is set up.\nget_lagrange_polynomial = function(f, x_points){ function(x){ basis_polynomials = array(1, c(length(x), length(x_points))) for(j in 1:length(x_points)){ for(m in 1:length(x_points)){ if(m==j) next basis_polynomials[,j] = basis_polynomials[,j] * ((x-x_points[m])/(x_points[j]-x_points[m])) } } p = 0 for(i in 1:length(x_points)){ p = p + basis_polynomials[,i]*f(x_points[i]) } return(p) } } x_points = seq(range(x)[1], range(x)[2], length=30) lagrange_polynomial = get_lagrange_polynomial(f, x_points) plot(x, f(x), type=\u0026quot;l\u0026quot;) points(x_points, f(x_points), col=\u0026quot;red\u0026quot;, pch=20) lines(x, lagrange_polynomial(x), col=\u0026quot;red\u0026quot;) We can see that the Lagrange polynomial approximation approximates the function reasonably well in the middle areas, but the approximation is completely off at the ends. This large deviation is known as Runge’s phenomenon, and this occurs when using polynomial interpolation and equally spaced interpolation points. This can be fixed by using Chebyshev points, which take the form \\[ \\cos \\left(\\frac{2i-1}{2k}\\pi\\right), \\] for \\(i=1,\\dots,k\\). This can be simply implented in R.\nchebyshev_points = function(k) cos(((2*seq(1,k,by=1)-1)*pi)/(2*k)) c_points = sort(chebyshev_points(30)) lagrange_polynomial = get_lagrange_polynomial(f, c_points) plot(x, f(x), type=\u0026quot;l\u0026quot;) points(c_points, f(c_points), col=\u0026quot;red\u0026quot;, pch=20) lines(x, lagrange_polynomial(x), col=\u0026quot;red\u0026quot;) The function is approximated a lot better without any significant deviations from the function. Now that we have a more accurate approximation, we can estimate the area underneath the curve by using a ‘histogram’ approximation to the area. A basic method will simply sum over all small areas of the function evaluation at each point, and the distance between midpoints, i.e. \\[ \\sum^k_{i=1}w_k f(x_k), \\] where \\(x_k\\) is the \\(k\\)-th point (calculated with Chebyshev or uniform approximations), and \\(w_k\\) is the distance between the midpoints above and below point \\(x_k\\).\nhist_approximation = function(f_points, x_points, xrange=c(-1,1)){ midpoints = (x_points[2:(length(x_points))]+x_points[1:(length(x_points)-1)])/2 midpoints = c(xrange[1] ,midpoints, xrange[2]) diffs = diff(midpoints) sum(diffs*f_points) } x_points = seq(range(x)[1], range(x)[2], length=30) lagrange_polynomial = get_lagrange_polynomial(f, x_points) hist_approximation(lagrange_polynomial(x_points), x_points) ## [1] 0.04426415 Which we can compare to the analytical solution to the integral. \\[ \\int^{1}_{-1} \\frac{1}{50+25\\sin{x}}dx \\approx 0.0443078 \\] So this isn’t too far off, but can be more approximately calculated with Simpson’s rule, given by \\[ \\int^b_a f(x) dx \\approx \\frac{b-a}{6} \\left( f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b) \\right). \\] This can be coded into R.\nsimpsons_approximation = function(f, x_points, xrange=c(-1,1)){ midpoints = c(xrange[1], x_points, xrange[2]) diffs = (midpoints[2:(length(midpoints))] - midpoints[1:(length(midpoints)-1)])/6 * ( f(midpoints[1:(length(midpoints)-1)]) + 4*f((midpoints[2:(length(midpoints))] + midpoints[1:(length(midpoints)-1)])/2) + f(midpoints[2:(length(midpoints))]) ) sum(diffs) } c_points = sort(chebyshev_points(30)) simpsons_approximation(get_lagrange_polynomial(f, c_points), c_points) ## [1] 0.044304 This is a closer estimate to the true value. Increasing the number of points increases the accuracy of the integral approximation.\nn=1000 ints = rep(NA, (n-10)) for(b in 10:n) { c_points = sort(chebyshev_points(b)) ints[b-10] = simpsons_approximation(f(c_points), c_points) } plot(11:n, ints, xlab=\u0026quot;No. of Points\u0026quot;, log=\u0026quot;x\u0026quot;, type=\u0026quot;l\u0026quot;, ylab=\u0026quot;Approximated Area\u0026quot;) abline(h=0.0443078, col=\u0026quot;red\u0026quot;) legend(\u0026quot;topright\u0026quot;, col=\u0026quot;red\u0026quot;, legend=\u0026quot;True Integral\u0026quot;, lwd=2, lty=1) This plot shows that at around 35 points, the accuracy of the integral doesn’t increase significantly.\n  Multi-dimensional Case Quadrature methods don’t work as well with more than one dimension. Since quadrature is based around using polynomial approximation to the real curve and calculating the area under there, this is less simple in higher dimensions, and comes with an extremely large computational complexity. Monte-Carlo algorithms provide more efficient convergence to the integral area in this case. This will not be covered in this portfolio.x\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"bb55ae2e9d73529156e5a4eda7e4c17f","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc1/integration/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc1/integration/","section":"portfolios","summary":"Numerical Integration Calculating a definite integral of the form \\[ \\int^b_a f(x) dx \\] can be difficult when an analytical solution is not possible. We are primarily interested in integration in statistics because we want to be able to compute expectations, i.","tags":null,"title":"Numerical Integration","type":"docs"},{"authors":null,"categories":null,"content":" There are two primary methods for parallelisation in Rcpp, the first being OpenMP and the second being the RcppParallel package for Rcpp. The RcppParallel package builds on existing methods and uses OpenMP, but provides neat functionality and simple usage, and so will be focused on in this portfolio.\nThe reader must have a working knowledge of C++ and Rcpp. To begin, each Rcpp file must include\n// [[Rcpp::depends(RcppParallel)]] #include \u0026lt;RcppParallel.h\u0026gt; in the header. This ensures that the settings for RcppParallel are automatically included in the compilation of the C++ code.\nRegular parallel approaches to Cpp can cause crashes due to the single-threaded nature of R. This is due to multiple threads attempting to access and interact with the same data structure. RcppParallel provides a straightforward way to account for this.\nBasic Parallel Operations There are two functions inbuilt which can provide the bulk of the parallel operations; parallelFor and parallelReduce. These are interfaces to a parallel for loop and reduce function (see ?Reduce in R).\nRcppParallel also provides two accessor classes, RVector and RMatrix, which are thread-safe accessors for an Rcpp vector and matrix, helping to deal with the problem of accessing the same data structure across multiple threads.\n Example: Matrix Transformations Consider taking the log of every element in a large matrix. In R, this process is simple, since log is a vectorised function, we can just run\nlog(A) where A is a matrix. This would also be easy to implement in Rcpp using the std::transform operator:\n#include \u0026lt;Rcpp.h\u0026gt; using namespace Rcpp; // [[Rcpp:export]] NumericMatrix MatrixLog(NumericMatrix A) { int n = A.nrow(); int d = A.ncol(); NumericMatrix out(n, d); std::transform(A.begin(), A.end(), out.begin(), ::log); return(out); } The std::transform function applies a given function across a range of values, here these are specified as all the elements in the matrix A, where the starting and ending point are supplied by A.begin() and A.end(). These transformed values are saved in out, starting at out.begin(). We can compare the speed of this function to an R implementation by applying both the base R log function and MatrixLog to a large matrix (and check that they give the same result.\nlibrary(Rcpp) sourceCpp(\u0026quot;MatrixLog.cpp\u0026quot;) d = 200 A = matrix(1:(d^2), d, d) all.equal(log(A), MatrixLog(A)) ## [1] TRUE Okay, they are equal, this is a good first step.\nlibrary(microbenchmark) microbenchmark(log(A), MatrixLog(A), times = 1000, unit=\u0026quot;relative\u0026quot;) ## Unit: relative ## expr min lq mean median uq max neval ## log(A) 1.008818 1.042968 0.980134 1.057816 0.979937 0.398702 1000 ## MatrixLog(A) 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1000 So on average, the C++ implementation is actually around the same speed as the base R implementation. We can speed up the Rcpp code through the use of parallel computing, using parallelFor.\nFirstly, parallelFor has four arguments:\n begin: the beginning of the for loop end: the end of the for loop worker: an object of type Worker, where the operations are specified grainSize: minimal chunk size for parallelisation, minimum number of operations for each thread  Before defining the parallel code, parallelFor needs a Worker object to specify what processes to perform within the for loop. For this case, we need to create a worker that takes the log of each set of elements that are passed to each thread.\nstruct Log : public RcppParallel::Worker { const RcppParallel::RMatrix\u0026lt;double\u0026gt; input; RcppParallel::RMatrix\u0026lt;double\u0026gt; output; Log(const NumericMatrix input, NumericMatrix output) : input(input), output(output) {} void operator()(std::size_t begin, std::size_t end) { std::transform(input.begin() + begin, input.begin() + end, output.begin() + begin, ::log); } }; Let’s break down this structure. Firstly, two objects of type RMatrix are specified, for the input and output (recall that an RMatrix is a thread-safe object given by RcppParallel). Since different chunks of the matrix will be passed between threads, they need to be converted to this safe RMatrix object before they are interacted with. Secondly, the Log function is defined, so that these inputs and outputs are passed through.\nFinally, the operator() is the main part, which is what will natively be called by parallelFor. This performs the same operation as what we saw before in MatrixLog, with a few key differences. Namely the begin and end function inputs, which change the range that std::transform is applied to based on the chunk of the matrix that parallelFor will be giving this worker.\nNow that this is set up, we can rewrite MatrixLog in parallel:\n#include \u0026lt;Rcpp.h\u0026gt; #include \u0026lt;RcppParallel.h\u0026gt; using namespace Rcpp; // [[Rcpp::depends(RcppParallel)]] // [[Rcpp::export]] NumericMatrix MatrixLogPar(NumericMatrix A) { int n = A.nrow(); int d = A.ncol(); NumericMatrix output(n, d); Log log_(A, output); parallelFor(0, A.length(), log_); return output; } This function is similar to the original MatrixLog, however the std::transform section has been replaced by the definition of the worker log_ (with class Log), and then the call to parallelFor. To reiterate, a worker is defined which has the pre-built operator that it will take the log of each element within the chunk that is specified to it. This worker is then spread out across multiple threads by the call to parallelFor, and the output is saved in output.\nNow we can compare the speed!\nsourceCpp(\u0026quot;MatrixLogPar.cpp\u0026quot;) all.equal(log(A), MatrixLog(A), MatrixLogPar(A)) ## [1] TRUE microbenchmark(log(A), MatrixLog(A), MatrixLogPar(A), unit=\u0026quot;relative\u0026quot;, times=1000) ## Unit: relative ## expr min lq mean median uq max neval ## log(A) 2.339494 2.318958 1.749330 2.048938 1.820713 0.7575495 1000 ## MatrixLog(A) 2.232776 2.197518 1.731507 1.977766 1.801228 0.8874397 1000 ## MatrixLogPar(A) 1.000000 1.000000 1.000000 1.000000 1.000000 1.0000000 1000 So originally we had roughly the same processing time as the base R implementation. Now this is roughly twice as fast as base R!\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6b7ed405a4f81ad1167ff19ca1a27610","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/parallelrcpp/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc2/parallelrcpp/","section":"portfolios","summary":"There are two primary methods for parallelisation in Rcpp, the first being OpenMP and the second being the RcppParallel package for Rcpp. The RcppParallel package builds on existing methods and uses OpenMP, but provides neat functionality and simple usage, and so will be focused on in this portfolio.","tags":null,"title":"ParallelRcpp","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":2537701200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2537701200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://dannyjameswilliams.co.uk/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"  von-Mises Fisher distribution We simulate \\(n=1000\\) samples from a von-Mises distribution with mean direction \\((\\pi/2, \\pi)\\) on the sphere, using the rvmf function from the Rfast package.\n# Simulated VMF n = 1000 centre = c(pi/2, pi) centre_euclid = sphere_to_euclid(centre) set.seed(4) z = rvmf(n, centre_euclid, k = 6) x = euclid_to_sphere(z) Now this can be estimated directly, using various methods such as MLE, but if we could only see a portion of this data, then truncated score matching can be used. Say there is a boundary set up such that all measurements below a constant line of \\(\\phi = \\pi/2 + 0.2\\) were missing. This can be constructed as follows\ndV = cbind(rep(pi/2+0.2, 200), seq(0, 2*pi, len=200)) outside_boundary = x[,1] \u0026gt; (pi/2+0.2) truncated_x = x[outside_boundary,] The mean direction can be estimated by calling sphere_sm with a specified boundary function, one of \"Haversine\" or \"Euclidean\".\nest_hav = sphere_sm(truncated_x, dV, g = \u0026quot;Haversine\u0026quot;, family = vmf(k=6)) est_euc = sphere_sm(truncated_x, dV, g = \u0026quot;Euclidean\u0026quot;, family = vmf(k=6)) Note that the family argument is vmf(k=6). This is a wrapper function that specifies which distribution to use. The argument k=6 corresponds to the concentration parameter being fixed a t 6, if this weren’t supplied and vmf() was used alone, the concentration parameter will be esitmated. By default the sphere_sm function uses the von Mises Fisher distribution without specifying a value for k. Now that the points are estimated, we can plot them using ggplot.\nplot_data = data.frame(colat = c(x[!as.logical(outside_boundary),1], x[as.logical(outside_boundary),1]), lon = c(x[!as.logical(outside_boundary),2], x[as.logical(outside_boundary),2]), Data = c(rep(\u0026quot;Not-Observed\u0026quot;, sum(!as.logical(outside_boundary))), rep(\u0026quot;Observed\u0026quot;, sum(outside_boundary)))) centre_data = data.frame( colat = c(est_hav$mu[1], est_euc$mu[1], centre[1]), lon = c(est_hav$mu[2], est_euc$mu[2], centre[2]), Centres = c(\u0026quot;Haversine\u0026quot;, \u0026quot;Projected Euclidean\u0026quot;, \u0026quot;Real Centre\u0026quot;) ) ggplot(plot_data) + geom_point(aes(x=lon, y=colat, col=Data), alpha=.7, size=2) + scale_color_brewer(type=\u0026quot;qual\u0026quot;, palette=3) + theme_minimal() + xlab(expression(theta)) + ylab(expression(phi)) + geom_point(data=centre_data, aes(lon, colat, fill=Centres), size=4, shape = \u0026quot;diamond filled\u0026quot;) The point has been estimated reasonably well. Now lets try the same approach without fixing the concentration parameter.\nest_hav = sphere_sm(truncated_x, dV, g = \u0026quot;Haversine\u0026quot;, family = vmf()) est_euc = sphere_sm(truncated_x, dV, g = \u0026quot;Euclidean\u0026quot;, family = vmf()) Again, the centre has been found with good accuracy. We can inspect the output of sphere_sm to see what value was estimated for k.\nest_hav #\u0026gt; $mu #\u0026gt; colat lon #\u0026gt; 1.539329 3.112377 #\u0026gt; #\u0026gt; $k #\u0026gt; k #\u0026gt; 5.415736 #\u0026gt; #\u0026gt; $family #\u0026gt; [1] \u0026quot;von Mises Fisher\u0026quot; #\u0026gt; #\u0026gt; $val #\u0026gt; [1] -2.044959 est_euc #\u0026gt; $mu #\u0026gt; colat lon #\u0026gt; 1.584297 3.111693 #\u0026gt; #\u0026gt; $k #\u0026gt; k #\u0026gt; 5.481328 #\u0026gt; #\u0026gt; $family #\u0026gt; [1] \u0026quot;von Mises Fisher\u0026quot; #\u0026gt; #\u0026gt; $val #\u0026gt; [1] -1.713839 A different Boundary We can experiment with a different boundary, implementing the same approach to the same dataset but a different truncation region. Start by simulating different data from the same distribution and constructing the boundary as before:\nset.seed(7) z = rvmf(n, centre_euclid, k = 6) x = euclid_to_sphere(z) dV = cbind(c(rep(1.2, 200), seq(1.2, 3, len=200)), c(seq(0, 3.6, len=200), rep(3.6, len=200))) outside_boundary = x[,2] \u0026lt; 3.6 \u0026amp; x[,1] \u0026gt; 1.2 truncated_x = x[as.logical(outside_boundary),] So in this case, the boundary is anything that fits \\(\\phi \u0026gt; 1.2\\) and \\(\\theta \u0026lt; 3.6\\), roughly a box around the top left portion of the data. We call sphere_sm as before.\nest_hav = sphere_sm(truncated_x, dV, g = \u0026quot;Haversine\u0026quot;, family=vmf()) est_euc = sphere_sm(truncated_x, dV, g = \u0026quot;Euclidean\u0026quot;, family=vmf()) The results are slightly less accurate than before, but still hold.\n  Kent Distribution We adopt a similar approach for the Kent distribution, simulating data, truncating it at an artificial boundary and experimenting with fitting the model to see how well the estimated mean direction \\(\\mu\\) is. Firstly, we use a boundary similar to before, truncated at \\(\\phi = \\pi/2+0.1\\).\nz = rkent(n, k = 10, m = centre_euclid, b=3) x = euclid_to_sphere(z) dV = cbind(rep(pi/2+0.1, 500), seq(0, 2*pi, len=500)) outside_boundary = x[,1] \u0026gt; (pi/2 + 0.1) truncated_x = x[outside_boundary,] Since the Kent distribution has five parameters, estimation can be difficult and slow in some cases. For the Kent distribution, we assume that the concentration parameter k and ovalness parameter b are known, and specified in advance.\nest_hav = sphere_sm(truncated_x, dV, g=\u0026quot;Haversine\u0026quot;, family=kent(k=10, b=3)) est_euc = sphere_sm(truncated_x, dV, g=\u0026quot;Euclidean\u0026quot;, family=kent(k=10, b=3)) And another boundary, similar to before.\nset.seed(12) z = rkent(n, m=centre_euclid, k = 10, b=3) dV = cbind(c(rep(1.2, 200), seq(1.2, 3, len=200)), c(seq(0, 3.6, len=200), rep(3.6, len=200))) x = euclid_to_sphere(z) outside_boundary = x[,2] \u0026lt; 3.6 \u0026amp; x[,1] \u0026gt; 1.2 truncated_x = x[as.logical(outside_boundary),] est_hav = sphere_sm(truncated_x, dV, g = \u0026quot;Haversine\u0026quot;, family=kent(k=10, b=3)) est_euc = sphere_sm(truncated_x, dV, g = \u0026quot;Euclidean\u0026quot;, family=kent(k=10, b=3))  ","date":1654732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642682471,"objectID":"a7573ccfb7cfb36228fcfd26cb0ce08e","permalink":"https://dannyjameswilliams.co.uk/tutorials/tutorialmanifoldtsm1/","publishdate":"2022-06-09T00:00:00Z","relpermalink":"/tutorials/tutorialmanifoldtsm1/","section":"tutorials","summary":"von-Mises Fisher distribution We simulate \\(n=1000\\) samples from a von-Mises distribution with mean direction \\((\\pi/2, \\pi)\\) on the sphere, using the rvmf function from the Rfast package.\n# Simulated VMF n = 1000 centre = c(pi/2, pi) centre_euclid = sphere_to_euclid(centre) set.","tags":["Score Matching","Research","PhD","Estimation","Data Science"],"title":"(Tutorial) Basic Usage of the Manifold TSM R Package","type":"tutorials"},{"authors":[],"categories":[],"content":"  For those of you familiar with either HBO’s Game of Thrones, or George RR Martin’s A Song of Ice and Fire (ASOIAF), you will probably be aware of the vast differences between each character and the depth of each storyline. For those of you who aren’t familiar; each book chapter is written from the perspective of a different character, and George RR Martin writes each character in a distinct and unique way. A chapter written from the perspective of Eddard Stark, an aged and grizzled war veteran, will be portrayed vastly different to a chapter from the perspective of his son, Bran Stark, a 9 year old innocent ‘sweet summer child’.\nThe stories surrounding all of the characters span continents, explore diverse themes and face different problems, and so we would expect there to be intrinsic differences between the language used for all of these different perspectives. Can we use Natural Language Processing (NLP) to show how George RR Martin writes each of these characters? More specifically, is there actually a mathematical difference between the chapters, grouped by character, based on the language alone?\nA note on spoilers: I am not including any excerpts from either the books or the show, and do not discuss the story. However, you will be able to see how many chapters are included for each character, as well as if any characters stop having point-of-view chapters after a specific book.\nHow are we going to use NLP for Game of Thrones? To show how characters are different amongst the ASOIAF books, I will use BERT, which is a NLP model built by Google, and an extremely powerful tool which can output sentence embeddings. Put simply, a sentence embedding is a sequence of words, e.g. a sentence, which has been converted to a very high-dimensional mathematical vector. The vector itself won’t have any meaning to you or I on its own, but it has context relative to other sentences. For example, if two different sentence embeddings are very far away from each other, we can probably infer that the two original sentences are quite different.\nBERT can achieve this as it has been trained on a large corpus of language data: a dataset of books and the full English Wikipedia. One way in which BERT trains is by masked language modelling, which means it tries to predict random words in the dataset using the ‘transformer’ architecture, which is an extremely clever way of incorporating left-to-right and right-to-left directionality in a sentence so that the full context of the word is integrated. By learning where words come in the context of a sentence, BERT learns a lot about the English language. Whilst we aren’t trying to predict any words for our task, we will use the information that BERT learned whilst training as our emebeddings.\nFortunately for us, BERT is made completely open-source and free to use via Huggingface, a collection of NLP models, datasets, and more. We can extract the embeddings from BERT and visualise them for each chapter to see if there is a strong degree of separation, which would indicate key differences in the language for different characters.\n ASOIAF Embeddings We can give BERT a sentence from ASOIAF, and it will output a relevant vector which gives information about this sentence. Repeating this for every sentence in the ASOIAF book series, we can obtain the high-dimensional sentence embeddings from each chapter of ASOIAF, by grouping by chapter and taking the average embedding across sentences.\nThe embedding dimension output given by the base BERT model is 768, which we definitely cannot visualise. For this reason I have used UMAP1, a low-dimension projection method, to reduce the dimension to 3 so that we can visualise it.\nSeparation by Character Below I have plotted the (now 3D) embeddings in an interactive scatter plot, which you can rotate and move around. You can hover over each point to see the point-of-view character for each chapter and the corresponding book, as well as click on character names on the legend to remove them.\n   Mean BERT sentence embeddings of chapters in ASOIAF, split by character (with each book labelled), restricted to the 10 most frequently occuring point-of-view characters. The dimension was reduced from 768 to 3 using UMAP. See in fullscreen here.    It is important to note that no information about the classes (chapters/characters) was used at any point, so any structure we can see that separates the different characters or chapters is purely based on the language alone. The factors that could influence this are, for example: word choice, writing style, sentence length, and more.\nSo what can we infer from this? The key aspects we are looking for are:\n How clustered are chapters from the same character? How separated are chapters from different characters? Are similar characters close to one another?  My Interpretation From what I can see, the honourable and consistent Eddard Stark occupies a distinct region of the plot, and doesn’t stray far from it, but he is joined by other chapters from the first book. Daenerys’s storyline is mostly separated from the rest of the characters, so it makes sense that her chapters are grouped together and distinct in the plot. Other characters, such as Arya, seem to be uniquely identifiable due to being separated from the other clusters. The questionably lovable dwarf, Tyrion, has a plotline which spans battlefields, court intrigue, romance, death, and more. This seems to be shown here by his embeddings being spread across the entire space, representing the large variability in his changing viewpoints and scenarios.\nSurprisingly, I expected the child characters, most notably Sansa and Bran, to occupy a distinct region because of their naive and childlike viewpoints, but their distributions aren’t too different from most other characters. In fact, Jon’s and Bran’s chapters seem to exist in the same regions of the plot, which could indicate a similarity in the characters, or at least how the characters are written.\n  Separation by Book There seems to be a divide based on the book which each chapter is written in. Just by switching the labels (and removing the top 10 character restriction), we can inspect the same plot with a different angle.\n   Mean BERT sentence embeddings of chapters in ASOIAF, split by book. The dimension was reduced from 768 to 3 using UMAP. See in fullscreen here.    The divide between classes is far more clear in this case2 - books 2 and 3 (ACOK and ASOS respectively), are far, far, different from the other books in the series. The first, fourth and fifth books (AGOT, AFFC and ADWD respectively), are more closely related, with the first book being in a more distinctive class. Books 4 and 5 were written at the same time, which might explain their embeddings being so intertwined.\n  Final Thoughts It is interesting how we can mathematically see the difference in language based on the book or the character. There is a clear mathematical distinction between writing styles used for different characters, and we have even shown the evolution of George RR Martin’s prose over the course of writing the ASOIAF series.\nWe can see the potential usefulness of these BERT embeddings, and they have far more use outside of plotting them to see their groups. We could’ve calculated the distance between embeddings to see exactly which characters are the most different or the most similar. We could use the embeddings themselves in a data science application; for example, how well can we classify which character is being written about based on only language?\nExisting research in NLP has given us the avenue to do all of this. It is an extremely interesting field, and is constantly developing. The methods we used here are free, and open to anyone for experimenting with. What other applications do you think would be cool to see? If you would like to ask any questions or have any discussion, see my contact page.\n  The UMAP transform was fit with hyperparameters min_dist = 0 and n_neighbors = 30, chosen by trial and error, to try to separate the classes as much as possible. All other values were kept as default in this function↩︎\n For completeness, here are the initialisms used for each book. AGOT: A Game of Thrones, ACOK: A Clash of Kings, ASOS: A Storm of Swords, AFFC: A Feast for Crows, ADWD: A Dance with Dragons↩︎\n   ","date":1642723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642771873,"objectID":"7a2db84f6a41cd8e9dbfeaa485213ada","permalink":"https://dannyjameswilliams.co.uk/post/asoiaf/","publishdate":"2022-01-21T00:00:00Z","relpermalink":"/post/asoiaf/","section":"post","summary":"For those of you familiar with either HBO’s Game of Thrones, or George RR Martin’s A Song of Ice and Fire (ASOIAF), you will probably be aware of the vast differences between each character and the depth of each storyline.","tags":["NLP","BERT","Data Visualisation"],"title":"Exploring Natural Language Embeddings of A Game of Thrones","type":"post"},{"authors":[],"categories":[],"content":"  Interested in how to plot BERT embeddings for different text? In the example linked, I obtained BERT embeddings for all the sentences from the book series ‘A Song of Ice and Fire’ (ASOIAF), the original novels which gave birth to the ‘A Game of Thrones’ TV show. By inspecting these embeddings, we could make mathematically make clear distinctions between how different characters were written, and how George RR Martin’s writing style changed for each book.\nThis is straightforward to do if you know how. Overall, you need to follow the process:\nGet the texts or the data that you want to embed and split it into chunks. Use each input in a forward pass on a pretrained BERT model, getting the last hidden state for the first token from the BERT model output. (Optional) Project each embedding into a lower dimension using dimensionality reduction, and plot.  These methods will use PyTorch. If you don’t have it installed, you will need to install it either via\npip install torch or\nconda install torch in your terminal. If you are using a notebook (like this tutorial), you must append an exclamation mark to the beginning of the lines.\n1. Get the Text or Data For the ASOIAF novels, I had access to the e-books, which I converted into a text file using Ghostscript. For the tutorial, I am going to assume that you have access to a file which contains the data you want to embed, otherwise I will be providing a different example. Due to copyright reasons, I will not be using any excerpts from a Song of Ice and Fire in this tutorial.\nInstead, I will use an open source data set from the datasets library, called sms_spam. This contains raw SMS messages and a hand-classified label indicating if they are spam or not. You can of course completely customise the text that you use, as all these methods will be generally applicable.\nDownload the dataset If you haven’t already, you will need to install the library into python, using e.g.\npip install datasets After that, we load the dataset.\nfrom datasets import load_dataset df = load_dataset(\u0026quot;sms_spam\u0026quot;, split=\u0026quot;train\u0026quot;) ## Reusing dataset sms_spam (/home/danny/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c) Once the dataset is loaded, we can inspect it:\ndf ## Dataset({ ## features: [\u0026#39;sms\u0026#39;, \u0026#39;label\u0026#39;], ## num_rows: 5574 ## }) Currently it is a huggingface Dataset object. It makes it easier to use a pandas dataframe, which we will use only 10% of (for simplicity, it’s a big dataset).\nimport pandas as pd df = pd.DataFrame(df).sample(frac=0.1) df.columns = [\u0026quot;text\u0026quot;, \u0026quot;label\u0026quot;] We can take a first look at the dataset using the .head() command giving us the top 5 entries\ndf.head() ## text label ## 2995 No idea, I guess we\u0026#39;ll work that out an hour a... 0 ## 3951 I got to video tape pple type in message lor. ... 0 ## 3134 So no messages. Had food?\\n 0 ## 3873 I am joining today formally.Pls keep praying.w... 0 ## 456 Si si. I think ill go make those oreo truffles.\\n 0 Here, a 1 corresponds to the text message being a spam message and 0 corresponds to a regular SMS.\nWe can embed each message (in the text column) individually, and see if there is any separability between spam messages and regular ones. Note that similar to the original ASOIAF example, we may also want to do an embedding for each individual sentence. In that case, we may want to split the text by sentence, then form a large dataframe for each sentence. However, for simplicity, I am going to embed each text in its entirety with BERT.\n  2. Pass the text through the BERT model To get access to the BERT model, we will use Huggingface. This is a python library and open source repository containing large amounts of language models and datasets. Like before, we will need to install the transformers library to access the models, via\npip install transformers then we can use python to load the BERT model.\nUsing a GPU or CPU If you have access to a GPU with CUDA set up, great! It makes the model run a lot faster, but requires moving variables to the cuda device. Otherwise, a CPU will be fine. Either way, you can run this line to set up the device that we are using.\nimport torch device = \u0026quot;cuda\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot; For me, this is:\ndevice ## \u0026#39;cuda\u0026#39;  Downloading and using the Model and Tokenizer Now we import two classes from Huggingface: the model itself, used for forward passes, and the tokenizer, used to translate raw text input into something that the model understands. Then we use the .from_pretrained() function to download the specific model and tokenizer from the BERT class of models that we want to use. In this case, I am specifying bert-base-cased, the smaller BERT model.\nfrom transformers import BertModel, BertTokenizer model = BertModel.from_pretrained(\u0026quot;bert-base-cased\u0026quot;).to(device) ## Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: [\u0026#39;cls.predictions.bias\u0026#39;, \u0026#39;cls.predictions.transform.LayerNorm.bias\u0026#39;, \u0026#39;cls.predictions.transform.LayerNorm.weight\u0026#39;, \u0026#39;cls.seq_relationship.bias\u0026#39;, \u0026#39;cls.seq_relationship.weight\u0026#39;, \u0026#39;cls.predictions.decoder.weight\u0026#39;, \u0026#39;cls.predictions.transform.dense.weight\u0026#39;, \u0026#39;cls.predictions.transform.dense.bias\u0026#39;] ## - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). ## - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). tokenizer = BertTokenizer.from_pretrained(\u0026quot;bert-base-cased\u0026quot;) Next I make a function where raw text is the input, and the output is a BERT embedding. I will explain each step in this function afterwards.\ndef embed(prompt): encoded_input = tokenizer(prompt, return_tensors=\u0026#39;pt\u0026#39;, padding=True, truncation=True).to(device) X = model(**encoded_input) return X.last_hidden_state[:, 0, :] The function inputs a prompt, which is a string. This string gets passed to the model’s tokenizer (which is already defined in the environment), which is padded and truncated. This means that if the string is too large for the model, it cuts off the end, and if it is too small, null strings get padded so that all inputs are the same length. Next, the encoded input goes through as forward pass of the model, which means augmenting the original input many times based on the parameters of BERT.\nDuring the tokenizing of the string, an extra [CLS] token is added to the beginning of the text, called a classification token. This token depends on the context of the sentence/paragraph because of the model architecture. This token is exactly what we want to use to represent our input, so we take the final hidden state output by the BERT model, and then the first element from that hidden state which corresponds to the [CLS] token, which is what the function is returning.\n Embedding the Text As an example of what the embedding actually is, we input a text and output a vector with shape:\nembed(\u0026quot;Help I\u0026#39;m stuck in a function input\u0026quot;).shape ## torch.Size([1, 768]) And so is a 768-dimensional representation of our input. Now we just need to repeat this for our entire data set to build our embeddings. In this example, we have a representation of Help I'm stuck in a function input, but we will want representations of text in our data. We preemptively make a new DataFrame for our embeddings, and embed each input text as a row, by looping over our data:\nembedding_df = pd.DataFrame(index=df.index, columns = range(768)) for i in range(len(df)): embedding_df.loc[df.index[i], :] = embed(df.text.values[i]).cpu().detach() Note that in my case, I am saving embedding_df to the CPU, as I won’t have enough memory to store this on my GPU. The extra .cpu().detach() line is not necessary if you have been using a CPU the whole time.\n  3. Project the Embeddings into a Lower Dimension to Plot Since the embeddings are 768-dimensional, we cannot inspect them manually to see how they look! Instead, we are going to use UMAP to project the 768 dimensions to 2 (or 3) and plot them as a scatter plot. For completeness, I will use the same hyperparameters as the original blog post, with n_neighbors=30 and min_dist=0, to try to preserve out-of-class clustering. See the basic UMAP parameters section on the UMAP documentation for more information.\nimport umap reducer = umap.UMAP(n_components = 2, n_neighbors = 30, min_dist=0.0, ) projected_embeddings = reducer.fit_transform(embedding_df.values) We have used the parameter n_components=2 as we want UMAP in 2D. If you wanted to do a 3D scatter plot, you could change this to n_components=3, but you would also need to adjust the scatter plot below to be 3D.\nNow we can plot the projected embeddings.\nimport matplotlib.pyplot as plt labels = df.label for l in labels.unique(): plt.scatter(projected_embeddings[labels == l, 0], projected_embeddings[labels == l, 1], label = l) plt.legend(); Recall: a label of 1 corresponds to spam and 0 corresponds to no spam. We can clearly see there is a large distinction between spam messages and regular text messages, as expected!\nThis indicates that the language that a spam message uses is identifiably different to the language from a regular message. Whilst this might seem trivial to a human, it means that we can use these embeddings to classify a spam message. Since we can easily draw a line to distinguish the two classes, a classification approach (such as logistic regression) can be straightforwardly applied to accurately detect if a message is a spam one.\nThese methods are obviously applicable to more complex scenarios, such as book chapters in the original post.\n ","date":1642636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642686292,"objectID":"0e6c8c15f95c10e2ce24733fae69d084","permalink":"https://dannyjameswilliams.co.uk/tutorials/tutorialasoiaf/","publishdate":"2022-01-20T00:00:00Z","relpermalink":"/tutorials/tutorialasoiaf/","section":"tutorials","summary":"Interested in how to plot BERT embeddings for different text? In the example linked, I obtained BERT embeddings for all the sentences from the book series ‘A Song of Ice and Fire’ (ASOIAF), the original novels which gave birth to the ‘A Game of Thrones’ TV show.","tags":["NLP","Tutorial","Python"],"title":"(Tutorial) Exploring Natural Language Embeddings","type":"tutorials"},{"authors":["Daniel Williams"],"categories":null,"content":"  This research serves as a pre-cursor to the work that I am carrying out as part of my PhD, and involves unnormalised and truncated probability density estimation. This is a field which I find extremely interesting, and I believe that it has far reaching applications not just across data science and machine learning, but across many other disciplines as well. This blog post is an attempt at a more accessible explanation behind some of the core concepts in this paper (in pre-print), by Song Liu, Takafumi Kanamori and myself.\nFirstly, consider the locations of reported crimes in Chicago below, and think about where the ‘centres’ of these reported crimes are.\n  Figure 1: Homicide locations inside the city of Chicago recorded in 2008.    Now think about where typical statistical estimation methods would put these crime ‘centres’. How do you think these would differ to what the ‘true’ centres are? Something like maximum likelihood estimation may be slightly incorrect in some way, because the reported crimes get cut off, or truncated beyond the city’s borders.\nThe dataset is incomplete; crimes do not magically stop where we decide the city ends, but data collection does.\nThis is an example of a more general problem in statistics named truncated probability density estimation: How do we estimate the parameters of a statistical model when data are not fully observed, and are cut off by some artificial boundary?\nThe Problem A statistical model (a probability density function involving some parameters \\(\\boldsymbol{\\boldsymbol{\\theta}}\\)) is defined as:\n\\[ p(\\boldsymbol{x}; \\boldsymbol{\\boldsymbol{\\theta}}) = \\frac{1}{Z(\\boldsymbol{\\boldsymbol{\\theta}})} \\bar{p}(\\boldsymbol{x};\\boldsymbol{\\boldsymbol{\\theta}}), \\qquad Z(\\boldsymbol{\\boldsymbol{\\theta}}) = \\int \\bar{p}(\\boldsymbol{x};\\boldsymbol{\\boldsymbol{\\theta}})d\\boldsymbol{x}. \\]\nThis is made up of two components: \\(\\bar{p}(\\boldsymbol{x}; \\boldsymbol{\\boldsymbol{\\theta}}\\)), the unnormalised part of the model, which is known analytically; and \\(Z(\\boldsymbol{\\boldsymbol{\\theta}})\\), the normalising constant. Over complicated boundaries such as city borders, this normalising constant cannot be calculated directly, which is why something like maximum likelihood would fail in this case.\nWhen we perform estimation, we aim to find \\(\\boldsymbol{\\boldsymbol{\\theta}}\\) which makes our model density, \\(p(\\boldsymbol{x}; \\boldsymbol{\\boldsymbol{\\theta}})\\), as closely resemble a theoretical data density, \\(q(\\boldsymbol{x})\\), as possible. Usually, we can estimate \\(\\boldsymbol{\\boldsymbol{\\theta}}\\) by trying to minimise some measure of difference between the \\(p(\\boldsymbol{x};\\boldsymbol{\\boldsymbol{\\theta}})\\) and \\(q(\\boldsymbol{x})\\). But we cannot do this while \\(Z(\\boldsymbol{\\boldsymbol{\\theta}})\\) is unknown!\nMost methods in this regard opt for a numerical approximation to integration, such as MCMC. But these methods are usually very slow and computationally heavy. Surely there must be a better way?\n A Recipe for a Solution Ingredient #1: Score Matching A novel estimation method called score matching enables estimation even when the normalising constant, \\(Z(\\boldsymbol{\\boldsymbol{\\theta}})\\), is unknown. Score matching begins by taking the derivative of the logarithm of the probability density function, i.e.,\n\\[ \\nabla_\\boldsymbol{x} \\log p(\\boldsymbol{x}; \\boldsymbol{\\theta}) = \\nabla_\\boldsymbol{x} \\log \\bar{p}(\\boldsymbol{x};\\boldsymbol{\\theta}), \\]\nwhich has become known as the score function. When taking the derivative, the dependence on \\(Z(\\boldsymbol{\\theta})\\) is removed. To estimate the parameter vector \\(\\boldsymbol{\\theta}\\), we can minimise the difference between \\(q(\\boldsymbol{x})\\) and \\(p(\\boldsymbol{x};\\boldsymbol{\\theta})\\) by minimising the difference between the two score functions, \\(\\nabla_\\boldsymbol{x} \\log p(\\boldsymbol{x}; \\boldsymbol{\\theta})\\) and \\(\\nabla_\\boldsymbol{x} \\log q(\\boldsymbol{x})\\). One such distance measure is the expected squared distance, so that score matching aims to minimise\n\\[ \\mathbb{E} [\\| \\nabla_\\boldsymbol{x} \\log p(\\boldsymbol{x}; \\boldsymbol{\\theta}) - \\nabla_\\boldsymbol{x} \\log q(\\boldsymbol{x})\\|_2^2]. \\]\nWith this first ingredient, we have eliminated the requirement that we must know the normalising constant.\n Ingredient #2: A Weighting Function Heuristically, we can imagine our weighting function should vary with how close a point is to the boundary. To satisfy score matching assumptions, we require that this weighting function \\(g(\\boldsymbol{x})\\) must have the property that \\(g(\\boldsymbol{x}\u0026#39;)\\) = 0 for any point \\(\\boldsymbol{x}\u0026#39;\\) on the boundary. A natural candidate would be the Euclidean distance from a point \\(\\boldsymbol{x}\\) to the boundary, i.e.,\n\\[ g(\\boldsymbol{x}) = \\|\\boldsymbol{x} - \\tilde{\\boldsymbol{x}}\\|_2, \\qquad \\tilde{\\boldsymbol{x}} = \\text{argmin}_{\\boldsymbol{x}\u0026#39;\\text{ in boundary}} \\|\\boldsymbol{x} - \\boldsymbol{x}\u0026#39;\\|_2. \\]\nThis easily satisfies our criteria. The distance is going to be exactly zero on the boundary itself, and will approach zero the closer the points are to the edges.\n Ingredient #3: Any Statistical Model Since we do not require knowledge of the normalising constant through the use of score matching, we can choose any probability density function \\(p(\\boldsymbol{x}; \\boldsymbol{\\theta})\\) that is appropriate for our data. For example, if we are modelling count data, we may choose a Poisson distribution. If we have some sort of centralised location data, such as in the Chicago crime example in the introduction, we may choose a multivariate Normal distribution.\n Combine Ingredients and Mix We aim to minimise the expected distance between the score functions, and we weight this by our function \\(g(\\boldsymbol{x})\\), to give\n\\[ \\min_{\\boldsymbol{\\theta}} \\mathbb{E} [g(\\boldsymbol{x}) \\| \\nabla_\\boldsymbol{x} \\log p(\\boldsymbol{x}; \\boldsymbol{\\theta}) - \\nabla_\\boldsymbol{x} \\log q(\\boldsymbol{x})\\|_2^2]. \\]\nThe only unknown in this equation now is the data density \\(q(\\boldsymbol{x})\\) (if we knew the true data density, there would be no point in estimating it with \\(p(\\boldsymbol{x};\\boldsymbol{\\theta})\\)). However, we can rewrite this equation using integration by parts as\n\\[ \\min_{\\boldsymbol{\\theta}} \\mathbb{E} \\big[ g(\\boldsymbol{x}) [\\|\\nabla_\\boldsymbol{x} \\log p(\\boldsymbol{x}; \\boldsymbol{\\theta})\\|_2^2 + 2\\text{trace}(\\nabla_\\boldsymbol{x}^2 \\log p(\\boldsymbol{x}; \\boldsymbol{\\theta}))] + 2 \\nabla_\\boldsymbol{x} g(\\boldsymbol{x})^\\top \\nabla_\\boldsymbol{x} \\log p(\\boldsymbol{x};\\boldsymbol{\\theta})\\big]. \\]\nThis can be numerically minimised, or if \\(p(\\boldsymbol{x}; \\boldsymbol{\\theta})\\) is in the exponential family, analytically minimised to obtain estimates for \\(\\boldsymbol{\\theta}\\).\n  Results Artificial Data As a sanity check for testing estimation methods, it is often reassuring to perform some experiments on simulated data before moving to real world applications. Since we know the true parameter values, it is possible to calculate how far our estimated values are from their true counterparts, thus giving a way to measure estimation accuracy.\nPictured below in Figure 2 are two experiments where data are simulated from a multivariate normal distribution with mean \\(\\boldsymbol{\\mu}^* = [1, 1]\\) and known variance \\(\\sigma^2 = 1\\). Our aim is to estimate the parameter \\(\\hat{\\boldsymbol{\\mu}}\\) to be close to \\([1,1]\\). The red crosses in the image are the true means at \\([1,1]\\), and the red dots are the estimates given by truncated score matching.\n  Figure 2 (a): Points are only visible around a unit ball from [0,0].      Figure 2 (b): Points are only visible inside a box to the left of the mean.    These figures clearly indicate that in this basic case, this method is giving sensible results. Even by ignoring most of our data, as long as we formulate our problem correctly, we can still get accurate results for estimation.\n Chicago Crime Back to our motivating example, where the task is to estimate the ‘centres’ of reported crime in the city of Chicago, given the longitudes and latitudes of homicides in 2008. Our specification changes from the synthetic data somewhat:\n from the original plots, it seems like there are two centres, so the statistical model we choose is a 2-dimensional mixed multivariate Normal distribution; the variance is no longer known, but to keep estimation straightforward, the standard deviation \\(\\sigma\\) is fixed so that \\(2\\sigma\\) roughly covers the ‘width’ of the city (\\(\\sigma = 0.06^\\circ\\)).  Below I implement estimation for the mean for both truncated score matching and maximum likelihood.\n  Figure 3: Homicides in Chicago recorded in 2008, with the means estimated by truncated score matching (truncSM) and maximum likelihood estimation (MLE).    Truncated score matching has placed its estimates for the means in similar, but slightly different places than standard MLE. Whereas the MLE means are more central to the truncated dataset, the truncated score matching estimates are placed closer to the outer edges of the city. For the region of crime in the top half of the city, the data are more ‘tightly packed’ around the border – which is a property we expect of Normally distributed data closer to its mean.\nWhilst we don’t have any definitive ‘true’ mean to compare it to, we could say that the truncSM mean is closer to what we would expect than the one estimated by MLE.\nNote that this result does not reflect the true likelihood of crime in a certain neighbourhood, and has only been presented to provide a visual explanation of the method. The actual likelihood depends on various characteristics in the city that are not included in our analysis, see here for more details.\n  Final Thoughts Score matching is a powerful tool, and by not requiring any form of normalising constant, enables the use of some more complicated models. Truncated probability density estimation is just one such example of an intricate problem which can be solved with this methodology, but it is one with far reaching and interesting applications.\nWhilst this blog post has focused on location-based data and estimation, truncated probability density estimation could have a range of applications. For example, consider disease/virus modelling such as the COVID-19 pandemic: The true number of cases is obscured by the number of tests that can be performed, so the density evolution of the pandemic through time could be fully modelled with incorporation of this constraint using this method. Other applications as well as extensions to this method will be fully investigated in my future PhD projects.\nFurther Reading and Bibliography Firstly, here is the github page where you can reproduce all the plots made available in this blog post, as well as some generic code for the implementation of truncated score matching.\nThe original score matching reference:\nEstimation of Non-Normalized Statistical Models by Score Matching, Aapo Hyvärinen (2005)\nThe paper from which this blog post originates:\nEstimating Density Models with Truncation Boundaries using Score Matching. Song Liu, Takafumi Kanamori, \u0026amp; Daniel J. Williams (2021)\n  ","date":1624537143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624894740,"objectID":"2e62dba1c8734924f83ecc8a9b8f68a1","permalink":"https://dannyjameswilliams.co.uk/post/nodata/","publishdate":"2021-06-24T12:19:03Z","relpermalink":"/post/nodata/","section":"post","summary":"This research serves as a pre-cursor to the work that I am carrying out as part of my PhD, and involves unnormalised and truncated probability density estimation. This is a field which I find extremely interesting, and I believe that it has far reaching applications not just across data science and machine learning, but across many other disciplines as well.","tags":["R","programming"],"title":"How can we do data science without all of our data?","type":"post"},{"authors":[],"categories":[],"content":" The notion of true random has perplexed scientists and statisticians for decades, if not longer. But there’s one process that we still don’t know much about - the human brain. What is it that causes our decisions to be made? When we are asked to ‘pick a random number between 1 and 10’, how ‘random’ is the number we give? Is it some complicated, deterministic signal of neurons in our brain? Or is it actually random?\nThe following analysis and results come from a survey collected online, which obtained \\(n = 2190\\) participants.\nIf you’re in a hurry, or don’t fancy reading this article, check out the handy infographic.\nPick a random number between 1 and 10, twice This is perhaps the most basic question to ask, and one that is very visually interpretable. This question was asked in two distinct ways, pictured below:\n  Question A.      Question B.    These questions are asked with the following hypothesis: does the answer input being on a scale change how people answer the question? In another way, if you are picking a random number, and you can see the numbers laid out in order in front of you, are you more likely to pick a number which is more central? Let’s look at the results to find out.\n    Frequencies of choices between 1 and 10 for question style A (left) and question style B (right). Included is a line of the expected value \\(\\mathbb{E}(\\text{no. of choices}) = np\\), plotted within a \\(\\pm 1.96 \\cdot \\sqrt{\\sigma}\\) confidence interval, where \\(\\sigma\\) is the binomial variance.    Surprisingly, both results are reasonably similar. Some statistics of these are:\n 4 is the most frequent number in both cases, different from the usual value of 7 that has been in similar surveys before. In the comments for this survey, people seemed to expect 7 to be the most picked. It is possible that since the participants were aware of previous research in the area, they deliberately would not have picked 7. The average ‘difference from uniformity’ was around 2.1% for both types of questions. This is the percentage difference from the expected value. 10.1% of people picked the same number for both questions, and for true random number generation, the true value would be 10%! The values at the edges, 1 and 10, were picked far less often than values in the centre. The largest difference between questions was that 10 was picked less in question type B than question type A. A possible reason is that 10 is two digits long, which would require extra effort for question type B than for question type A. A Pearson’s \\(\\chi^2\\) test for uniformity showed \\(p\\)-values where \\(p \u0026lt;\u0026lt; 0.0001\\), indicating with a reasonable level of certainty that neither sets of answers were uniformly distributed.  So people don’t seem to be entirely random in this case, but this is only one aspect of randomness. People picked the same number for both questions the correct amount of times! How about pairs of answers, i.e. for both answer A and answer B?\n   Distribution of (answer A - answer B), plotted with the expected Irwin–Hall distribution.    If we subtract the answers from one another, we are essentially summing uniform random variables, which are expected to have a Irwin-Hall distribution, or a triangular distribution when we are only summing two variables.\nPut simply, imagine you are rolling two six-sided dice. The most common sum of their values is 7, since there are more combinations that can sum to 7 than anything else. The same is true in this case, we expect the most common result of answer A - answer B to be 0, and then equally 1 and -1, and so on.\nOur distribution above actually does look similar to the expected triangular distribution (although we do have some oddities towards the centre of the triangle, likely caused by the lack of 1’s and 10’s). Does this mean human randomness isn’t too bad after all? In this aspect, we are quite good with the pairs of our answers, even if our individual answers aren’t quite as good.\n Pick a random letter from the alphabet Okay, so we have trouble with edge effects - we don’t like to pick 1s and 10s when we are trying to randomly think of a number. What if we were to randomly select a letter? There’s no real intrinsic numeric value associated to a letter (unless you count its position in the alphabet), so maybe we are better at selecting a random letter. Let’s take a look.\n   Frequency of chosen letters arranged alphabetically.    Ah, so we can’t select letters randomly either, but it doesn’t seem to depend on any sort of edge effects alphabetically. That is, people seem to pick A and Z a healthy amount. The frequencies for each letter are very far from what we’d expect if we were to have sampled these letters uniformly. What could have been affecting our judgement here? Maybe the popularity of a certain letter in the English language?\nPlotted below is the relationship between the frequency at which a letter occurs in the English language (as a percentage) versus the frequency it was picked in the survey.\n   Letter percentage in the English Language plotted against the number of times the letter was picked in the survey. The red line is a line of best fit captured from a linear regression with a cubic polynomial transform on the English language percentage.    There is no significant relationship here, the \\(R^2\\) value for a linear regression is very low, at \\(R^2 \\approx 0.152\\), meaning the model is not capturing the variability in the data. What else could be affecting how we are choosing these letters?\nEarlier I stated the assumption that there was no intrinsic numeric value associated to a letter, so that we would not have any trouble with edge effects skewing the results. That assumption was wrong! If you’re reading this on a computer or laptop, take a look what is just underneath your screen.\n   Frequency of chosen letters as a heatmap overlaid on a keyboard.    It turns out that the most frequently picked letters are those that are most central on a QWERTY keyboard. This notion of picking the most central option is corroborated by the choice of number in the first question, being that people do not like to pick numbers or letters that appear on what they consider as the ‘edge’.\n Pick a random number between 1 and 50 What if our selection range is so large that these edge effects can be nullified? If we ask people to select numbers between 1 and 50, will we see an extension of what we saw in the first question? I.e. are people going to pick the most central values again (say, between 10 and 40), or will it be something else?\n   Frequency of chosen numbers between 1 and 50.    An interesting selection of numbers here. This range does not seem to be uniformly distributed in the slightest. Here are some facts:\n Against the expected value of 10%, only 4.3% of people selected a multiple of 10, whereas 18.7% of people chose a number with a 7, e.g. 17, 27 etc. The lowest picked number was 30, by 0.5% of people, and the highest picked number was 37, by 5.8% of people.   Conclusions Perhaps you remember the ‘trick’ that your friend would play on you as a kid in the playground. They would ask you a series of maths questions, and then ask you to name a vegetable. You would inevitably say ‘carrot’, then they’d reveal a piece of paper with the word ‘carrot’ on it. Were you truly tricked? Turns out, if you’re asked to name a vegetable, most people say carrot regardless. It’s not a trick, when asked to name something at random, we pick an extremely common vegetable.\n ","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602770953,"objectID":"40195b0756acabfe8b8582649f01a881","permalink":"https://dannyjameswilliams.co.uk/post/randomchoices/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/post/randomchoices/","section":"post","summary":"The notion of true random has perplexed scientists and statisticians for decades, if not longer. But there’s one process that we still don’t know much about - the human brain.","tags":["Probability","Analysis","Data Visualisation"],"title":"How random are you?","type":"post"},{"authors":[],"categories":[],"content":"    “2020 I’mma run the whole election” may not sound like the words of a lyrical genius, but I ask you to “name one genius that ain’t crazy”. For the duration of your read of this post, take a minute to separate an artist from their art. It is undeniable that Kanye West has had serious influence over the musical industry during his career. He has single-handedly influenced hip hop since his first album, The College Dropout, and that’s not to mention all of his awards. \u0026quot;I woke up early this mornin’ with a new state of mind A creative way to rhyme without usin’ nines and guns\u0026quot;  \nControversy aside, what makes Kanye West so influential to the hip-hop industry? His lyrics must play a part to his success. The Google Natural Language API can perform a full language analysis of Kanye West’s lyrics for free. This uses a pre-trained natural language model by Google, and the API is available for use in Python.\nThe data were mostly obtained by using the genius package in R, but missing entries were copied and pasted manually from the Genius website. The analysis is restricted to his studio albums, but not solo albums, so that his collaborations with Kid Cudi and Jay Z are included. All albums considered are The College Dropout, Late Registration, Graduation, 808’s \u0026amp; Heartbreak, My Beautiful Dark Twisted Fantasy, Watch the Throne, Yeezus, The Life of Pablo, ye, Kids See Ghosts and Jesus is King.\nThis article is split into four sections:\n Entity Analysis Sentiment Analysis Sentiment and Magnitude against Album Reception Generating a New Kanye Song  All code used for the analysis and graphics in this post can be found in the github repository kanyenet.\nEntity Analysis The natural language API provided by Google includes entity analysis - the identification of entities, which can be interpreted as the ‘important parts’ of the text. Each song is passed individually to the API, collating the number of entities for each song. Below is a network of word connections, where the links between nodes represent words that appear in the same song. The graph is interactive, so you can scroll around and zoom in and out to see all the connections. You can click on a node to view the total number of times it appears across all songs.\nIt might take a while to load, and is best viewed on desktop. Content warning: An effort has been made to censor offensive words, but some may have slipped through the cracks.   {\"x\":{\"links\":{\"source\":[0,0,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,4,5,5,5,5,5,5,5,5,5,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,9,9,9,10,10,10,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,14,14,14,14,14,14,15,15,15,15,15,15,16,16,16,16,16,16,17,17,17,18,18,18,19,19,19,19,19,19,19,19,19,19,20,20,21,21,22,22,23,23,23,23,23,23,23,24,24,25,25,25,25,26,26,27,27,27,27,27,28,28,28,28,28,29,29,29,29,29,29,29,29,30,30,30,31,31,32,32,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,35,35,36,36,36,36,36,36,36,36,37,37,37,37,37,37,38,39,39,39,40,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,43,44,45,46,46,47,48,48,48,49,49,49,49,49,50,50,50,51,51,51,51,52,52,52,53,53,53,53,53,54,54,54,54,55,55,55,56,56,56,56,57,57,57,57,58,58,58,58,59,59,60,60,60,61,61,61,61,61,61,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,64,64,65,65,65,65,65,65,65,66,66,67,67,67,67,67,68,68,68,68,68,68,69,69,69,69,70,71,71,71,71,71,71,71,71,71,71,72,72,72,72,72,73,73,74,74,74,75,75,75,75,76,77,77,78,79,80,80,80,80,80,80,81,81,82,83,83,83,83,83,83,83,83,83,83,83,84,84,85,85,86,86,86,86,87,87,87,87,88,89,89,89,89,89,89,89,89,89,89,90,90,90,90,91,91,92,92,92,92,92,93,93,94,95,95,95,96,96,97,98,98,98,98,98,98,98,98,98,98,98,98,98,98,99,100,100,100,100,100,101,102,103,103,104,104,105,106,107,108,108,108,109,109,109,109,109,109,109,110,110,110,111,111,111,111,112,112,113,113,113,113,113,114,114,115,116,116,116,116,116,116,116,116,116,116,117,117,117,117,117,117,118,119,119,119,120,120,121,121,122,123,124,125,125,125,125,125,125,125,125,125,125,126,127,127,128,128,129,129,129,129,129,130,130,130,131,132,133,134,135,135,136,137,138,139,139,139,139,139,139,140,140,140,141,141,141,142,143,144,145,146,147,148,148,149,149,150,150,151,152,153,154,155,156,157,157,158,159,160,161,162,163,164,164,165,166,167,168],\"target\":[0,124,1,95,121,167,2,15,71,116,125,139,155,3,7,90,116,161,4,5,36,61,83,98,117,140,164,165,6,17,59,80,7,77,79,86,90,109,125,127,132,154,156,161,8,30,96,101,9,112,168,10,73,89,11,43,48,66,90,12,33,34,62,63,83,98,116,125,129,139,144,13,20,76,14,53,75,89,90,97,15,71,116,125,139,155,16,113,116,117,139,154,17,59,80,18,26,142,19,58,89,92,98,125,130,131,149,153,20,76,21,61,22,126,23,37,49,57,108,150,151,24,45,25,50,98,162,26,142,27,65,119,120,125,28,54,89,104,166,29,42,68,71,80,110,116,134,30,96,101,31,115,32,89,33,34,62,63,83,98,116,125,129,139,144,34,62,63,83,98,116,125,129,139,144,35,70,36,61,83,98,117,140,164,165,37,49,57,108,150,151,38,39,42,125,40,41,61,67,72,109,116,125,42,68,71,80,110,116,125,134,157,163,43,48,66,90,98,125,129,133,139,44,45,46,82,47,48,66,90,49,57,108,150,151,50,98,162,51,52,141,147,52,141,147,53,75,89,90,97,54,89,104,166,55,99,102,56,74,103,139,57,108,150,151,58,130,149,153,59,80,60,84,146,61,67,72,83,98,106,109,116,117,125,137,140,164,165,62,63,83,98,116,125,129,139,144,63,83,98,116,125,129,139,144,64,159,65,81,85,107,119,120,125,66,90,67,72,109,116,125,68,71,80,110,116,134,69,139,141,145,70,71,80,89,110,116,125,134,136,139,155,72,109,116,125,158,73,89,74,103,139,75,89,90,97,76,77,79,78,79,80,89,105,110,116,134,81,160,82,83,98,116,117,125,129,139,140,144,164,165,84,146,85,107,86,109,127,132,87,98,109,123,88,89,90,92,97,98,104,105,125,131,166,90,91,97,161,91,152,92,98,122,125,131,93,94,94,95,121,167,96,101,97,98,109,116,117,123,125,129,131,139,140,144,162,164,165,99,100,111,129,148,154,101,102,103,139,104,166,105,106,107,108,150,151,109,116,123,125,127,129,132,110,116,134,111,129,148,154,112,168,113,116,117,139,154,114,126,115,116,117,125,129,134,139,143,144,154,155,117,139,140,154,164,165,118,119,120,125,120,125,121,167,122,123,124,125,129,131,136,137,139,144,154,155,156,126,127,132,128,166,129,139,144,148,154,130,149,153,131,132,133,134,135,154,136,137,138,139,141,144,145,154,155,140,164,165,141,145,147,142,143,144,145,146,147,148,154,149,153,150,151,151,152,153,154,155,156,157,163,158,159,160,161,162,163,164,165,165,166,167,168],\"value\":[1,2,1,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,1,1,2,2,2,2,2,2,2,2,1,2,2,2,6,2,2,2,2,2,2,2,2,2,2,2,1,2,2,2,1,2,2,1,2,2,1,2,2,2,2,4,4,4,4,4,4,4,8,4,4,4,4,1,2,2,2,2,2,2,2,2,1,2,2,2,2,2,1,2,2,2,2,2,1,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,1,2,1,4,1,2,1,2,2,2,2,2,2,1,2,1,2,2,2,1,2,1,2,2,2,2,1,2,2,2,2,1,2,2,2,2,2,2,2,1,2,2,1,2,1,2,1,2,2,2,2,2,4,2,2,2,2,1,2,2,2,2,4,2,2,2,2,1,2,1,2,2,2,2,2,2,2,1,2,2,2,2,2,1,1,2,2,1,1,2,2,2,2,2,2,3,2,2,2,2,2,2,2,2,2,5,2,2,2,2,2,2,2,2,1,1,1,2,1,2,2,2,1,2,2,2,2,1,2,2,1,2,2,2,1,2,2,1,2,2,2,2,1,2,2,2,2,2,2,1,2,2,2,1,2,2,2,1,2,2,2,1,2,1,2,2,8,2,2,2,2,2,2,2,2,4,2,2,2,2,1,2,2,2,4,2,2,2,2,1,2,2,4,2,2,2,2,1,2,5,2,2,2,2,2,2,1,2,1,2,2,2,2,1,2,2,2,2,2,1,2,2,2,1,5,2,2,2,4,4,2,2,2,2,2,2,2,2,2,1,2,1,2,2,1,2,2,2,1,1,2,2,1,3,2,2,2,2,2,3,2,1,3,4,4,2,2,2,2,2,2,2,2,1,2,1,2,1,2,2,2,1,2,2,2,1,8,2,2,2,2,2,2,4,2,2,4,2,2,2,2,2,3,2,2,2,2,1,2,1,1,2,2,1,2,1,8,4,4,2,2,6,4,2,2,2,2,2,2,2,1,1,2,2,2,2,1,1,1,2,1,2,1,1,1,1,2,2,4,2,2,2,2,2,2,1,2,2,1,2,2,2,1,2,1,2,2,2,2,1,2,1,17,2,8,4,2,8,2,4,2,2,2,2,2,2,2,2,1,1,2,2,1,2,1,2,1,3,1,14,2,2,2,2,4,2,2,2,2,2,1,2,1,2,4,4,2,2,2,1,2,2,1,1,2,1,1,4,1,1,1,6,2,2,2,2,2,1,2,2,2,2,2,1,1,1,1,1,1,1,2,1,2,1,2,1,1,1,8,1,1,1,2,1,1,1,3,1,1,1,2,1,2,1,1],\"colour\":[\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\",\"#c2c2c2\"]},\"nodes\":{\"name\":[\"300\",\"a million\",\"air\",\"all\",\"america\",\"ass\",\"ayy\",\"baby\",\"baby jesus\",\"ball\",\"bang\",\"beam\",\"beat\",\"big brother\",\"bitch\",\"bitches\",\"blame game\",\"bottle\",\"bout\",\"boy\",\"brother\",\"bulls***\",\"c'mon homie\",\"cab\",\"champion\",\"chance\",\"chill\",\"church\",\"city\",\"concert\",\"coretta\",\"crack music n****\",\"dad\",\"dame\",\"deal\",\"dem\",\"dessert\",\"destination\",\"diamond\",\"door\",\"dream\",\"eighteen\",\"everybody\",\"everything\",\"ey 'ey 'ey\",\"eye\",\"f***\",\"fadin\",\"faith\",\"fare\",\"fire\",\"fly\",\"fore\",\"freak\",\"freedom\",\"friend\",\"frightenin\",\"front\",\"gangsta\",\"genie\",\"ghost\",\"girl\",\"glass\",\"glasses\",\"glory\",\"god\",\"god dream\",\"gold digger\",\"gossip\",\"graveshift\",\"gwaan\",\"hand\",\"head\",\"hell\",\"help\",\"highlight\",\"hip hop brother\",\"hoe\",\"home\",\"homie\",\"i'ma\",\"jesus\",\"jungle\",\"kanye\",\"kid\",\"king\",\"l.a.\",\"la la\",\"lie\",\"life\",\"light\",\"lord\",\"love\",\"love lock-down\",\"love lockdown\",\"luxury\",\"malcolm\",\"mama\",\"man\",\"many\",\"mars\",\"martin\",\"memorie\",\"menacin\",\"mind\",\"mine\",\"mistake\",\"mob\",\"moment\",\"money\",\"monster\",\"moon\",\"mothaf***a\",\"motherf***er\",\"murder\",\"music\",\"n****\",\"name\",\"new\",\"night sky\",\"nightlife\",\"nike\",\"nobody\",\"nothing\",\"omen\",\"one\",\"paper\",\"parties\",\"party\",\"people\",\"pimp\",\"pinocchio\",\"place\",\"power\",\"profit\",\"rain\",\"reason\",\"robocop\",\"rosie\",\"s***\",\"salad\",\"sky\",\"slave\",\"somebody\",\"song\",\"spaceship\",\"spirit\",\"spot\",\"star\",\"step\",\"street\",\"street light\",\"stress\",\"talk\",\"thing\",\"thirty\",\"toast\",\"two\",\"vision\",\"war\",\"water\",\"way\",\"wire\",\"word\",\"work\",\"workout plan\",\"world\",\"yeezy\",\"zone\"],\"group\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169],\"nodesize\":[6,5,12,14,7,7,8,49,6,6,5,6,14,14,20,5,11,6,5,15,6,9,11,5,9,6,8,11,6,9,5,19,5,7,7,12,5,5,8,5,5,5,20,42,12,7,5,8,10,5,9,8,8,6,8,16,6,5,6,6,22,64,8,9,8,52,9,8,6,8,6,45,21,5,6,6,6,8,19,22,23,29,7,41,21,6,13,9,8,50,42,27,25,12,9,11,6,6,62,6,5,6,5,6,5,6,10,6,5,27,12,9,7,6,17,12,84,21,9,13,5,5,15,27,5,97,10,12,10,29,6,5,5,16,7,6,6,8,7,54,5,19,7,5,5,10,6,8,9,24,6,5,9,6,72,5,15,15,7,8,12,18,5,12,6,8,11,16,9],\"size\":[6,5,12,14,7,7,8,49,6,6,5,6,14,14,20,5,11,6,5,15,6,9,11,5,9,6,8,11,6,9,5,19,5,7,7,12,5,5,8,5,5,5,20,42,12,7,5,8,10,5,9,8,8,6,8,16,6,5,6,6,22,64,8,9,8,52,9,8,6,8,6,45,21,5,6,6,6,8,19,22,23,29,7,41,21,6,13,9,8,50,42,27,25,12,9,11,6,6,62,6,5,6,5,6,5,6,10,6,5,27,12,9,7,6,17,12,84,21,9,13,5,5,15,27,5,97,10,12,10,29,6,5,5,16,7,6,6,8,7,54,5,19,7,5,5,10,6,8,9,24,6,5,9,6,72,5,15,15,7,8,12,18,5,12,6,8,11,16,9]},\"options\":{\"NodeID\":\"name\",\"Group\":1,\"colourScale\":\"d3.scaleOrdinal(d3.schemeCategory20);\",\"fontSize\":18,\"fontFamily\":\"Calibri\",\"clickTextSize\":45,\"linkDistance\":\"function(d){return d.value * 10}\",\"linkWidth\":\"function(d) { return Math.sqrt(d.value); }\",\"charge\":-150,\"opacity\":0.9,\"zoom\":true,\"legend\":false,\"arrows\":false,\"nodesize\":true,\"radiusCalculation\":\" Math.sqrt(d.nodesize)+6\",\"bounded\":false,\"opacityNoHover\":0.15,\"clickAction\":\"alert(\\\"Total count of '\\\" + (d.name) + \\\"': \\\" + (d.size) + \\\"\\n\\\")\"}},\"evals\":[\"options.linkDistance\"],\"jsHooks\":{\"render\":[{\"code\":\"function(el, x) { \\n d3.selectAll(\\\".node text\\\").style(\\\"fill\\\", \\\"black\\\");\\n }\",\"data\":null}]}}  Network of entities. Connections between nodes represent entities being in the same song.    \nTo see a larger version of this network, with the ability to filter by album and minimum number of occurences of a word, click here to view it as an R shiny app.\nThis graph only contains entities that appear more than 4 times across all songs, as the full network would contain far too much information, and nothing would be visible. The larger size of the node indicates a word being more frequent across all songs.\nSince this undirected graph is not fully connected, then it is impossible to connect all entities to each other via other entities that appear in the same song. Here we can also see the most common words at the center of the graph, and as expected, the most frequent words also seem to have the most connections.\nWe can also note a few interesting qualities; firstly that Kanye never talks about Jesus in a song without also talking about God. Secondly, Kanye only talks about mistakes when he also talks about girls, I wonder what could that mean?\nThemes are also apparent in different clusters of the graph, for example, a small section containing star, moon and mars, or entities such as workout plan, dessert and salad in another section, seemingly referring to exercise and health. Particular songs can also be isolated out at the outer sections of the graph, which explains some of the more uncommon words that appear with few connections.\nBut how do the frequency of these entities change over time? We can inspect the top 10 entities separately for each album, which gives a good indication of their individual themes, as well as Kanye’s use of different lyricism throughout his career.\n  Top 10 entities by album in order of their release.    Kanye’s albums have a broad range of different themes, which correspond to different entities being more prevalent within them. Kanye’s most heartfelt album, 808’s \u0026amp; Heartbreak (the fourth album) contained no curse words, but instead had more references to ‘love’ and ‘life’. It is easy to judge the theme of each album by their most common entities, such as Jesus is King (the final one). Again there are no profanities, instead the album is heavily focused on religion, being a gospel album. Here, ‘Jesus’ is the most common entity, followed by god.\nSome of the entities which might seem ‘irrelevant’ that can be found within these charts often correspond to a single song with purposeful repetition. ‘Toast’ appears in the top 10 entities for the album My Beautiful Dark Twisted Fantasy, not because it is an amazing breakfast food, but because it is repeated in the chorus of Runaway. “Let’s have a toast for the douche bags, let’s have a toast for the assholes,\nLet’s have a toast for the scumbags, every one of them that I know”  \nAs a side note, you may wonder why the word ‘amazing’ does not appear in the top 10 entities for 808’s \u0026amp; Heartbreak, due to it appearing a whopping 55 times in the song Amazing. Thankfully, Google’s API does not classify it as an entity, since it is actually an adjective. However, ‘love lockdown’ and ‘robocop’ still made it through, as both words are repeated many times in their own songs.\n Sentiment Analysis Natural language analysis can also classify the sentiment of a piece of text, in this case, the sentiment of song lyrics, one song at a time. The sentiment ranges from -1 to 1, where a negative/positive value means the song has a lower/higher sentiment, generally referring to the mood of the song - whether it is more uplifting or sad.\nFirstly, the sentiment API extracts the sentiment from each sentence separately. We can take a look at the density of all sentence sentiments below.\n  Density of sentence sentiment.    So most of the sentence sentiments are negative, in general Kanye’s lyrics convey a more sombre tone than they do a positive one. Wording can play a key part in how the natural language analysis measures sentiment. The lowest sentiment sentences are at -0.9, so let’s take a look at some examples of these.\n “Oh, how could you be so heartless?” “The devil is alive I feel him breathing.” “And when I’m older, you ain’t gotta work no more.”  We can see the first two generally are quite negative, but the last one has been misrepresented. In this line, Kanye talks about when he was a kid, he wanted to take care of his mother when she got older so that she wouldn’t have to go to work again, but the syntax of the sentence tricked the API into believing it was a generally negative sentence. This isn’t very common, but does highlight one of the limitations of this natural language API.\nA song’s magnitude is defined as the sum of the absolute values of the sentiments for each sentence in the song. Quite a mouthful - consider it as the ‘emotionality’ of the song; the higher the sentiment is (in one way or another), the more emotional the song becomes. We can break down the sentiment and magnitude over all songs by album.\n  Sentiment (top) and magnitude (bottom), averaged for each song and split by album.    Generally, Kanye’s albums are all quite negative, with the exception of Kids See Ghosts and Jesus is King, his two latest albums. After the release of Graduation, Kanye went through various personal traumas. We can see this reflected here, as the sentiment up to Graduation was increasing, after which it began decreasing again. Only recently has the sentiment began to increase again.\nThe album with the lowest sentiment is Yeezus, which surprisingly does not have the largest magnitude, although the magnitude does not vary as much with album. The sentiment in 808’s \u0026amp; Heartbreak has the largest variance, with certain songs reaching very low and (comparatively) high values. These two albums generally are considered quite emotional, so it is reassuring to see this reflected in the sentiment analysis.\nWe might say that it’s nice that Kanye’s latest releases are more positive, but there is a general attitude of “I miss the old Kanye”. Is that the lower mood Kanye? The always rude Kanye?\n Sentiment and Magnitude against Album Reception The next question in our heads should be, what can we infer from this? Some albums have a higher or lower sentiment than others, but does this have any relationship with the album itself?\nIt turns out: well, maybe. Plotted below are the relationships between the mean sentiment and the mean magnitude, by album, against the aggregated critic reviews for each album collected from Metacritic.\n   Linear regression for Album sentiment against metacritic score.       Linear regression for Album magnitude against metacritic score.    Linear regression models are chosen because of the sparsity of data. Any more complex model is likely unneeded, but would also lack sufficient degrees of freedom to perform any meaningful inference. Modelling these variables separately allowed for more succinct visualisation and interpretation of their effects in isolation. The goodness of fit for each model can be evaluated somewhat with \\(R^2\\) values: \\[ R^2_{\\text{sentiment}} \\approx 0.270, \\qquad R^2_{\\text{magnitude}} \\approx 0.675. \\] So the model involving each album’s mean magnitude explains more of the variance than the sentiment, and there is some definite correlation there. Does this mean an album with a higher magnitude (i.e. contains more emotional lyrics) will be more critically acclaimed? I can’t say for sure, but it’s an interesting point to note.\n Generating a New Kanye Song Finally, I would like to end this post by creating the lyrics for a new Kanye song. OpenAI’s unsupervised GPT-2 language model was trained on 40GB of internet text, and tasked with predicting the next word given in the text. The model contains an amazing 1.5 billion parameters. The gpt-2-simple package in Python provides straightforward access to finetuning the GPT-2 model to an additional dataset, so that text generations are based on the new data, but the language has already been learned from the 40GB of internet text (if you think GPT-2 is impressive, take a look at GPT-3).\nI input the formatted lyric text file to finetune the GPT-2 model to Kanye’s lyrics, and generated many large sized texts to be considered as songs. This was done in Google colab notebooks, and the code used to do so is available here.\nOf the songs generated, I picked a coherent and ‘song-like’ generation and changed the formatting (added spacing). I hope you enjoy the latest release!\nMy name is Bess, I’m 21 years old and I just want to be a real star\nI just want to be a superstar\nThe city skyline, the planes flying overhead\nI’m groovy as f***, like Good Charlotte\nUh, and I just want my daddy to be proud of me\nCause I ain’t talkin’ about Kris when it comes to being in the club\nIt’s Jay that I’m talkin’ about, man\nEven though he got the baby’s clothes on\nI done wore nothing but red until he tucked me in\nAnd when he woke up, I was still wearing everything buterin’\nS*** was very “The Big Lebowski”\nI was standin’ by myself writing this song\nAnd I just started to cry\nBecause this s*** can’t get any worse\nThis s*** can’t get any worse\nOh, Lord, oh, Lord\nI’m comin’, I’m comin’ in, load ya weapons\nI’m comin’, I’m comin’ in, load ya weapons\nI’m comin’, I’m comin’ in, load ya weapons\nI’m comin’, I’m comin’ in, load ya weapons\nAnd I came back, I came back, I came back\nAnd I looked in the mirror and I seen the biggest\nThe guns are in the table, the weapons is in the air\nYeah, make America great again\nKeep America great again\nKeep America great again\nKeep America great again\nKeep America great again\nKeep America great again\nKeep America great again\n Seems to match Kanye pretty well, we have mention of his mother-in-law, Kris Jenner, as well as his sort-of friend and collaborator Jay-Z. Then we finish with a classic controversial Kanye segment.\n Interactive Visualisations Most of the visualisations that I have shown can be found in an R shiny app below.\nThe kanyenet interactive R shiny app.\nAlso included are additional interactive plots, enabling the filtering by album for sentiment densities and viewing the sentiment and magnitude of each song. You can also view more generations from the GPT-2 model!\nAgain, you can view all code used for this report in the github repository kanyenet.\n ","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594044553,"objectID":"5e971dd0fcf821222a6ba3b23c69aaae","permalink":"https://dannyjameswilliams.co.uk/post/kanye/","publishdate":"2020-07-06T00:00:00Z","relpermalink":"/post/kanye/","section":"post","summary":"“2020 I’mma run the whole election” may not sound like the words of a lyrical genius, but I ask you to “name one genius that ain’t crazy”.","tags":["Natural Language","Data Analysis","Text Generation","Data Visualisation"],"title":"Natural Language Analysis of the Lyrics of Kanye West","type":"post"},{"authors":["Daniel Williams"],"categories":null,"content":"  The arrow assigment operator \u0026lt;- is useless. Before I’m crucified by the R community, hear me out and read this post.\nEvery time I read code written by an academic, lecturer or someone who uses R frequently, I come across the arrow symbol \u0026lt;- used for assignment of variables. Never in my career have I seen someone systematically use the equals symbol = across their code.\nBenefits of the arrow A frequent association with \u0026lt;- is in how assignment works in R. The variable on the right hand side of the operator is assigned to the one on the left. Hence the arrow makes a lot of sense. We can also do it the other way around, for instance:\n3 -\u0026gt; x y \u0026lt;- 5 cat(\u0026quot;x is\u0026quot;, x, \u0026quot;and y is\u0026quot;, y, \u0026quot;\\n\u0026quot;) ## x is 3 and y is 5 So the arrow has a benefit when teaching programming, so if you’re a beginner it is obvious which way around variables are assigned. If you’re not a beginner, it might reinforce this knowledge so that you don’t make mistakes.\nYou can also use the arrow inside of functions to assign variables, for example:\nsystem.time(x \u0026lt;- solve(matrix(rnorm(100^2), 100, 100))) ## user system elapsed ## 0.002 0.000 0.004 Now we can view x separately, even though it was assigned inside the system.time function.\nx[1:5, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.50730275 -0.35703351 -0.39847262 0.7788050 0.14551130 ## [2,] -0.18092188 0.23194703 0.11982541 -0.4136690 -0.04548487 ## [3,] -0.09788994 0.08614508 0.10585201 -0.1223789 -0.01548020 ## [4,] 0.06537141 -0.16506482 0.09142846 0.1438222 0.02654319 ## [5,] 0.03935626 -0.05008914 -0.09419370 0.2286113 -0.03929192 This is perhaps its most useful application, which you cannot do with =. The = sign inside of a function argument is strictly used for matching the function argument with the variable you’re passing through.\nThe arrow also has historical significance, since R’s predecessor, S, used \u0026lt;- exclusively. This R-bloggers post explains that S was based on an older language called APL, which was designed on a keyboard that had an arrow key exactly like \u0026lt;-. But our keyboards now only have a key for =, right?\n Why you should accept the equals sign But I’m here today to tell you to not use \u0026lt;- and to use = instead. Start by asking yourself why you use the arrow? Maybe you have historical reasons and used R before 2001, or more likely, you’re following convention for coding in R that even styling guides recommend.\nFirstly, no other programming language uses the arrows, at least, none of the most frequently used ones such as Python, MATLAB, C++, Julia, Javascript, etc. So if you’re like me and use R alongside other programming languages, why would you bother using \u0026lt;- instead of =? Wouldn’t you like consistency across the languages you write in, at least so that your muscle memory doesn’t have to change depending on whether you’re fitting a Neural network in Python, or a GAM in R?\nOkay fair enough, maybe you don’t mind switching coding styles depending on what language you’re writing in, after all, you are going to be changing a lot more than just the assignment operator. So what other benefits does = have?\n There is a button for it on the keyboard. Consistency between function arguments and assignment. Increased readability and neatness since it has fewer character (admittedly, this is subjective). Similarity with equality operator (==). No confusion between for example x\u0026lt;-2 (\\(x=2\\)) and x \u0026lt; -2 (\\(x \u0026lt; -2\\)). Consistency with mathematics itself.  In general, I prefer to use the equals assigment operator over the arrow, because I like to code in more than just one language.\n The neat full stop While I’m on the subject of the arrow, using a full stop in a variable name brings a lot of confusion. This one is a lot less controversial than disregarding the arrow in my opinion. We can name a variable in R as\nsome.variable = 1 This looks neat! But in other languages, this would throw an error. Why is that? Languages like Python use . as a class operator, and you use it to access elements of a class exclusively, so you cannot use it in variable names. But R doesn’t have this problem, right?\nWhen defining an S3 class in R, you can overwrite some default functions (such as print or plot) with a new function that handles these default operations in a different way for your specific S3 class. To do this for an S3 class called mys3class, you would write a new functions as follows:\nprint.mys3class = function(x, ...){ ... } plot.mys3class = function(x, ...){ ... } Look familiar? So full stops do have a purpose in R apart from assigning neat variable names. For me, I don’t like using full stops for the main reason I don’t like using the \u0026lt;- operator: consistency. If I’m using \u0026lt;- or ., it will be for a specific purpose where I cant use = or _ (however, these are rules that I’ve broken myself, and you can probably find instances of it in my portfolios).\nSo whilst neither the arrow (\u0026lt;-) for assignment nor the full stop (.) for variable naming are completely useless, better alternatives do exist. However, if you value your code looking neat above all else, and aren’t bothered by cross platform consistency; then you can use R’s exclusive \u0026lt;-, or its inconsistent . without issue.\nplot(1:5, 1:5)  ","date":1593358740,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593358740,"objectID":"90c037475b235f73036ad79471899a8b","permalink":"https://dannyjameswilliams.co.uk/post/hottakes/","publishdate":"2020-06-28T15:39:00Z","relpermalink":"/post/hottakes/","section":"post","summary":"The arrow assigment operator \u0026lt;- is useless. Before I’m crucified by the R community, hear me out and read this post.\nEvery time I read code written by an academic, lecturer or someone who uses R frequently, I come across the arrow symbol \u0026lt;- used for assignment of variables.","tags":["R","programming"],"title":"Hot Takes for R","type":"post"},{"authors":null,"categories":null,"content":"The second group project I worked on at COMPASS mainly involved learning how Gaussian process classification worked, as it is a complicated procedure, and not as straight forward as Gaussian process regression.\nOur work involved a number of aspects that have improved on Gaussian process classification in recent literatures:\n Pseudo-Marginal Likelihood: An importance sampling procedure to approximate the marginal likelihood in MCMC sampling. Subset Selection: An entropy based measure that chooses a subset of a full dataset that maximises information across the dataset, referred to as the Information Vector Machine (IVM). Laplace Approximation: An approximation of the posterior of the latent variables.  Together, these approximations makes Gaussian process classification feasibile. Without approximations such as these, the procedure would have an incredible runtime.\nFinally, we compared the results on an e-mail spam dataset, and had a higher prediction accuracy than a JAGS implementation of logistic regression. We combined our code, written in Rcpp, into an R package, available here.\n","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"8e4d5805fb4a45700e848acf820486f1","permalink":"https://dannyjameswilliams.co.uk/projects/gpc/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/projects/gpc/","section":"projects","summary":"Learning, detailing and experimenting with using Gaussian processes for classification. Exploring how to use approximations to avoid unnecessary computational slowdowns with an MCMC sampler.","tags":["Bayesian","Gaussian Process","Simulated Data","MCMC"],"title":"Gaussian Process Classification","type":"projects"},{"authors":["Daniel Williams"],"categories":null,"content":" In February I participated in a COMPASS hackathon, where me and my fellow students fit statistical models to try to improve predictions in forecasting electricity demand based on weather related variables.\nWe were fortunate to be visited by Dr Jethro Browell, a Research Fellow at the University of Strathclyde, who gave a brief lecture on how electricity demand was calculated, and how much it has changed over the last decade. After the lecture, Dr Mateo Fasiolo, a lecturer who works with us, explained a basic Generalised Additive Model (GAM) which can be used to forecast electricity demand for a particular dataset.\nOur task was to output a set of predictions for a testing dataset and submit them to the group git repository. We only had access to the predictor variables of this dataset, so we wouldn’t know how well our model was doing until it was submit and checked by Dr Fasiolo, who then put all submitted scores on the projector at the front of the room. The team with the lowest root mean squared error at the end would be crowned the winner.\nMe and my team “Jim” (named so because we went to the gym) performed well at the start, extending the basic GAM to include additional covariates and interactions, as well as including some feature engineering. The second team “AGang” (because all of their names began with “A”) took the edge over us by removing a single variable that we didn’t realise was actually making our model worse, and their GAM produced better predictions overall by a small margin. The third team “D \u0026amp; D” (because both their names began with a D) was having no luck at all, trying to implement a random forest model as opposed to a GAM, but their predictions were significantly off, and their code took much longer to run than ours, leaving them with little time to troubleshoot.\n  Figure 1: The COMPASS cohort after participating in the hackathon, with Dr Jethro Browell and Dr Mateo Fasiolo in the front.   Try as we did, we were unable to do any better than our original model; but we limited our scope to a GAM, and did not try anything out-of-the-box compared to the other two teams.\nThe “AGang” were set to win it, until a surprise twist of fate sent “D\u0026amp;D” soaring into the lead, with predictions that had a far smaller error than anyone elses. The random forest model they were fitting before had an error, and they managed to fix the error, finish running the model and submit their predictions with only moments to spare. Thus, we came last.\nThis was a fun competition, even though we lost. I realise that our mistake now was that we did not include anything special in our model that accounted for different weather patterns in different regions. Our model would have done very well if it was more variable; so that certain predictors were included in some areas that had more solar power, for instance. The way which we fit the model was the same for all regions, even though they were all quite different.\nYou can read the article from the Bristol school of mathematics here.\n","date":1581953940,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581953940,"objectID":"48fa808d6c6589305e7795237d5ee51f","permalink":"https://dannyjameswilliams.co.uk/post/hackathon/","publishdate":"2020-02-17T15:39:00Z","relpermalink":"/post/hackathon/","section":"post","summary":"In February I participated in a COMPASS hackathon, where me and my fellow students fit statistical models to try to improve predictions in forecasting electricity demand based on weather related variables.","tags":["GAM","modelling","prediction","hackathon"],"title":"Electricity Demand Forecasting Hackathon","type":"post"},{"authors":null,"categories":null,"content":"My most recent project was a group project as part of the first term of the COMPASS CDT. It involved modelling different aspects of a large data set detailing crimes in Chicago. Pictured is a kernel density estimate of both the location of crimes in the city as well as the population.\nMy input to this project was to use logistic regression methods to classify whether an arrest would be successful or not, given other covariates in the crime set; detailing the location, the area, the type of crime type, amongst others. This involved feature engineering and using LASSO coefficient paths to judge the most \u0026lsquo;important\u0026rsquo; covariates.\nThe package that was created for this project can be found as a Github repository here.\n","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574380800,"objectID":"96ae31225f4d14d45d66e031f38eeb3b","permalink":"https://dannyjameswilliams.co.uk/projects/chicago/","publishdate":"2019-11-22T00:00:00Z","relpermalink":"/projects/chicago/","section":"projects","summary":"Modelling crimes in Chicago, using a public dataset. Logistic regression was used to predict whether an arrest would be successful or not, based on a number of predictors.","tags":["Classification","Spatial Statistics","Real Data","Big Data"],"title":"Chicago Crime Classification","type":"projects"},{"authors":null,"categories":null,"content":"Introduction Python is a powerful programming language that is very popular for data analysis as well as many other applications in the real world. This portfolio will go over some intermediary processes in Python 3.7.3, before implementing basic statistical models; including linear models, regression and classification.\nPython Overview Functions and Modules Similar to R, functions can be defined and called in the same script. The function definition syntax is as follows\ndef function_name(input1, input2): variable = function_operation second_variable = second_function_operation return output  There\u0026rsquo;s a couple things to note here. Firstly, indentation is essential to defining the function. Unlike R or MATLAB, there is nothing to write that marks the beginning or end of the function as this is all handled by where the indentation begins and ends. Secondly, the function name comes after the def, instead of the other way around (as in R). Finally, the colon (:) is necessary, as it is in loops and if statements.\nFunctions can be written in one script, and called in another, and the way this is achieved is through modules. Modules are a collection of code that you can load from another file, similar to how source and library work in R. You load modules with any of these lines:\nimport module_name import module_name as short_name from module_name import function_name from module_name import *  All these commands load the module (or parts of it), but do it slightly differently. In Python, all modules loaded have to be accessed through their name to use their functions. For example, to use a function from NumPy, you would have to use numpy.function_name. This name can be shortened if you used the second line above to load the module, commonly, NumPy is abbreviated to np so that you would only have to type np.function_name to access it.\nYou can also load specific functions from a module by using the third and fourth line above, where the asterisk * indicates to load all functions from the module. Note that if the module is loaded this way, you do not need to specify the name of the module preceding the function. It is considered bad practice to load functions this way, as declaring the module name removes the chance of overlapping function names within modules.\nExample: Morse Code As an example of basic operations in Python, consider writing two functions to encode and decode morse code respectively. The encoding function will take a string, or a piece of text, and convert it into morse code. The decoding function does the opposite: converts morse code into plain english text. To write these functions, we first must define a dictionary:\nletter_to_morse = {'a':'.-', 'b':'-...', 'c':'-.-.', 'd':'-..', 'e':'.', 'f':'..-.', 'g':'--.', 'h':'....', 'i':'..', 'j':'.---', 'k':'-.-', 'l':'.-..', 'm':'--', 'n':'-.', 'o':'---', 'p':'.--.', 'q':'--.-', 'r':'.-.', 's':'...', 't':'-', 'u':'..-', 'v':'...-', 'w':'.--', 'x':'-..-', 'y':'-.--', 'z':'--..', '0':'-----', '1':'.----', '2':'..---', '3':'...--', '4':'....-','5':'.....', '6':'-....', '7':'--...', '8':'---..', '9':'----.', ' ':'/'}  So this dictionary can be accessed by indexing with the letter that we want a translation for. For example\nletter_to_morse[\u0026quot;6\u0026quot;]  Likewise, we need to define the reverse; a dictionary that takes morse code and outputs an english letter. We can reverse this dictionary with a loop.\nmorse_to_letter = {} for letter in letter_to_morse: morse = letter_to_morse[letter] morse_to_letter[morse] = letter morse_to_letter[\u0026quot;.-\u0026quot;]  Now we can define the two functions encode and decode that use these two dictionaries. For the encoding function:\ndef encode(x): morse = [] for letter in x: letter = letter.lower() morse.append(letter_to_morse[letter]) morse_message = \u0026quot; \u0026quot;.join(morse) return morse_message  This function initialises an array morse, and loops over all letters in the input x and appends to the morse array the translation in morse code. After the loop, the morse array is joined together. For the decoding function:\ndef decode(x): english = [] morse_letters = x.split(\u0026quot; \u0026quot;) for letter in morse_letters: english.append(morse_to_letter[letter]) english_message = \u0026quot;\u0026quot;.join(english) return english_message  This has a similar process as with encode. First the input x is split, and then each split morse symbol is looped over and converted to english using the morse_to_letter dictionary. Before we run these functions, we can save them in a separate file called morse.py and put it in our working directory. Then using another script (which will be called run_morse.py), we can import the morse.py functions as a module.\nSo we can import morse and run the decode function.\nimport morse morse_message = \u0026quot;.... . .-.. .--. / .. -- / - .-. .- .--. .--. . -.. / .. -. / ... --- -- . / -- --- .-. ... . / -.-. --- -.. .\u0026quot; morse.decode(morse_message)  Likewise, we can encode a secret message using morse.encode:\nmorse.encode(\u0026quot;please dont turn me into morse code\u0026quot;)  One thing to note here is that the dictionaries morse_to_letter and letter_to_morse were defined outside of the functions. Since this module was imported, we can actually access this:\nmorse.letter_to_morse  Although oftentimes we would want variables to remain hidden, this is not possible if we are importing all the contents of a script. Importing Python files as modules is usually done so that functions can be imported across the working directory, as opposed to having one big file that contains all the code.\nClasses Object oriented programming in Python is done through classes, the general form of which is\nclass class_name: def __init__(self): self.variable_I_want_to_define = variable self.another_variable_I_want_to_define = variable_2 def some_class_function(self, x): something_I_can_do_with_my_variables = x return some_output def some_other_function(self): self.an_internal_variable = 3 return another_output  The functions can be run as a member of the class, and the __init__(self) section of the class is a function that is run when the class is initialised. Any variables that are within the __init__ section of the class become defined, sometimes depending on the inputs to the __init__ function. These could be variables such as initial conditions, or initial estimates. We could add more inputs to the __init__ function after self, if needed.\nThe other functions can be called, and these could change some of the internal self. variables, e.g. updating the initial conditions, running a certain loop or adding additional variables.\nExample: Morse Code Again We can update the morse code example to write a class called morse_code_translator that has an encode and decode function as part of the class. This is written as\nclass morse_code_translator: def __init__(self): self.letter_to_morse = {'a':'.-', 'b':'-...', 'c':'-.-.', 'd':'-..', 'e':'.', 'f':'..-.', 'g':'--.', 'h':'....', 'i':'..', 'j':'.---', 'k':'-.-', 'l':'.-..', 'm':'--', 'n':'-.', 'o':'---', 'p':'.--.', 'q':'--.-', 'r':'.-.', 's':'...', 't':'-', 'u':'..-', 'v':'...-', 'w':'.--', 'x':'-..-', 'y':'-.--', 'z':'--..', '0':'-----', '1':'.----', '2':'..---', '3':'...--', '4':'....-', '5':'.....', '6':'-....', '7':'--...', '8':'---..', '9':'----.', ' ':'/'} self.morse_to_letter = {} for letter in self.letter_to_morse: morse = self.letter_to_morse[letter] self.morse_to_letter[morse] = letter self.english_message_history = [] self.morse_message_history = [] def encode(self, x): morse = [] for letter in x: letter = letter.lower() morse.append(self.letter_to_morse[letter]) morse_message = \u0026quot; \u0026quot;.join(morse) self.english_message_history.append(x) self.morse_message_history.append(morse_message) return morse_message def decode(self, x): english = [] morse_letters = x.split(\u0026quot; \u0026quot;) for letter in morse_letters: english.append(self.morse_to_letter[letter]) english_message = \u0026quot;\u0026quot;.join(english) self.english_message_history.append(english_message) self.morse_message_history.append(x) return english_message  So this has simply moved the decode and encode functions over inside a class. The dictionaries are now written as self.letter_to_morse and self.morse_to_letter, with the self. prefix. Other than that, I have also added two more variables: english_message_history and morse_message_history, to exemplify how variables can be updated by calling the functions. Every time encode or decode is run, these arrays keep track of all messages that have passed through either function. We can test to see if this works, by importing this class from morse_class.py.\nfrom morse_class import morse_code_translator translator = morse_code_translator() translator.encode(\u0026quot;please not again\u0026quot;) translator.decode(\u0026quot;.--. .-.. . .- ... . / -.. --- -. - / - ..- .-. -. / -- . / .. -. - --- / . -. --. .-.. .. ... ....\u0026quot;)  So this has worked as expected, and the translator object now can both decode and encode morse code messages. Now that these two messages have passed through the class, we can see if the history has worked.\ntranslator.english_message_history translator.morse_message_history  So the internal self variables have been updated with the two messages that were passed through the class.\nStatistical Models Python has a lot of support for fitting statistical models and machine learning, this is mostly as part of the sklearn package. Other modules must also be imported that provide different funtionality. Statistical models are handled by sklearn, dataframes are handled by pandas and numpy, randomness is handled by numpy and random, and plots are handled by matplotlib.\nimport sklearn as sk import sklearn.linear_model as lm import pandas as pd import numpy as np import random import matplotlib.pyplot as plt  Linear Regression A basic linear regression model in python is handled as part of the sklearn.linear_model module. The use of a linear model in Python can be explained through an example. Some simulated data can be set up as follows $$ \\boldsymbol{x} \\sim \\text{Unif}(0,1), \\qquad y \\sim N(\\boldsymbol{\\mu}, .5), \\qquad \\boldsymbol{\\mu} = \\beta_0 + \\beta_1 \\boldsymbol{x}, $$ where both $\\boldsymbol{x}$ and $\\boldsymbol{y}$ are of length $n$. Since this is simulated data, we pre-define $\\beta_0 = 5$ and $\\beta_1 = -2$ as the intercept and the gradient. We can use numpy to sample this data, and the linear_model module in sklearn to fit a linear model to it.\nn = 200 beta0 = 5 beta1 = -2 x = np.random.uniform(0, 1, n) mu = beta0 + beta1*x y = np.random.normal(mu, .5, n)  This data can be converted into a dataframe with the DataFrame function in pandas.\ndata = pd.DataFrame({\u0026quot;x\u0026quot;:x, \u0026quot;y\u0026quot;:y})  To set up the linear model, a model object must be set up in advance, and then the fitting routine can be called to it. This can be done in one line by nesting operations.\nmodel = lm.LinearRegression(fit_intercept=True).fit(data[[\u0026quot;x\u0026quot;]], data[\u0026quot;y\u0026quot;])  Here, the model has been fit by first creating a linear model object with lm.LinearRegression (and specifying that we do want an intercept). Secondly, the .fit() function has been called on that object to obtain parameter estimates $\\beta_0$ and $\\beta_1$ by passing the covariates x and response variable y to it. Now we can see whether these estimates bare resemblance to the true parameter values used to simulate the data.\nprint(\u0026quot;Intercept:\u0026quot;, model.intercept_, \u0026quot;,\u0026quot;, \u0026quot;Gradient:\u0026quot;, model.coef_)  Intercept: 4.89676478340633 , Gradient: [-1.90845212]  This is reasonably close to the true values. Note that here there is only one covariate vector, but this method is applicable to multiple covariates which would give multiple estimates of the gradient for each covariate. Now we can plot the data and the fit, a pandas data frame has an inbuilt method used for plotting the data, which produces a scatter plot from matplotlib.\ndata.plot.scatter(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, figsize=(14, 11)); plt.plot(x, model.predict(data[[\u0026quot;x\u0026quot;]]), 'r');  Multiple Linear Regression In the previous section, we fit a linear model with a single covariate vector. This is good for exemplary purposes as a two dimensional linear model is easy to visualise. Often in data analysis we can have multiple explanatory variables $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_p$ that give information about the respone variable $\\boldsymbol{y}$.\nFor an example of multiple linear regression, we will be using the Boston house prices dataset, which is part of the freely provided datasets from sklearn. This dataset describes the median value of owner-occupied homes (per $1000), as well as 13 predictors that could provide information about the house value. We load this dataset and convert it to a pandas data frame.\nfrom sklearn.datasets import load_boston data = load_boston() boston = pd.DataFrame(data.data, columns = data.feature_names) boston[\u0026quot;MEDV\u0026quot;] = data.target boston.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV     0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0   1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6   2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7   3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4   4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2     The variable we are interested in predicting is MEDV, and all other variables will be used to give information to predicting MEDV.\nBefore fitting any models, we can perform some exploratory data analysis to help the decision as to what predictor variables need to be included in the model fitting. Luckily, there are easy methods available that inspect how strongly correlated variables are with other variables in a pandas data frame. Firstly, we will be using the inbuilt corr function and plotting it as a heatmap. This uses the seaborn module to create a heatmap.\nimport seaborn corr = boston.corr() cmap = seaborn.diverging_palette(-500, -720, as_cmap=True) plt.figure(figsize=(14, 11)) seaborn.heatmap(corr, cmap=cmap);  There are two things to infer from this plot: how correlated the predictor variables are from one another, and how correlated the predictor variables are from the response. We are mostly interested in how correlated the predictors are with MEDV, as if they influence the value of MEDV significantly, then they are likely to be important to include in a model. Another plot we can use to explore the data is a multi variable scatter plot. That is, a scatter plot for each pair of variables. pandas has a function for this, as part of the plotting submodule.\nfrom pandas.plotting import scatter_matrix scatter_matrix(boston, figsize=(16, 16));  There is a lot to process in this plot, but you can restrict your attention to the final column on the right. This column will show scatter plots with all predictor variables and the response MEDV. We are looking for obvious relationships between the two variables. CRIM seems to have an effect at lower values, ZN seems to have a small effect, INDUS has a non-linear relationship,RM has a clear linear relationship, AGE seems to have a significant effect at lower values, B has an effect at larger values and LSTAT seems to have a quadratic relationship. These variables will be important to consider, but that does not mean all others are not. Since some of these predictors are categorical (even binary), it can be hard to infer their relationship with MEDV from a scatter plot.\nWe can now fit the model using the information above. The predictors we want to include are CRIM, ZN, INDUS, RM, AGE, B and LSTAT. We use the linear model from sklearn again, but first create the new data frame of the reduced dataset. Additionally we can include a quadratic effect for certain predictors by creating a new column in the data frame with the squared values.\nboston_x = boston[[\u0026quot;CRIM\u0026quot;, \u0026quot;ZN\u0026quot;, \u0026quot;INDUS\u0026quot;, \u0026quot;RM\u0026quot;, \u0026quot;AGE\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;LSTAT\u0026quot;]] boston_x.insert(len(boston_x.columns), \u0026quot;LSTAT2\u0026quot;, [i**2 for i in boston_x[\u0026quot;LSTAT\u0026quot;] ]) boston_x.insert(len(boston_x.columns), \u0026quot;AGE2\u0026quot;, [i**2 for i in boston_x[\u0026quot;AGE\u0026quot;] ]) boston_x.insert(len(boston_x.columns), \u0026quot;B2\u0026quot;, [i**2 for i in boston_x[\u0026quot;B\u0026quot;] ])  Now using the linear model with this modified data frame to fit the full model.\nmv_model = lm.LinearRegression(fit_intercept=True).fit(boston_x, data.target) print(\u0026quot;Intercept: \\n\u0026quot;, mv_model.intercept_, \u0026quot;\\n\u0026quot;) print(\u0026quot;Coefficients:\u0026quot;) print(\u0026quot; \\n\u0026quot;.join([str(boston_x.columns[i]) + \u0026quot; \u0026quot; + str(mv_model.coef_[i]) for i in np.arange(len(mv_model.coef_))]))  Intercept: 5.916804277809813 Coefficients: CRIM -0.11229848683846888 ZN 0.0037476008537828498 INDUS -0.0021805395766587208 RM 4.0621316253156285 AGE 0.07875634757295602 B 0.042639346099976716 LSTAT -2.063750521016358 LSTAT2 0.042240400122337596 AGE2 -0.0002509463690499019 B2 -7.788232883181183e-05  Penalised Regression The linear_model submodule of sklearn doesn\u0026rsquo;t just contain the LinearRegression object, it contains many other variations of linear models. Some examples of these are penalised regression models, such as the Lasso model or ridge regression model. You can view the documentation for these online, and they work similarly to LinearRegression. Below I will briefly go through an example of using a Lasso path algorithm on the dataset provided above. Firstly, the model object can be created immediately using the linear_model.lars_path object.\n_,_,coefs = lm.lars_path(data.data, data.target)  This is using the full dataset provided in data.data, as the Lasso path plot can be used for feature selection, similar to the scatter matrix approach above.\nxx = np.sum(np.abs(coefs.T), axis=1) xx /= xx[-1] plt.figure(figsize=(16, 9)) plt.plot(xx, coefs.T) ymin, ymax = plt.ylim() plt.vlines(xx, ymin, ymax, linestyle='dashed') plt.xlabel('|coef| / max|coef|') plt.ylabel('Coefficients') plt.title('LASSO Path') plt.axis('tight') plt.legend(data.feature_names) plt.show()  The coefficients for covariates that provide a lot of information to the response variable are likely going to take a very large value of regularisation parameter to shrink them to zero. These are the lines that are more separated in the plot above, i.e. CRIM, RM, NOX, and CHAS. Interestingly, these were not the variables that were selected in the scatter matrix approach earlier in this report, highlighting the variation in parameter selection depending on the method.\nClassification The final statistical model introduced will be one interested in classifying. Classification is interested in prediction of a usually non-numeric class, i.e. assign the output to a pre-defined class based on some inputs. The classification model explained here will be Logistic Regression (ignore the name, it\u0026rsquo;s not a regression method).\nLogistic regression is called so because it uses a logistic function to map fitted values to probabilities. The basic form of a classification model is the same of that of a regression method, except the output from the model corresponds to latent, unobserved fitted values which decide the class of each set of inputs.\nWe will start by simulating some data $\\boldsymbol{y}$ that need to be classified, which will be generated by a mathematical combination of some inputs $\\boldsymbol{x}$. Each element of $\\boldsymbol{y}$ will be given one of the class labels $\\mathcal{C}_1$, $\\mathcal{C}_2$ or $\\mathcal{C}_3$. The goal of the statistical model is parameter estimation, similar to the regression methods.\n# Simulate x n = 1000 x1 = np.random.uniform(0, 1, n) x2 = np.random.uniform(0, 1, n) # Create parameters and mean beta0 = 3 beta1 = 1.5 beta2 = -4.8 beta3 = 0.5 mu = beta0 + beta1*x1 + beta2*x2 + beta3*x1**2 # Set class based on value of mean y = np.repeat(\u0026quot;Class 1\u0026quot;, n) y[mu \u0026gt; 0] = \u0026quot;Class 2\u0026quot; y[mu \u0026gt; 2] = \u0026quot;Class 3\u0026quot; # Combine into DataFrame clsf_dat = pd.DataFrame({\u0026quot;y\u0026quot;:y,\u0026quot;x1\u0026quot;:x1,\u0026quot;x2\u0026quot;:x2 })  colors = y colors[y == \u0026quot;Class 1\u0026quot;] = \u0026quot;red\u0026quot; colors[y == \u0026quot;Class 2\u0026quot;] = \u0026quot;blue\u0026quot; colors[y == \u0026quot;Class 3\u0026quot;] = \u0026quot;green\u0026quot; clsf_dat.plot.scatter(\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;, c=colors, figsize=(14, 11));  Now that the data is set up, we can fit the logistic regression model to it. LogisticRegression is a function as part of the linear_model submodule of sklearn, just as before, so this next step should be familiar to you.\nlr_model = lm.LogisticRegression(fit_intercept=True, solver='newton-cg', multi_class = 'auto') lr_model = lr_model.fit(clsf_dat[[\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;]], clsf_dat[\u0026quot;y\u0026quot;])  print(\u0026quot;Intercept: \\n\u0026quot;, lr_model.intercept_, \u0026quot;\\n\u0026quot;) print(\u0026quot;Coefficients:\u0026quot;) print(\u0026quot; \\n\u0026quot;.join([str(clsf_dat.columns[i]) + \u0026quot; \u0026quot; + str(lr_model.coef_[i]) for i in np.arange(len(lr_model.coef_))]))  Intercept: [-4.45432353 0.92640783 3.52791569] Coefficients: y [-4.06694321 9.66044839] x1 [0.18794764 0.76440687] x2 [ 3.87899558 -10.42485527]  Note that there are multiple coefficients for each input! This is because even though the data were simulated using one parameter each, the logistic regression model has a different input combination for each class. The first class has both coefficients aliased to the intercept, and anything significantly different (in line with the combination of the remaining parameters) is given a different class.\nWe can roughly evaluate the model\u0026rsquo;s performance by inspecting a plot of its predictions.\npredictions = lr_model.predict(clsf_dat[[\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;]])  colors = predictions colors[predictions == \u0026quot;Class 1\u0026quot;] = \u0026quot;red\u0026quot; colors[predictions == \u0026quot;Class 2\u0026quot;] = \u0026quot;blue\u0026quot; colors[predictions == \u0026quot;Class 3\u0026quot;] = \u0026quot;green\u0026quot; clsf_dat.plot.scatter(\u0026quot;x1\u0026quot;, \u0026quot;x2\u0026quot;, c=colors, figsize=(14, 11));  Here we can see an extremely similar plot to the one above, although it is not quite exactly the same! At the point of overlap between classes, some points are mislabelled as the wrong class, due to uncertainty around the decision boundaries.\nMore complex models exist for classification and regression, but that\u0026rsquo;s for another time.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"347a7c270a086d94287e30d3cc120556","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/pythonstats/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc2/pythonstats/","section":"portfolios","summary":"Introduction Python is a powerful programming language that is very popular for data analysis as well as many other applications in the real world. This portfolio will go over some intermediary processes in Python 3.","tags":null,"title":"Introduction to Python for Statistics","type":"docs"},{"authors":null,"categories":null,"content":"Introduction We fit a neural network in Python to a dataset of cat and dog images, in an attempt to distinguish features that can correctly identify whether the image is of a cat or of a dog.\nThis work uses the keras library from tensorflow. This portfolio will go over the following processes:\n Loading the data and formatting it be fit by a neural network in tensorflow Fitting a neural network to the images in a training set Identifying and evaluating predictive performance on a testing set  import tensorflow as tf import numpy as np import IPython.display as display import matplotlib.pyplot as plt import os import pathlib from tensorflow import keras from PIL import Image tf.__version__  '2.1.0'  Loading and Formatting the Dataset The data are from the Kaggle competition \u0026ldquo;Dogs vs Cats\u0026rdquo;. These images are loaded into the working directory and defined below. These are pre-separated into a training set, validation set and a testing set.\ndata_dir = \u0026quot;/home/fs19144/Documents/SC2/Section10/data/train/\u0026quot; data_dir = pathlib.Path(data_dir) class_names = np.array([item.name for item in data_dir.glob('*')]) image_count = len(list(data_dir.glob('*/*.jpg')))  The data are pre-downloaded and saved separately. This code aboves load the data, retrieves the class names and the total number of images in the training set. The class names are retrieved from the folder names, being cat and dog.\nclass_names  array(['cat', 'dog'], dtype='\u0026lt;U3')  The image count is calculated by reading the length of the list containing all elements in the data directory.\nimage_count  6002  To format the data for a neural network, a few things must be accomplished. Firstly, the images need to have a lower resolution that normal, to avoid large file sizes and to maintain consistency across the dataset. Secondly, the images need to be loaded in batches, to avoid storing all pixel values for all images in a large array. Consider doing this, we have 6002 images, each having a corresponding pixel value for red, green and blue. If the image height and width is 60 pixels, then in total the number of elements in the training set would be\n60 * 60 * 3 * 6002  64821600  which is extremely large. This size increases greatly with a higher image resolution, or a larger dataset, highlighting the need to load images in batches. We define these variables to go into the model fitting below.\nbatch_size = 32 image_height = 60 image_width = 60  We can use functionality from tensorflow to list the data objects in a tf.Dataset:\ntrain_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))  This training set contains the path to all files in the train directory, which can be iterated over every time we want to create a batch. Below are some functions that we will be applying to this data.\ndef get_label(file_path): parts = tf.strings.split(file_path, os.path.sep) return parts[-2] == class_names def decode_img(img): img = tf.image.decode_jpeg(img, channels=3) img = tf.image.convert_image_dtype(img, tf.float32) return tf.image.resize(img, [image_width, image_height]) def process_path(file_path): label = get_label(file_path) img = tf.io.read_file(file_path) img = decode_img(img) return img, label  The process_path function takes the file_path (given when we map across train_ds), retrieves the label from get_label and decodes the image into pixel values in decode_img. get_label simply reads the folder name which the file being iterated over is stored in, whilst decode_img uses tensorflow functionality to read the image in the required format. The tf.data.Dataset structure which train_ds is saved as has inbuilt mapping functionality, so we apply process_path to get a labelled training set.\nlabelled_ds = train_ds.map(process_path, num_parallel_calls=2)  Now we create a function that prepares the dataset for training iterations. This shuffles the dataset, and takes a batch of the specified size we defined earlier.\ndef prepare_for_training(ds, shuffle_buffer_size=1000): ds = ds.shuffle(buffer_size=shuffle_buffer_size) ds = ds.repeat() ds = ds.batch(batch_size) ds = ds.prefetch(buffer_size=2) return ds  train_ds_2 = prepare_for_training(labelled_ds)  Validation Set A neural network model can be improved by using a validation set to reduce overfitting. The neural network model will be fit using the validation set to check the loss on data that it is not training on. The process for formatting the data above can be repeated for different images in a validation folder.\ndata_dir_val = \u0026quot;/home/fs19144/Documents/SC2/Section10/data/validation/\u0026quot; data_dir_val = pathlib.Path(data_dir_val) class_names_val = np.array([item.name for item in data_dir_val.glob('*')]) image_count_val = len(list(data_dir_val.glob('*/*.jpg'))) val_ds = tf.data.Dataset.list_files(str(data_dir_val/'*/*')) labelled_ds_val = val_ds.map(process_path, num_parallel_calls=2) val_ds_2 = prepare_for_training(labelled_ds_val)  Example Batch We can iterate once over train_ds_2 to view an example image batch, along with their labels:\nimage_batch, label_batch = next(iter(train_ds_2)) plt.figure(figsize=(10,12)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(image_batch[i], cmap=plt.cm.binary) ind = [bool(i) for i in (label_batch[i])] plt.xlabel(class_names[ind]) plt.show()  Fitting the Neural Network We use the keras library, a part of tensorflow, to fit a neural network to classify these images. Most importantly, the Sequential function to build the model and different layer functions to create the neural network layers within the Sequential model build.\nfrom tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D  Below is the code used to specify the model using Sequential.\nmodel = Sequential([ Conv2D(16, 3, padding='same', activation='relu', input_shape=(image_height, image_width ,3)), MaxPooling2D(), Dropout(0.2), Conv2D(32, 3, padding='same', activation='relu'), MaxPooling2D(), Conv2D(64, 3, padding='same', activation='relu'), MaxPooling2D(), Dropout(0.2), Flatten(), Dense(512, activation='relu'), Dense(2) ])  This convolutional neural network contains 8 layers, with a final layer to output a classification. The convolutional layers, given by Conv2D, apply convolutional operations to the image and output a single value based on the operations. The pooling layers, given by MaxPooling2D, extract key features of the image and reduce the dimensionality to reduce computational time. In this case, the max pooling approach extracts the maximum value over subregions of the image. Dropout is another method that helps to reduce overfitting, dropping out a proportion of the dataset, and makes the distribution of weight values in the neural network more regular.\nFinally, the Flatten function reformats the data, so that it is vector form and not in a high dimensional format. The Dense layer provides most of the information to the model. These fully connected layers perform the classification on the output to all the other layers. The final Dense(2) will output two nodes, giving probabilities for each one. This will be classifying either a cat or a dog.\nThis model needs to be compiled. We use the adam optimiser, which is a variation on gradient descent.\nmodel.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])  Now we fit the model.\nhistory = model.fit( train_ds_2, steps_per_epoch = image_count // batch_size, epochs = 10, validation_data = val_ds_2, validation_steps = image_count_val // batch_size )  Train for 187 steps, validate for 62 steps Epoch 1/10 187/187 [==============================] - 22s 119ms/step - loss: 0.6936 - accuracy: 0.5044 - val_loss: 0.6906 - val_accuracy: 0.5514 Epoch 2/10 187/187 [==============================] - 22s 119ms/step - loss: 0.6578 - accuracy: 0.5639 - val_loss: 0.6227 - val_accuracy: 0.6127 Epoch 3/10 187/187 [==============================] - 21s 112ms/step - loss: 0.6024 - accuracy: 0.6527 - val_loss: 0.6277 - val_accuracy: 0.6235 Epoch 4/10 187/187 [==============================] - 21s 113ms/step - loss: 0.5647 - accuracy: 0.6868 - val_loss: 0.5571 - val_accuracy: 0.6920 Epoch 5/10 187/187 [==============================] - 21s 114ms/step - loss: 0.5284 - accuracy: 0.7214 - val_loss: 0.5474 - val_accuracy: 0.7054 Epoch 6/10 187/187 [==============================] - 22s 116ms/step - loss: 0.4949 - accuracy: 0.7411 - val_loss: 0.5419 - val_accuracy: 0.7114 Epoch 7/10 187/187 [==============================] - 21s 114ms/step - loss: 0.4685 - accuracy: 0.7585 - val_loss: 0.5114 - val_accuracy: 0.7384 Epoch 8/10 187/187 [==============================] - 21s 114ms/step - loss: 0.4392 - accuracy: 0.7807 - val_loss: 0.4992 - val_accuracy: 0.7523 Epoch 9/10 187/187 [==============================] - 22s 117ms/step - loss: 0.4091 - accuracy: 0.7996 - val_loss: 0.5721 - val_accuracy: 0.7256 Epoch 10/10 187/187 [==============================] - 21s 113ms/step - loss: 0.3819 - accuracy: 0.8182 - val_loss: 0.4976 - val_accuracy: 0.7573  Evaluating the Model Now that the model is fit, how do we know that it\u0026rsquo;s doing a good job? Firstly, by the final lines in the model fitting, we can see that the prediction accuracy ended at around 0.82, so 82% of training set predictions were correct, and around 75% of validation set predicctions were.\nWe can also plot the loss on the training set and the validation set below. We want this to decrease in both cases, as a smaller loss means a better fit. We also want the predictiona accuracy to increase, so that predictions are more correct. If the validation set loss has also decreased to a low value, and the accuracy has increased, then the model has avoided overfitting.\nacc = history.history['accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] val_acc = history.history['val_accuracy'] epochs_range = range(10) plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label='Training Accuracy') plt.plot(epochs_range, val_acc, label='Validation Accuracy') plt.legend(loc='lower right') plt.title('Training and Validation Accuracy') plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label='Training Loss') plt.plot(epochs_range, val_loss, label='Validation Loss') plt.legend(loc='upper right') plt.title('Training and Validation Loss') plt.show()  These plots show that the loss steadily decreases, and the accuracy increases in both cases. Now we can evaluate some predictions on the test set. The test set do not have specific class labels, and we are treating these as unknown.\nTest Set Predictions To further test the performance of the model, and inspect it ourselves, we can see how well the model predicts on a test set (which the model has never seen). This is a separate dataset to the validation set which was part of the model fitting process.\nFirstly, we load the image using the PIL.Image module, and load the images into a list.\nimport numpy as np import PIL.Image as Image import glob pred_list = [] for filename in glob.glob('/home/fs19144/Documents/SC2/Section10/data/test/*.jpg'): x = Image.open(filename).resize((image_height, image_width)) pred_list.append(x)  Next, we loop over the 20 elements in this small test set to inspect the model predictions. Each image is loaded as a numpy array, which automatically converts it into pixel format. These pixel values need to be divided by 255, to get RGB values in $[0,1]$. We obtain model predictions using the simple predict function as part of the model object. These predictions come in terms of fitted values (not strictly probabilities). The predicted classes are then given by taking the largest fitted values output by the model. We can plot the images and their predictions below.\npred_class = np.empty([len(pred_list)], dtype=\u0026quot;S3\u0026quot;) plt.figure(figsize=(10,12)) for i in np.arange(len(pred_list)): x = np.array(pred_list[i])/255.0 pred = model.predict(x[np.newaxis, ...]) pred_class[i] = class_names[np.argmax(pred[0])] # Plot plt.subplot(4,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(x) plt.xlabel(pred_class[i])  There are some funny looking dogs in this sample, and some funny looking cats. Clearly we can see that oftentimes the model calls a cat a dog, and a dog a cat (I imagine the dogs are more offended by that), but for the most part the predictions look accurate.\nThere is a lot more that can go into improving this model, such as\n Pre-processing the image data Increasing the dataset size Increasing the image resolution Adding more layers to the neural network Adding \u0026lsquo;null\u0026rsquo; images, that show neither a cat nor dog, so that the model does not always predict a cat or dog, it can be neither  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"f81255011666b6c474e798ad3dbad971","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/neuralnet/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/portfolios/sc2/neuralnet/","section":"portfolios","summary":"Introduction We fit a neural network in Python to a dataset of cat and dog images, in an attempt to distinguish features that can correctly identify whether the image is of a cat or of a dog.","tags":null,"title":"Neural Network for Identifying Cats and Dogs in Python","type":"docs"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://dannyjameswilliams.co.uk/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"For my Masters dissertation project, I used extreme value theory to model the extremes of precipitation across the South West of the UK, and downscale sparse areas using a numerical weather model on a gridded scale. Pictured is the high resolution elevation levels across the South West, with a grid that samples the same elevation levels every 0.25 degrees in longitude and latitude. This highlights one of the problems with downscaling in such a scenario.\nExtreme values of rainfall can be categorised by their maxima, and using a generalised extreme value (GEV) distribution, these maxima can be modelled. I modelled two sets of maxima separately; for the observations (point locations) and the model output (gridded locations). A separate downscaling generalised additive model (GAM) was used to compare predictions from both models, and link them together using various covariates.\nYou can read more about this project in my final dissertation, which can be found here.\n","date":1531267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531267200,"objectID":"64386cfe76442936701b4a3985a23455","permalink":"https://dannyjameswilliams.co.uk/projects/precip/","publishdate":"2018-07-11T00:00:00Z","relpermalink":"/projects/precip/","section":"projects","summary":"My Masters project, using extreme value theory to model maxima of precipitation across my home region. Comparing results from gridded model output and observations.","tags":["Real Data","Big Data","Environment","GAM","Extreme Value Theory"],"title":"Downscaling Extremes of Precipitation in the South West","type":"projects"},{"authors":null,"categories":null,"content":" Introduction We’ve all been in the situation where we want to perform some extremely complicated computing process, such as MCMC or manipulating an extremely large data frame a lot of times, but our laptops just aren’t good enough. They don’t have enough cores, no dedicated GPU and only a small amount of memory. Luckily for academics and PhD students, universities in general sympathise with us. A high performance computing (HPC) cluster is a collection of highly powerful computers located somewhere that can be accessed remotely, and used to run terminal scripts for coding.\nThis portfolio will detail the use of Bluecrystal, the super computers available at the University of Bristol. To access these computers, one needs to do so through the terminal, and so a basic knowledge of manipulating and using file structures in bash is required. Section 4 details the fundamentals of bash if the reader is not already familiar.\n Bluecrystal Phase 3 This tutorial will focus on Bluecrystal Phase 3, that is the third generation of Bluecrystal machines. The cluster is made up of 312 compute nodes, which are where the processes are run. The basic nodes have specifications:\n 64GB RAM 300TB storage 16 Cores Infiniband High Speed Network  As well as the large memory nodes, which have 256GB of RAM, and the GPU nodes which each have an NVIDIA Tesla K20 - an extremely powerful GPU.\n Connecting to the HPC Cluster To access the HPC cluster, you would need to log in via SSH (secure shell) from a University of Bristol connection (or a VPN). The ssh command in bash is your friend here. To log in (assuming you have an account), you perform the following command:\nssh your_username@bluecrystalp3.bris.ac.uk It will then ask for your password, which you supply immediately after writing this command. From here you will be in the log-in node, note that you should not run any code on the log-in node, as this node is only purposed for connecting to the compute nodes. If you run any large code on the log-in node you will slow down the HPC cluster for everyone else.\n File System Within the log-in node, you will have your own personal file directory where you can store your files and your code. By default, after logging in you will be in this directory, so if you use the ls command you will see the contents of your personal directory immediately. You can write code here, through the terminal, or copy it into your directory from your own personal computer. You are free to make directories and files here, as the contents of your directory will be read by the compute nodes when you want to run a job.\nTo copy a file from your own computer to your file system in the HPC cluster, you can use the scp command in bash, a command that is run from your own computer only and looks like\nscp path_to_my_file/file.txt your_username@bluecrystalp3.bris.ac.uk::path_inside_hpc/ Performing this operation would copy file.txt in folder path_to_my_file into the path_inside_hpc folder on your directory in the HPC cluster. If you want to do this the other way around, and copy something from your HPC cluster file system to your personal computer, just switch the order of the arguments to scp, but always do it from your own machine.\n Running Jobs To run a job, you must write a bash script that tells the compute node what to do. This bash script will be interpreted at the HPC node and run accordingly. Below is a general template for what this bash script would look like to be passed across:\n#!/bin/bash # #PBS -l nodes=2:ppn=1,walltime=24:00:00 # working directory export WORK_DIR=$HOME cd $WORK_DIR # print to output echo JOB ID: $PBS_JOBID echo Working Directory `pwd` # run something /bin/hostname The first line #!/bin/bash tells the compiler to read this as bash, and the third line #PBS -l nodes=2:ppn=1,walltime=24:00:00 gives information to the HPC cluster as to what you want for the job. You can change these arguments to your suiting, e.g. increase the walltime if you think your code will run for more than 24 hours.\nYou must save this bash script as something like run_R.sh, and then when logged into Bluecrystal, use the command qsub - meaning to submit this job to the queue, i.e.\nqsub run_R.sh which would add this job to the queue. Since there are many people that use the HPC cluster, your job may not start immediately, and you might have to wait. You may have to wait longer if your walltime is particularly high, as you will be waiting for enough nodes to become available.\nOther Functions As well as qsub, there are other commands that you can use to play with the HPC cluster. Some notable ones are\n qstat gives a list of current jobs being run and those in the queue qstat -u user_name gives a list of current jobs queued and running by user_name qstat job_id gives information about the job job_id being run qdel job_id deletes a job with a given job_id    Running Different Code To run other programming languages on the HPC cluster, it can be a bit of faff. To run code such as Python or R on the cluster, you must first load the module associated with the particular language. On the log-in node, you can run\nmodule avail to get a list of all available modules. There will be a lot. Choose one that you like, for example, I am a personal fan of languages/R-3.6.2-gcc9.1.0, and you can load this with module load module_name, for example\nmodule load languages/R-3.6.2-gcc9.1.0 which will allow you to run R and R scripts. To submit a job that runs an R script, you must add this line to the job script before you run the code. To run an R script from bash, you use\nRscript script_name.R Using R Packages Since packages cannot be installed globally on the log-in node, you can install them locally instead. You first type the command R into bash, and then install.packages(\u0026quot;package_name\u0026quot;). It will ask you if you want the package to be installed locally, which you say yes to.\nAfter this, all packages installed on your local file system on the log-in node will be accessible as normal when running job scripts.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9774ef7453eda5512a2ec89fcb6971e8","permalink":"https://dannyjameswilliams.co.uk/portfolios/sc2/hpc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/portfolios/sc2/hpc/","section":"portfolios","summary":"Introduction We’ve all been in the situation where we want to perform some extremely complicated computing process, such as MCMC or manipulating an extremely large data frame a lot of times, but our laptops just aren’t good enough.","tags":null,"title":"Section 5: High Performance Computing with Bluecrystal","type":"portfolios"}]